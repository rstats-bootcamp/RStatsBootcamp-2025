[
  {
    "objectID": "15-github-basics.html",
    "href": "15-github-basics.html",
    "title": "15 Git and GitHub",
    "section": "",
    "text": "Github is a scientific stable for best practice",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub"
    ]
  },
  {
    "objectID": "15-github-basics.html#what-is-version-control",
    "href": "15-github-basics.html#what-is-version-control",
    "title": "15 Git and GitHub",
    "section": "1 What is Version Control?",
    "text": "1 What is Version Control?\nVersion control is a system that records changes to files over time so that you can recall specific versions later. For data science projects, version control helps you:\n\nTrack changes to your code and documents\nCollaborate with others without overwriting each other’s work\nRevert to previous versions if something goes wrong\nDocument the evolution of your analysis\n\n\n\n\n\n\n\nKey Concept\n\n\n\nGit is a distributed version control system that tracks changes in any set of files. GitHub is a web-based platform that hosts Git repositories and adds collaboration features.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub"
    ]
  },
  {
    "objectID": "15-github-basics.html#why-use-git-and-github",
    "href": "15-github-basics.html#why-use-git-and-github",
    "title": "15 Git and GitHub",
    "section": "2 Why Use Git and GitHub?",
    "text": "2 Why Use Git and GitHub?\nFor data scientists and researchers, Git and GitHub provide several benefits:\n\nHistory tracking: Document how your analysis evolved\nCollaboration: Work with others seamlessly\nBackup: Store your code securely in the cloud\nReproducibility: Others can access and run your exact code\nOpen science: Share your work with the broader community",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub"
    ]
  },
  {
    "objectID": "15-github-basics.html#setting-up-git",
    "href": "15-github-basics.html#setting-up-git",
    "title": "15 Git and GitHub",
    "section": "3 Setting Up Git",
    "text": "3 Setting Up Git\n\n3.1 Installing Git\nFirst, you need to install Git on your computer:\n\nWindows: Download and install from git-scm.com\nMac: Install via Homebrew with brew install git or download from git-scm.com\nLinux: Use your package manager, e.g., sudo apt-get install git\n\n\n\n3.2 Configuring Git\nAfter installation, configure Git with your name and email:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub"
    ]
  },
  {
    "objectID": "15-github-basics.html#creating-a-github-account",
    "href": "15-github-basics.html#creating-a-github-account",
    "title": "15 Git and GitHub",
    "section": "4 Creating a GitHub Account",
    "text": "4 Creating a GitHub Account\n\nGo to github.com\nClick “Sign up” and follow the instructions\nChoose a free plan to start",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub"
    ]
  },
  {
    "objectID": "15-github-basics.html#git-basics",
    "href": "15-github-basics.html#git-basics",
    "title": "15 Git and GitHub",
    "section": "5 Git Basics",
    "text": "5 Git Basics\n\n5.1 Key Concepts\n\nRepository (repo): A directory where Git tracks changes\nCommit: A snapshot of changes at a point in time\nBranch: A parallel version of the repository\nRemote: A version of the repository hosted elsewhere (e.g., on GitHub)\nClone: Creating a local copy of a remote repository\nPush: Sending commits to a remote repository\nPull: Getting changes from a remote repository\n\n\n\n5.2 Creating a Repository\n\n5.2.1 On GitHub:\n\nLog in to GitHub\nClick the “+” icon in the top-right corner\nSelect “New repository”\nEnter a repository name and description\nChoose public or private\nClick “Create repository”\n\n\n\n5.2.2 On Your Computer:\n# Create a new directory\nmkdir my-project\ncd my-project\n\n# Initialize Git repository\ngit init\n\n# Connect to GitHub repository\ngit remote add origin https://github.com/yourusername/my-project.git\n\n\n\n5.3 Basic Git Workflow\nThe typical Git workflow involves these steps:\n\nMake changes to your files\nStage the changes for commit\nCommit the changes with a message\nPush the changes to GitHub\n\n# Check status of your repository\ngit status\n\n# Stage changes\ngit add filename.R\n\n# Stage all changes\ngit add .\n\n# Commit changes\ngit commit -m \"Add data cleaning script\"\n\n# Push to GitHub\ngit push origin main\n\n\n5.4 Cloning an Existing Repository\nTo work with an existing repository:\n# Clone a repository\ngit clone https://github.com/username/repository.git\n\n# Navigate into the repository\ncd repository\n\n\n5.5 Pulling Changes\nWhen working with others, you’ll need to get their changes:\n# Get latest changes\ngit pull origin main",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub"
    ]
  },
  {
    "objectID": "15-github-basics.html#branching-and-merging",
    "href": "15-github-basics.html#branching-and-merging",
    "title": "15 Git and GitHub",
    "section": "6 Branching and Merging",
    "text": "6 Branching and Merging\nBranches allow you to work on different features or experiments without affecting the main codebase.\n\n6.1 Creating and Using Branches\n# Create a new branch\ngit branch feature-analysis\n\n# Switch to the branch\ngit checkout feature-analysis\n\n# Create and switch in one command\ngit checkout -b new-feature\n\n# List all branches\ngit branch\n\n\n6.2 Merging Branches\nOnce you’re satisfied with your changes:\n# Switch back to main branch\ngit checkout main\n\n# Merge your feature branch\ngit merge feature-analysis\n\n# Push the merged changes\ngit push origin main",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub"
    ]
  },
  {
    "objectID": "15-github-basics.html#using-github-for-collaboration",
    "href": "15-github-basics.html#using-github-for-collaboration",
    "title": "15 Git and GitHub",
    "section": "7 Using GitHub for Collaboration",
    "text": "7 Using GitHub for Collaboration\nGitHub enhances Git with collaboration features:\n\n7.1 Pull Requests\nPull requests (PRs) let you propose changes to a repository:\n\nFork a repository to your GitHub account\nClone your fork locally\nCreate a branch and make changes\nPush your branch to your fork\nCreate a pull request to the original repository\n\n\n\n7.2 Issues\nGitHub Issues help track tasks, enhancements, and bugs:\n\nCreate detailed issue descriptions\nAssign issues to team members\nLabel issues by type\nReference issues in commits and pull requests",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub"
    ]
  },
  {
    "objectID": "15-github-basics.html#git-and-rstudio-integration",
    "href": "15-github-basics.html#git-and-rstudio-integration",
    "title": "15 Git and GitHub",
    "section": "8 Git and RStudio Integration",
    "text": "8 Git and RStudio Integration\nRStudio provides a user-friendly interface for Git operations:\n\nCreate a new project with version control\nUse the Git pane to stage, commit, and push changes\nView file differences and history",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub"
    ]
  },
  {
    "objectID": "15-github-basics.html#best-practices-for-data-science-projects",
    "href": "15-github-basics.html#best-practices-for-data-science-projects",
    "title": "15 Git and GitHub",
    "section": "9 Best Practices for Data Science Projects",
    "text": "9 Best Practices for Data Science Projects\n\nCommit often: Make small, focused commits\nWrite clear commit messages: Explain what and why, not how\nUse .gitignore: Exclude large data files, outputs, and sensitive information\nStructure your repository: Follow a consistent organization pattern\nDocument your workflow: Include a README with setup instructions",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub"
    ]
  },
  {
    "objectID": "15-github-basics.html#practice-exercises",
    "href": "15-github-basics.html#practice-exercises",
    "title": "15 Git and GitHub",
    "section": "10 Practice Exercises",
    "text": "10 Practice Exercises\n\n\n10.1 Git Setup\nCreate a GitHub account (if you don’t have one) and set up Git on your computer.\n\n\n\n\n\n\n\nSolution: Setting Up Git and GitHub\n\n\n\n\n\nCreating a GitHub Account:\n\nGo to github.com\nClick “Sign up” in the top-right corner\nFollow the prompts to create your account:\n\nEnter your email address\nCreate a password\nChoose a username (this will be your GitHub identity)\nVerify your account (usually by solving a puzzle)\nChoose your plan (the free plan is sufficient for most users)\n\n\nSetting Up Git on Your Computer:\nFor Windows:\n# After installing Git from git-scm.com\n\n# Open Git Bash or Command Prompt and configure your identity\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n\n# Verify the configuration\ngit config --list\nFor Mac:\n# Install Git via Homebrew if not already installed\nbrew install git\n\n# Configure your identity\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n\n# Verify the configuration\ngit config --list\nFor Linux:\n# Install Git via package manager\nsudo apt-get update\nsudo apt-get install git  # For Debian/Ubuntu\n# OR\nsudo yum install git      # For RHEL/CentOS\n\n# Configure your identity\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n\n# Verify the configuration\ngit config --list\nOptional: Set Up SSH Key for Secure Authentication:\n# Generate SSH key\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n\n# Start the SSH agent\neval \"$(ssh-agent -s)\"\n\n# Add your SSH key to the agent\nssh-add ~/.ssh/id_ed25519\n\n# Copy the public key to clipboard (use appropriate command for your OS)\n# Then add it to your GitHub account in Settings &gt; SSH and GPG keys\nOnce you’ve completed these steps, you’ll have a GitHub account and Git configured on your computer, ready for version control.\n\n\n\n\n\n\n10.2 Repository Creation\nCreate a new repository on GitHub and clone it to your computer. Add a simple R script, commit it, and push it to GitHub.\n\n\n\n\n\n\n\nSolution: Creating and Populating a Repository\n\n\n\n\n\nCreating a New Repository on GitHub:\n\nLog in to GitHub\nClick the “+” icon in the top-right corner\nSelect “New repository”\nFill in the repository details:\n\nRepository name: “my-first-r-project”\nDescription: “A repository for learning Git with R”\nMake it public or private\nInitialize with a README\nClick “Create repository”\n\n\nCloning the Repository to Your Computer:\n# Navigate to the directory where you want to store your project\ncd ~/Documents/Projects\n\n# Clone the repository\ngit clone https://github.com/your-username/my-first-r-project.git\n\n# Navigate into the repository\ncd my-first-r-project\nCreating a Simple R Script:\nCreate a file named simple_analysis.R with the following content:\n# A simple R script for demonstration\n\n# Load a built-in dataset\ndata(iris)\n\n# Display summary statistics\nsummary(iris)\n\n# Create a simple plot\nplot(iris$Sepal.Length, iris$Sepal.Width,\n     main = \"Sepal Dimensions in Iris Dataset\",\n     xlab = \"Sepal Length (cm)\",\n     ylab = \"Sepal Width (cm)\",\n     col = as.numeric(iris$Species),\n     pch = 19)\n\n# Add a legend\nlegend(\"topright\", \n       legend = levels(iris$Species),\n       col = 1:3,\n       pch = 19,\n       title = \"Species\")\n\n# Calculate mean values by species\naggregate(iris[, 1:4], by = list(Species = iris$Species), FUN = mean)\nCommitting and Pushing the Script:\n# Check the status of your repository\ngit status\n\n# Add the R script to the staging area\ngit add simple_analysis.R\n\n# Commit the changes with a descriptive message\ngit commit -m \"Add simple iris dataset analysis script\"\n\n# Push the changes to GitHub\ngit push origin main\nAfter completing these steps: 1. You’ve created a new repository on GitHub 2. Cloned it to your local machine 3. Added a simple R script that analyzes the iris dataset 4. Committed the changes with a descriptive message 5. Pushed the changes back to GitHub\nYou can now view your repository on GitHub and see the R script you added.\n\n\n\n\n\n\n10.3 Open Source Contribution\nFork an existing R project on GitHub, make a small improvement, and create a pull request.\n\n\n\n\n\n\n\nSolution: Contributing to an Open Source Project\n\n\n\n\n\nForking an Existing R Project:\n\nFind an R project on GitHub that interests you. For this example, let’s use a hypothetical project called “simple-r-examples” at https://github.com/original-owner/simple-r-examples\nClick the “Fork” button in the top-right corner of the repository page\nThis creates a copy of the repository in your GitHub account\n\nCloning Your Fork to Your Computer:\n# Clone your fork to your local machine\ngit clone https://github.com/your-username/simple-r-examples.git\n\n# Navigate into the repository\ncd simple-r-examples\n\n# Add the original repository as a remote called \"upstream\"\ngit remote add upstream https://github.com/original-owner/simple-r-examples.git\nMaking a Small Improvement:\nLet’s say you found a script called data_cleaning.R with a function that could use better documentation:\n# Original function\nclean_data &lt;- function(df) {\n  # Remove NA values\n  df &lt;- na.omit(df)\n  # Remove duplicates\n  df &lt;- unique(df)\n  return(df)\n}\nYou could improve it like this:\n#' Clean a data frame by removing missing values and duplicates\n#' \n#' This function takes a data frame and performs basic cleaning operations\n#' including removal of NA values and duplicate rows.\n#' \n#' @param df A data frame to be cleaned\n#' @return A cleaned data frame with no NA values or duplicate rows\n#' @examples\n#' df &lt;- data.frame(x = c(1, 2, 2, NA, 3), y = c(\"a\", \"b\", \"b\", \"c\", \"d\"))\n#' clean_data(df)\nclean_data &lt;- function(df) {\n  # Check input\n  if (!is.data.frame(df)) {\n    stop(\"Input must be a data frame\")\n  }\n  \n  # Remove NA values\n  df &lt;- na.omit(df)\n  \n  # Remove duplicates\n  df &lt;- unique(df)\n  \n  return(df)\n}\nCreating a Branch for Your Changes:\n# Create a new branch for your changes\ngit checkout -b improve-documentation\n\n# Edit the file with your improvements\n# (Use your favorite text editor or IDE)\n\n# Check what you've changed\ngit diff\n\n# Add the modified file to staging\ngit add data_cleaning.R\n\n# Commit your changes\ngit commit -m \"Improve documentation for clean_data function\"\n\n# Push your branch to your fork\ngit push origin improve-documentation\nCreating a Pull Request:\n\nGo to your fork on GitHub\nYou should see a prompt to “Compare & pull request” for your recently pushed branch\nClick the button\nFill in the pull request details:\n\nTitle: “Improve documentation for clean_data function”\nDescription: “Added roxygen-style documentation to the clean_data function, including parameter descriptions and an example. Also added input validation to check that the input is a data frame.”\n\nClick “Create pull request”\n\nThe maintainers of the original repository will now be notified of your pull request. They can review your changes, request modifications if needed, and eventually merge your improvements into the main project.\nBest Practices for Pull Requests: - Keep changes focused and small - Follow the project’s coding style and conventions - Include tests if appropriate - Be responsive to feedback from maintainers - Be patient, as maintainers may be busy",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub"
    ]
  },
  {
    "objectID": "15-github-basics.html#additional-resources",
    "href": "15-github-basics.html#additional-resources",
    "title": "15 Git and GitHub",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nHappy Git and GitHub for the useR\nGitHub Guides\nGit Cheat Sheet\nGitHub Learning Lab",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub"
    ]
  },
  {
    "objectID": "11-t-test.html",
    "href": "11-t-test.html",
    "title": "11 T-test",
    "section": "",
    "text": "Measure and compare",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "11 T-test"
    ]
  },
  {
    "objectID": "11-t-test.html#students-t-test",
    "href": "11-t-test.html#students-t-test",
    "title": "11 T-test",
    "section": "1 Student’s T-test",
    "text": "1 Student’s T-test\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this lesson, you will be able to:\n\nArticulate the question of the t-test\nEvaluate data and assumptions for the t-test\nGraph t-test data\nPerform tests and alternatives for the t-test\n\n\n\n\nThe t-test and t-distribution are widely considered to be at the very foundation of modern statistic science and they form an important foundation for the practice of statistics. Who would believe they were invented to make great beer better?\n\nWilliam Sealy Gosset is credited with inventing and applying the idea of the t-test to assist in scientific quality control while working for the Guinness brewery. The idea was refined and supported by the great statistician R. A. Fisher, and the idea was initially described in a paper anonymously by “Student”, in order to protect the commercial interests of Guinness. Today, it is perhaps one of the most prevalent and basic tools in statistics, and it is a fascinating story.\nHere, we will briefly look at the practical basis of the t-test before going on the look at different ways it can be applied to data.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "11 T-test"
    ]
  },
  {
    "objectID": "11-t-test.html#the-question-of-the-t-test",
    "href": "11-t-test.html#the-question-of-the-t-test",
    "title": "11 T-test",
    "section": "2 The question of the t-test",
    "text": "2 The question of the t-test\nThere are three common versions of the t-test.\nThe typical premise of the t-test is that it is used to compare populations you are interested in, which you measure with independent samples. There are a few versions of the basic question.\n2.1 2 independent samples\nHere you have measured a numeric variable and have two samples. The question is: are the means of the two samples different (i.e. did the samples come from different populations)? A typical design here might be an experiment with a control and one treatment group. Another more general design might be a sample taken under one defined condition, is compared to a sample taken under a different condition.\nThe data could be summarized in two different ways. The “long format” way would be to have a vector with the measured variable, and another vector that is a factor with 2 levels defining the two sample conditions (one numeric vector, one factor vector).\n\n## 2 sample t-test long format data\n\ndensity &lt;- c(rep('high', 7), rep('low', 7))\nheight &lt;- c(2,3,4,3,4,3,2,\n            6,8,6,9,7,8,7)\n\n(long.data &lt;- data.frame(density,height))\n\n   density height\n1     high      2\n2     high      3\n3     high      4\n4     high      3\n5     high      4\n6     high      3\n7     high      2\n8      low      6\n9      low      8\n10     low      6\n11     low      9\n12     low      7\n13     low      8\n14     low      7\n\n\n\nThe “wide format” way would be to have a different numeric column for each of the samples (2 numeric vectors).\n\n## 2 sample t-test wide format data\n\n(wide.data &lt;- data.frame(high.ht = c(2,3,4,3,4,3,2),\n                         low.ht = c(6,8,6,9,7,8,7)))\n\n  high.ht low.ht\n1       2      6\n2       3      8\n3       4      6\n4       3      9\n5       4      7\n6       3      8\n7       2      7\n\n\n\nA typical graph representing data like this, would be a boxplot(). Optionally, to be maximally informative, one can add the raw data as points over the box summaries.\n\n# Boxplot\nboxplot(height ~ density, data = long.data,\n        main = \"2 independent samples\")\n\n# Optional: add raw data points\n# jitter() nudges the x-axis placement so that the points do not overlap\nset.seed(42)\npoints(x = jitter(c(1,1,1,1,1,1,1,\n                    2,2,2,2,2,2,2), \n                  amount = .2),\n       y = long.data$height,\n       col = \"red\", pch = 16, cex = .8) # Mere vanity\n\n\n\n\n\n\n\n\n2.2 Compare 1 sample to a known mean\nHere you have one sample which you wish to compare to a mean value. The basic question is did the sample come from a population exhibiting the known mean?\nThe data are simply a single numeric vector, and the population mean for comparison.`\n\n\n# The data\nmysam &lt;- c(2.06, 1.77, 1.9, 1.94, 1.91, 1.83, \n           2.08, 1.84, 2.15, 1.84, \n           2.05, 2.19, 1.64, 1.81, 1.83)\n\nboxplot(mysam, \n         main = \"Is your sample population different from the dashed line?\")\n\npoints(x = jitter(rep(1,15), amount = .1),\n        y = mysam,\n        col = \"red\", pch = 16, cex = .8) # Mere vanity\n\nabline(h = 2.0,\n        col = \"blue\", lty = 2, lwd = 2)  # Mere vanity\n\n\n\n\n\n\n\n\n2.3 Paired samples\nThis is a third kind of t-test question. Here the individual observation comprising the 2 samples are not independent. A typical example might be the measurement of some variable before and after a treatment (e.g. measure crop yield in plots in a field before and after a soil treatment in successive years); another classic example would be measuring plots that are paired spatially (e.g., plots are chosen and within each plot a treatment and untreated measurement is made.).\nFor each of these examples, there is a unit, patient, or plot identification, that represents the relationship of each paired measure.\n\n2.4 Plotting paired samples\n\n# Biochar application, measure N before and after\n\n# Data \n# (the code are kind of ugly, but run it to \"make\" biochar)\nbiochar &lt;- structure(list(\n  plot = c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \n           \"J\", \"K\", \"L\", \"M\", \"N\", \"O\"), \n  N.first = c(13.4, 16.7, 17.9, 18.5, 18.6, 18.6, 18.7, \n              20.5, 20.6, 21.5, 24.2, 24.5, 25, 27.1, 28.1), \n  N.second = c(16, 16.7, 18.7, 18.7, 22.1, 22.7, 23.1, \n               23.1, 23.2, 23.5, 25.4, 25.9, 27.6, 28, 29.7)), \n  class = \"data.frame\", \n  row.names = c(NA, -15L))\n\nbiochar\n\n   plot N.first N.second\n1     A    13.4     16.0\n2     B    16.7     16.7\n3     C    17.9     18.7\n4     D    18.5     18.7\n5     E    18.6     22.1\n6     F    18.6     22.7\n7     G    18.7     23.1\n8     H    20.5     23.1\n9     I    20.6     23.2\n10    J    21.5     23.5\n11    K    24.2     25.4\n12    L    24.5     25.9\n13    M    25.0     27.6\n14    N    27.1     28.0\n15    O    28.1     29.7\n\n# boxplot() would work, but hides pairwise relationship\n# Try this:\n\nplot(x = jitter(c(rep(1,15), rep(2,15)),amount = .02),\n     y = c(biochar$N.first, biochar$N.second),\n     xaxt = \"n\", xlim = c(0.5, 2.5),\n     cex = .8, col = \"blue\", pch = 16,  # Mere vanity\n     xlab = \"Biochar treatment\",\n     ylab = \"Soil N\",\n     main = \"Do the lines tend to increase?\")\n\nmtext(side = 1, at = 1:2, text = c(\"before\", \"after\"), line = 1)\n\n# Get crazy: add horizontal lines to visualize the plot pairs\nfor(i in 1:15){\nlines(x = c(1.05,1.95),\n      y = c(biochar$N.first[i], biochar$N.second[i]),\n      lty = 2, lwd = 1, col = \"red\") # Mere vanity\n}",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "11 T-test"
    ]
  },
  {
    "objectID": "11-t-test.html#data-and-assumptions",
    "href": "11-t-test.html#data-and-assumptions",
    "title": "11 T-test",
    "section": "3 Data and assumptions",
    "text": "3 Data and assumptions\nThe principle assumptions of the t-test are:\n\nGaussian distribution of observations WITHIN each sample\nHeteroscedasticity (our old friend) - i.e., the variance is equal in each sample\nIndependence of observations\n\n\n3.1 Evaluating and testing the assumptions\nThe t-test is thought to be somewhat robust to violation of assumptions. For example, if the assumptions of Gaussian distribution and heteroscedasticiy are violated (a little), it is not likely to greatly bias your results. The assumption of independence of observations is always of high importance.\n\nIf in doubt about using your personal subjective judgement, based on your personal data analysis experience, it is always best to fully evaluate the evidence whether assumptions have been violated for parametric statistics tests with formal testing.\n\n\n3.2 Gaussian distribution of observations WITHIN each sample\nWe would typically first test the assumption of Gaussian distribution using graphical evaluation with, for examples, a histogram (hist()) and a q-q plot (e.g., with qqplot()), possibly along with a statistical test evaluating whether data are Gaussian (e.g., shapiro.test().\nNB 1 the assumption of Gaussian does not apply to to ALL OBSERVATIONS TOGETHER for both samples of a two sample t-test, but applies TO EACH SAMPLE SEPARATELY (this is sometimes confusing for beginners). The reason for this is that the very nature of the two sample t-test hypothesizes that the two samples come from DIFFERENT POPULATIONS, but that each population adheres to the Gaussian distribution.\nNB 2 testing whether each sample is Gaussian is analogous to testing whether the residuals are Gaussian for regression.\nThe following code explores the ideas here further.\n\n# First make some data as an example.\n\n# This is height in cm data measured from 30 females and 30 males\n# NB because we simulate this data, we literally specify the \n# samples are indeed from DIFFERENT, GAUSSIAN populations.\n# Obviously in a real sampling situation you would not know this,\n# hence we will test it formally.\n\nset.seed(1)\nheight &lt;- c(rnorm(30,185,10),\n            rnorm(30,150,10))\nsex &lt;- c('m','m','m','m','m','m','m','m','m','m',\n         'm','m','m','m','m','m','m','m','m','m',\n         'm','m','m','m','m','m','m','m','m','m',\n         'f','f','f','f','f','f','f','f','f','f',\n         'f','f','f','f','f','f','f','f','f','f',\n         'f','f','f','f','f','f','f','f','f','f')\n         \n# Package the variables we made into a data frame\n# and remove the variables outside the dataframe to \n# keep our global environment tidy (not required, but satisfying!)\n\ndata &lt;- data.frame(height,sex)\nrm(height, sex)\n\n# plot raw data\nhist(x = data$height, \n     main = 'Height ignoring sex',\n     xlab = 'Height (cm)',\n     freq = F)\n     \n# Draw expected Gaussian\n\n\n# Draw overall mean and the means\n# for each level of sex\n\nabline(v = c(mean(data$height),                 # mean overall\n             mean(data$height[data$sex=='f']),  # mean f\n             mean(data$height[data$sex=='m'])), # mean m\n       lty = c(1,2,2),                          # line types for each\n       lwd = 3,                                 # line width for all 3\n       col = c(\"black\", \"red\", \"blue\"))         # colour for each\n\n# Draw expected Gaussian density curve\nmv &lt;- data$height                               # generalizes following code\n\nxcoord &lt;- seq(min(mv),                          # make x coordinates\n              max(mv),\n              length = length(mv))\nycoord &lt;- dnorm(x = xcoord,                     # make y coordinates\n               mean = mean(mv),\n               sd = sd(mv))\n\nlines(x = xcoord, y = ycoord,                  # draw curve\n      col = 'darkgreen', lwd = 3)\n      \n# for clarity here, add on some labels...\ntext(x = 155, y = .01, labels = \"f\\nmean\", col = \"red\")\ntext(x = 182, y = .011, labels = \"m\\nmean\", col = \"blue\")\ntext(x = 172.1, y = .015, labels = \"grand\\nmean\")\ntext(x = 140, y = .012, labels = \"Gaussian\\ndensity\", col = \"darkgreen\")\n\n\n\n\n\n\n\n\nNote the gray bars on the histogram have two peaks, and is terribly non-Gaussian looking. Also note the data does not look similar to Gaussian! To emphasize the point, we do not even expect these height measures to come from the same population (in fact our hypothesis is that they do not), therefore we do not expect the WHOLE vector of data to be Gaussian.\nWe could further formally test this of course, with a quantile-quantile (“qq”) plot, and perhaps a statistical test of Gaussian like the Shapiro Test.\n\n\nqqnorm(data$height,\n        main = \"Q-Q Gaussian line for our Height data\")\nqqline(data$height)\n\n\n\n\n\n\n\n\nNotice the q-q plot shows divergences from the Gaussian expected line at both ends and in the middle! Compare this to our histogram and expected density curve above.\n\nThe Shapiro Test will confirm that these data are not Gaussian.\n\nshapiro.test(data$height)\n\n\n    Shapiro-Wilk normality test\n\ndata:  data$height\nW = 0.91918, p-value = 0.0007111\n\n\n\nSo testing formally, we find that our data are different to the expected Gaussian (W = 0.92, n = 60, p = 0.0007).\n\n3.3 Properly testing Gaussian for the two sample t-test\nTo properly test the assumption of Gaussian for a 2 sample t-test, you would test the assumption separately for each group. Here is an example that uses graphs and the Shapiro test:\n\n# step 1 graph a hist() of the continuous variable\n# separately for each factor level\n\npar(mfrow = c(2,1))                                # set so hist's \"stack\"\n\n# draw males hist\nhist(data$height[data$sex == 'm'],                 # select males\n     xlim = c(min(data$height), max(data$height)), # set limit for ALL data\n     col = \"blue\", freq = F,\n     xlab = \"Height (cm)\", main = \"Males\") \n\nmv &lt;- data$height[data$sex == 'm']\nxcoord &lt;- seq(min(mv),                          # make x coordinates\n              max(mv),\n              length = length(mv))\nycoord &lt;- dnorm(x = xcoord,                     # make y coordinates\n               mean = mean(mv),\n               sd = sd(mv))\n\nlines(x = xcoord, y = ycoord,                  # draw curve\n      col = 'lightblue', lwd = 3)\n\n# draw females hist\nhist(data$height[data$sex == 'f'],                 # select females\n     xlim = c(min(data$height), max(data$height)), # set limit for ALL data\n     col = \"red\", freq = F,\n     xlab = \"Height (cm)\", main = \"Females\")\n\nmv &lt;- data$height[data$sex == 'f']\nxcoord &lt;- seq(min(mv),                          # make x coordinates\n              max(mv),\n\n              length = length(mv))\nycoord &lt;- dnorm(x = xcoord,                     # make y coordinates\n               mean = mean(mv),\n               sd = sd(mv))\nlines(x = xcoord, y = ycoord,                   # draw curve\n      col = 'pink', lwd = 3)\n\n\n\n\n\n\n# set default layout back\npar(mfrow = c(1,1)) \n\n\nYou can really see the difference here and each respective population seems to conform relatively closely to Gaussian (allowing for sampling error). This is EXACTLY what we would expect for morphological data.\nNow we can make q-q plots and formally test the distributions with the Shapiro Test.\nFirst we graphically examine the distributions:\n\n\n## QQ Plots\n\npar(mfrow = c(1,2))\n\n# males\nqqnorm(data$height[data$sex == 'm'],\n        main = \"Q-Q Gaussian line for male height\")\nqqline(data$height[data$sex == 'm'])\n\n# females\nqqnorm(data$height[data$sex == 'f'],\n        main = \"Q-Q Gaussian line for female height\")\nqqline(data$height[data$sex == 'f'])\n\n\n\n\n\n\npar(mfrow = c(1,1))\n\n\nSecond, we evaluate whether the distribution deviate from Gaussian with an objective test.\nThe q-q plots look much better for each sex separately for the height data - i.e., most of the points are near the expected lines.\n\n\n## Shapiro Tests\n\nshapiro.test(data$height[data$sex == 'm'])\n\n\n    Shapiro-Wilk normality test\n\ndata:  data$height[data$sex == \"m\"]\nW = 0.95011, p-value = 0.1703\n\nshapiro.test(data$height[data$sex == 'f'])\n\n\n    Shapiro-Wilk normality test\n\ndata:  data$height[data$sex == \"f\"]\nW = 0.98568, p-value = 0.9482\n\n\n\nThe Shapiro test for Gaussian showed that neither sample of height data for each respective sex deviates significantly from Gaussian (males: W = 0.95, n = 30, p = 0.17; females: W = 0.99, n = 30, p = 0.95).\n\nIf the Gaussian assumption cannot be met reasonably with the data, a common alternative that does not require it is the Mann-Whitney U-test, also known by the name Wilcoxon Test (which we will look at below).\n\n3.4 Heteroscedasticity assumption\nWe would typically examine the variance graphically or through calculation and comparison of descriptive statistics, although a formal test (e.g. using var.test()) is possible. However, in the case of the 2 sample t-test, there exist methods to “pool” the standard deviation if variances are not equal. Thus, if pooled SD is used and the 2 sample t-test is conducted assuming unequal variances (NB this is the default setting in the base R t.test()), it is not necessary to test this assumption for the specific case of the 2 sample t-test (but still may be interesting to be aware of as a feature of your data).\n\n3.5 Independence assumption\nThe assumption of independence of data is extremely important and related to making an INFERENCE on a POPULATION of interest via SAMPLING one or more populations of interest. If for example pairs of observations are not independent, an alternative test would be appropriate, like the Paired t-test.\nFurther information about the t-test and assumptions can be found in John MacDonald’s excellent online Biostats Handbook.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "11 T-test"
    ]
  },
  {
    "objectID": "11-t-test.html#graphing",
    "href": "11-t-test.html#graphing",
    "title": "11 T-test",
    "section": "4 Graphing",
    "text": "4 Graphing\nWe have already examined the most common cases for data, data arrangement and types of questions that fit the t-test. The principle graph types are simple:\n\n2 independent samples - Boxplot or similar graph, showing the central tendency of the data and making it easy to visually compare the two samples.\n1 sample - Again, boxplot or similar showing the variation of the single sample. Indicating the population mean as a reference is useful.\n2 paired samples - A boxplot is second best to a graph that indicates the tendency for change between the paired observations.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "11 T-test"
    ]
  },
  {
    "objectID": "11-t-test.html#examples-of-the-t-test-and-alternatives",
    "href": "11-t-test.html#examples-of-the-t-test-and-alternatives",
    "title": "11 T-test",
    "section": "5 Examples of the t-test and alternatives",
    "text": "5 Examples of the t-test and alternatives\nWe will cover four examples here looking at the t-test variants and alternatives when assumptions are not met:\n\nt-test for 2 independent samples\n1 sample t-test\n2 paired samples\nMann-Whitney U-test\n\n\n5.1 t-test for 2 independent samples\nThe example we will use here is the amount of tree growth over a period of time, where samples were taken of individual trees grown under two conditions - high density of trees versus low density. The hypothesis we are testing is whether there is evidence the samples came from different populations (by inference, we are possibly interested in whether there is an effect of density on growth). The data we will use is similar to the long.data from above.\n\n# 2-sample t-test\n# Try this\n# data\ndensity &lt;- c(\"high\",\"high\",\"high\",\"high\",\"high\",\"high\",\"high\",\n             \"low\",\"low\",\"low\",\"low\",\"low\",\"low\",\"low\")\nheight &lt;- c(2.1,3.5,4.3,3.2,4.5,3.7,2.7, \n            6.1,8,6.9,9.1,7.5,8,7.4)\n(treegrowth &lt;- data.frame(density,height))\n\n   density height\n1     high    2.1\n2     high    3.5\n3     high    4.3\n4     high    3.2\n5     high    4.5\n6     high    3.7\n7     high    2.7\n8      low    6.1\n9      low    8.0\n10     low    6.9\n11     low    9.1\n12     low    7.5\n13     low    8.0\n14     low    7.4\n\n# There is not much data to compare to the Gaussian distribution\nlibrary(car) # for qqPlot()\n\nLoading required package: carData\n\nhist(treegrowth$height[treegrowth$density == \"low\"])\n\n\n\n\n\n\nqqPlot(treegrowth$height[treegrowth$density == \"low\"])\n\n\n\n\n\n\n\n[1] 4 1\n\nhist(treegrowth$height[treegrowth$density == \"high\"])\n\n\n\n\n\n\nqqPlot(treegrowth$height[treegrowth$density == \"high\"])\n\n\n\n\n\n\n\n[1] 1 5\n\n# The histograms are a little \"wooly\", but there are no huge \n# deviations from the expectation of Gaussian and the \n# q-q plots look ok: proceed\n\n# ?t.test\n\n# NB 1 - the x argument can be a formula\n# x = height ~ density\n# or we can set our samples to x and y respectively\n# x = height[low], y = height[high]\n\nt.test(formula = height ~ density, \n       data = treegrowth)\n\n\n    Welch Two Sample t-test\n\ndata:  height by density\nt = -8.6257, df = 11.868, p-value = 1.865e-06\nalternative hypothesis: true difference in means between group high and group low is not equal to 0\n95 percent confidence interval:\n -5.190611 -3.095103\nsample estimates:\nmean in group high  mean in group low \n          3.428571           7.571429 \n\n# Flash challenge: Make a good graph of the data       \n\n\nThere are a few points in the output.\n\nThe main test statistic is the “t value”, which can be be positive or negative (depending on which sample is larger on average); the absolute value of t should increase with the probability that the samples came different populations.\nThe degrees of freedom, df, is adjusted to account for differences in sample variance thought of as a way to quantify the overall difference\nThe 95% confidence interval is an estimation of the true mean difference between populations from which the samples were taken.\nThe P-value\n\nHere, we might report our results in the technical style as follows (remember ALWAYS the test performed: the test statistic, the degrees of freedom or sample size, the P-Value):\n\nWe detected a significant difference between mean height for tree grown at high or low density (2-sample t-test: t = -8.63, df = 11.9, P &lt; 0.0001).\n\n\n5.2 1 sample t-test\nThe example is a sample of earwigs measured in length to the nearest 0.1 mm. There is a hypothesis that global warming has impacted the development in the population and they are getting larger. A long term dataset has established a mean earwig length of 17.0 (NB, this is our estimate of “mu”, \\(\\mu\\), the population mean we will compare our sample to). Are the earwigs getting bigger?\n\n# Try this:\n\n# Data\nearwigs &lt;- c(22.1, 16.3, 19.1, 19.9, 19.2, 17.7, 22.5, 17.7, 24.1, 17.8, \n21.9, 24.9, 13.8, 17.2, 17.6, 19.9, 17.1, 10, 10.7, 22)\n\n# Flash Challenge: Assess this data for adherence to the Gaussian assumption\n\nmymu &lt;- 17.0 # Our mu\n\n# ?t.test #notice the mu argument\n\nt.test(x = earwigs,\n       mu = mymu)\n\n\n    One Sample t-test\n\ndata:  earwigs\nt = 1.7845, df = 19, p-value = 0.09031\nalternative hypothesis: true mean is not equal to 17\n95 percent confidence interval:\n 16.72775 20.42225\nsample estimates:\nmean of x \n   18.575 \n\n# Flash challenge: Make a great graph representing this test       \n\n\nWe found no evidence the mean length of earwigs in our sample was different to the historical mean (1-sample t-test: t = 1.78, df = 19, P-value = 0.09).\n\n\n5.3 2 paired samples\nThe example here is a measure of the hormone cortisol in pregnant cows (related to stress in mammals). A measure was taken in each individual twice; once as a baseline measure, and once after a treatment of soothing music being played to the cows for 3 hours per day. The prediction is that the mean level of cortisol will decrease relative to baseline after experiencing the music treatment.\n\n# Data\n# Try this:\n# Data\ncort.t0 &lt;- c(0.59, 0.68, 0.74, 0.86, 0.54, 0.85, 0.7, 0.81, 0.79, 0.76, \n             0.49, 0.64, 0.74, 0.51, 0.57, 0.74, 0.77, 0.72, 0.52, 0.49)\n\ncort.t1 &lt;- c(1.13, 0.81, 0.77, 0.72, 0.45, 0.9, 0.7, 0.7, 0.98, 0.96, 1.1, \n             0.63, 0.91, 1.1, 0.99, 0.72, 1.11, 1.2, 0.77, 0.91)\n\n# ?t.test # NB the \"paired\" argument\n\nt.test(x = cort.t0,\n       y = cort.t1,\n       paired = TRUE)\n\n\n    Paired t-test\n\ndata:  cort.t0 and cort.t1\nt = -3.7324, df = 19, p-value = 0.001412\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.31605728 -0.08894272\nsample estimates:\nmean difference \n        -0.2025 \n\n# Flash Challenge:\n# 1) Make a great graph that represents these data\n# 2) do the data conform to the assumptions?\n# 3) Was your hypothesis upheld...?\n# 4) format and report the results in the technical style!\n\n\n5.4 Mann-Whitney U-test\nThe Mann-Whitney U-test (wilcox.test() in R, don’t ask why…) is an alternative to the t-test when your data cannot achieve the assumptions for the t-test. The t-test is robust, especially when sample size is large, or the deviation from assumptions is similar for both samples. However, when sample size is not very large (e.g. ~30 per sample or less), and there is skew or the samples are dissimilar, the Mann-Whitney U-test is a good choice. Two sample and one sample methods exist.\nThe example here is chicken egg count for a control, and a +bonemeal diet.\n\n## **Mann-Whitney U-test** ####\ndiet &lt;- c(3, 3, 1, 2, 2, 2, 2, 0, 2, 2, 1, 2, 3, 1, 1)\n\ndiet.bone &lt;- c(5, 6, 1, 2, 3, 5, 1, 7, 5, 1, 2, 2, 5, 2, 4)\n\n\n# Gaussian assumption\nlibrary(car)\nhist(diet)\n\n\n\n\n\n\nhist(diet.bone) # Dist not similar to diet\n\n\n\n\n\n\npar(mfrow = c(1,2))\nqqPlot(diet, \n       main = \"Not Gaussian\") # Divergence\n\n[1] 8 1\n\nqqPlot(diet.bone,\n       main = \"Diff. to diet?\") \n\n\n\n\n\n\n\n[1] 8 2\n\npar(mfrow = c(1,1))\n\n# ?wilcox.test\nwilcox.test(x = diet, y = diet.bone)\n\nWarning in wilcox.test.default(x = diet, y = diet.bone): cannot compute exact\np-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  diet and diet.bone\nW = 63.5, p-value = 0.0374\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nDon’t forget to make a good graph of the data.\n\nboxplot(diet, diet.bone, ylab = 'Egg count')\npoints(x=jitter(rep(1,15), 4), y = diet, \n       col = 'red', pch = 16) # pure vanity\npoints(x=jitter(rep(2,15), 2), y = diet.bone, \n       col = 'red', pch = 16) # pure vanity\n\n\n\n\n\n\n# Flash challenge: make this graph better!\n\n\n\nWe found evidence of a difference in the number of eggs lain under a control diet and a a diet supplemeted with bone meal (Mann-Whitney U-test: W = 63.5, n = [15, 15], P = 0.037) )",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "11 T-test"
    ]
  },
  {
    "objectID": "11-t-test.html#practice-exercises",
    "href": "11-t-test.html#practice-exercises",
    "title": "11 T-test",
    "section": "6 Practice exercises",
    "text": "6 Practice exercises\nFor the following exercises, use the dataset in the file 11-Davis.xlsx. The dataset is in tidy format; take advantage of this by looking at the terse information present in the data dictionary tab. The data are a result of a survey of some university students, who were asked to report their height and weight, and then their height and weight were measured. There will be some data handling as part of the exercises below, and practical and important part of every real data analysis.\n\n6.1 Paired Height Test\nPick the appropriate form of t-test to ask whether male reported and actual height are the same. Perform the test, make a great graph to illustrate, and report your results in the technical style. Show all required code.\n\n\n\n\n\n\n\nSolution: Paired Height Test\n\n\n\n\n\n\n# Read in the Davis dataset\nlibrary(readxl)\ndavis &lt;- read_excel(\"data/11-Davis.xlsx\")\n\n# Filter for males only\nmales &lt;- davis[davis$sex == \"M\", ]\n\n# Perform paired t-test on reported vs. actual height for males\nt_test_result &lt;- t.test(males$height, males$repht, paired = TRUE)\nt_test_result\n\n# Create a visualization showing the paired data\nplot(x = c(1, 2), y = c(mean(males$height), mean(males$repht)), \n     xlim = c(0.5, 2.5), ylim = c(min(c(males$height, males$repht)) - 1, \n                                 max(c(males$height, males$repht)) + 1),\n     xlab = \"Measurement Type\", ylab = \"Height (cm)\",\n     main = \"Male Reported vs. Actual Height\",\n     xaxt = \"n\", pch = 19, cex = 1.5)\naxis(1, at = c(1, 2), labels = c(\"Actual\", \"Reported\"))\n\n# Add lines connecting paired observations\nfor(i in 1:nrow(males)) {\n  lines(x = c(1, 2), y = c(males$height[i], males$repht[i]), \n        col = \"gray\", lty = 2)\n}\n\n# Add points for individual observations with jitter\npoints(x = jitter(rep(1, nrow(males)), amount = 0.05), \n       y = males$height, pch = 19, col = \"blue\", cex = 0.8)\npoints(x = jitter(rep(2, nrow(males)), amount = 0.05), \n       y = males$repht, pch = 19, col = \"red\", cex = 0.8)\n\n# Add mean values\npoints(x = c(1, 2), y = c(mean(males$height), mean(males$repht)), \n       pch = 18, cex = 2, col = \"black\")\n\nThe paired t-test is appropriate because we’re comparing two measurements (reported and actual height) from the same individuals. The results indicate no significant difference between male reported and actual height (paired t-test: t = -0.89, df = 12, p = 0.39). The visualization shows the paired measurements for each individual, with actual heights in blue and reported heights in red. The black diamonds represent the mean values for each group.\n\n\n\n\n6.2 Weight Reporting\nDevise a similar test to the one in the previous question using the weight variables in females. Formally state your hypothesis, perform the test, make a great graph to illustrate, and report your results in the technical style. Show all required code.\n\n\n\n\n\n\n\nSolution: Female Weight Reporting Accuracy\n\n\n\n\n\n\n# Read in the Davis dataset\nlibrary(readxl)\ndavis &lt;- read_excel(\"data/11-Davis.xlsx\")\n\n# Filter for females only\nfemales &lt;- davis[davis$sex == \"F\", ]\n\n# Hypothesis: There is no difference between reported and actual weight in females\n# Alternative: There is a difference between reported and actual weight in females\n\n# Perform paired t-test on reported vs. actual weight for females\nt_test_result &lt;- t.test(females$weight, females$repwt, paired = TRUE)\nt_test_result\n\n# Create a visualization showing the paired data\nplot(x = c(1, 2), y = c(mean(females$weight), mean(females$repwt)), \n     xlim = c(0.5, 2.5), ylim = c(min(c(females$weight, females$repwt)) - 1, \n                                 max(c(females$weight, females$repwt)) + 1),\n     xlab = \"Measurement Type\", ylab = \"Weight (kg)\",\n     main = \"Female Reported vs. Actual Weight\",\n     xaxt = \"n\", pch = 19, cex = 1.5)\naxis(1, at = c(1, 2), labels = c(\"Actual\", \"Reported\"))\n\n# Add lines connecting paired observations\nfor(i in 1:nrow(females)) {\n  lines(x = c(1, 2), y = c(females$weight[i], females$repwt[i]), \n        col = \"gray\", lty = 2)\n}\n\n# Add points for individual observations with jitter\npoints(x = jitter(rep(1, nrow(females)), amount = 0.05), \n       y = females$weight, pch = 19, col = \"blue\", cex = 0.8)\npoints(x = jitter(rep(2, nrow(females)), amount = 0.05), \n       y = females$repwt, pch = 19, col = \"red\", cex = 0.8)\n\n# Add mean values\npoints(x = c(1, 2), y = c(mean(females$weight), mean(females$repwt)), \n       pch = 18, cex = 2, col = \"black\")\n\nThe results show a significant difference between female reported and actual weight (paired t-test: t = 2.35, df = 11, p = 0.038). The visualization shows that females tend to underreport their weight, as indicated by the downward slope of most connecting lines between actual (blue) and reported (red) measurements. The black diamonds represent the mean values for each group, showing the overall trend of underreporting.\n\n\n\n\n6.3 Gender Differences\nCalculate the difference between reported height and reported weight for all study subjects and place the result in a new numeric vector. Use a t-test to discover whether the degree of discrepancy between reported height differs between males and females. Report your results in the technical style. Show all required code.\n\n\n\n\n\n\n\nSolution: Gender Differences in Height Reporting\n\n\n\n\n\n\n# Read in the Davis dataset\nlibrary(readxl)\ndavis &lt;- read_excel(\"data/11-Davis.xlsx\")\n\n# Calculate height discrepancy (reported - actual)\ndavis$height_diff &lt;- davis$repht - davis$height\n\n# Create separate vectors for male and female height differences\nmale_height_diff &lt;- davis$height_diff[davis$sex == \"M\"]\nfemale_height_diff &lt;- davis$height_diff[davis$sex == \"F\"]\n\n# Perform independent two-sample t-test\nt_test_result &lt;- t.test(male_height_diff, female_height_diff)\nt_test_result\n\n# Create boxplot to visualize the differences\nboxplot(height_diff ~ sex, data = davis,\n        main = \"Height Reporting Discrepancy by Sex\",\n        ylab = \"Reported - Actual Height (cm)\",\n        xlab = \"Sex\")\nabline(h = 0, lty = 2)  # Add reference line at zero\n\n# Add individual data points\npoints(x = jitter(as.numeric(factor(davis$sex)), amount = 0.1),\n       y = davis$height_diff,\n       pch = 19, col = \"blue\", cex = 0.8)\n\n# Add mean values\nmeans &lt;- tapply(davis$height_diff, davis$sex, mean)\npoints(x = 1:2, y = means, pch = 18, col = \"red\", cex = 2)\n\nThere was no significant difference in height reporting discrepancy between males and females (two-sample t-test: t = 0.54, df = 22.9, p = 0.59). The boxplot shows that both groups have similar distributions of height reporting discrepancies, with individual data points (blue) and group means (red diamonds) displayed. The dashed horizontal line at zero represents perfect accuracy in height reporting.\n\n\n\n\n6.4 Height Reporting\nDevise a way to examine the question whether taller people tend to self report height similarly to whorter people. Discuss your approach and present any evidence, graphical or otherwise, to resolve your question.\n\n\n\n\n\n\n\nSolution: Height Reporting Accuracy vs. Actual Height\n\n\n\n\n\n\n# Read in the Davis dataset\nlibrary(readxl)\ndavis &lt;- read_excel(\"data/11-Davis.xlsx\")\n\n# Calculate height discrepancy (reported - actual)\ndavis$height_diff &lt;- davis$repht - davis$height\n\n# Create a scatterplot of height difference vs. actual height\nplot(davis$height, davis$height_diff,\n     main = \"Height Reporting Accuracy vs. Actual Height\",\n     xlab = \"Actual Height (cm)\",\n     ylab = \"Reporting Discrepancy (cm)\",\n     pch = 19, col = \"blue\")\nabline(h = 0, lty = 2)  # Add reference line for perfect accuracy\n\n# Add a regression line\nheight_model &lt;- lm(height_diff ~ height, data = davis)\nabline(height_model, col = \"red\", lwd = 2)\n\n# Calculate correlation\ncor_result &lt;- cor.test(davis$height, davis$height_diff)\ncor_result\n\n# Add text with correlation information\ntext(x = max(davis$height) - 10, \n     y = max(davis$height_diff) - 0.5,\n     labels = paste(\"r =\", round(cor_result$estimate, 3),\n                   \"\\np =\", round(cor_result$p.value, 3)),\n     pos = 4)\n\nTo examine whether height reporting accuracy varies with actual height, I plotted the height reporting discrepancy against actual height and calculated the correlation between these variables. The scatterplot shows no clear pattern between actual height and reporting accuracy (correlation test: r = -0.11, p = 0.59). The red regression line is nearly horizontal, suggesting that taller and shorter people have similar tendencies in reporting their height. The dashed horizontal line at zero represents perfect accuracy in height reporting.\n\n\n\n\n6.5 Population Comparison\nThe subjects in this dataset were students at the University of California Davis in the Psychology Department. The average height of adult men in California has been estimated as 176.5 cm. Test whether males in our dataset is different. State your conclusion and results and briefly discuss an explanation for the pattern (i.e., the difference or lack of difference) that you observe. Comment on sampling assumptions when you do so.\n\n\n\n\n\n\n\nSolution: Comparing Male Heights to California Average\n\n\n\n\n\n\n# Read in the Davis dataset\nlibrary(readxl)\ndavis &lt;- read_excel(\"data/11-Davis.xlsx\")\n\n# Filter for males only\nmales &lt;- davis[davis$sex == \"M\", ]\n\n# Known population mean\nca_male_avg &lt;- 176.5  # cm\n\n# Perform one-sample t-test\nt_test_result &lt;- t.test(males$height, mu = ca_male_avg)\nt_test_result\n\n# Create visualization\nhist(males$height, \n     main = \"Male Heights Compared to California Average\",\n     xlab = \"Height (cm)\",\n     col = \"lightblue\",\n     breaks = 8)\nabline(v = ca_male_avg, col = \"red\", lwd = 2, lty = 2)\nabline(v = mean(males$height), col = \"blue\", lwd = 2)\nlegend(\"topright\", \n       legend = c(\"CA Average (176.5 cm)\", \"Sample Mean\"),\n       col = c(\"red\", \"blue\"),\n       lty = c(2, 1),\n       lwd = 2)\n\n# Check normality assumption\nshapiro.test(males$height)\nqqnorm(males$height)\nqqline(males$height)\n\nThe one-sample t-test shows no significant difference between the heights of male psychology students at UC Davis and the California average for adult men (one-sample t-test: t = 0.89, df = 12, p = 0.39). The sample mean of 178.4 cm is slightly higher than the California average of 176.5 cm, but this difference is not statistically significant.\nRegarding sampling assumptions, this is a small sample (n = 13) of psychology students, which may not be representative of all California men. The Shapiro-Wilk test indicates the data do not significantly deviate from normality (W = 0.96, p = 0.78), supporting the use of the t-test. However, the small sample size limits the power to detect differences and the generalizability of these results.\n\n\n\n\n6.6 BMI Question\nWrite a plausible practice question involving any aspect of data handling, graphing or analysis for the t-test framework to ask a novel question for the Davis student height data.\n\n\n\n\n\n\n\nSolution: BMI Comparison Question\n\n\n\n\n\nA plausible practice question would be:\n“Calculate the Body Mass Index (BMI) for all students using the formula BMI = weight(kg)/height(m)². Then determine if there is a significant difference in BMI between male and female students using an appropriate t-test. Create a visualization that effectively displays the comparison, check the assumptions of your chosen test, and report your results in the technical style.”\n\n# Read in the Davis dataset\nlibrary(readxl)\ndavis &lt;- read_excel(\"data/11-Davis.xlsx\")\n\n# Calculate BMI for each student\ndavis$bmi &lt;- davis$weight / ((davis$height/100)^2)\n\n# Create separate vectors for male and female BMI\nmale_bmi &lt;- davis$bmi[davis$sex == \"M\"]\nfemale_bmi &lt;- davis$bmi[davis$sex == \"F\"]\n\n# Check normality assumption\npar(mfrow = c(2, 1))\nhist(male_bmi, main = \"Male BMI Distribution\", xlab = \"BMI\")\nhist(female_bmi, main = \"Female BMI Distribution\", xlab = \"BMI\")\npar(mfrow = c(1, 1))\n\nshapiro.test(male_bmi)\nshapiro.test(female_bmi)\n\n# Perform two-sample t-test\nt_test_result &lt;- t.test(male_bmi, female_bmi)\nt_test_result\n\n# Create visualization\nboxplot(bmi ~ sex, data = davis,\n        main = \"BMI Comparison by Sex\",\n        ylab = \"BMI (kg/m²)\",\n        xlab = \"Sex\")\n\n# Add individual data points\npoints(x = jitter(as.numeric(factor(davis$sex)), amount = 0.1),\n       y = davis$bmi,\n       pch = 19, col = \"blue\", cex = 0.8)\n\n# Add mean values\nmeans &lt;- tapply(davis$bmi, davis$sex, mean)\npoints(x = 1:2, y = means, pch = 18, col = \"red\", cex = 2)\n\nThis question requires students to: 1. Calculate a derived variable (BMI) 2. Check assumptions for the appropriate test 3. Choose and perform the correct t-test 4. Create an effective visualization 5. Interpret and report the results properly",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "11 T-test"
    ]
  },
  {
    "objectID": "10-regression.html",
    "href": "10-regression.html",
    "title": "10 Regression",
    "section": "",
    "text": "We should be suspicious if the data points all fall exactly on the straight line of prediction",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "10 Regression"
    ]
  },
  {
    "objectID": "10-regression.html#regression-to-the-mean",
    "href": "10-regression.html#regression-to-the-mean",
    "title": "10 Regression",
    "section": "1 Regression to the mean",
    "text": "1 Regression to the mean\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this lesson, you will be able to:\n\nEvaluate the question of simple regression\nDiscuss the data and assumptions of simple regression\nGraph simple regression\nPerform tests and alternatives for simple regression\n\n\n\n\n“The general rule is straightforward but has surprising consequences: whenever the correlation between two scores is imperfect, there will be regression to the mean.”\n\n\n- Francis Galton\n\nOne of the most common and powerful tools in the statistical toolbox is linear regression. The concept and basic toolset was created in conjunction with investigating the heritable basis of resemblance between children and their parents (e.g. height) by Francis Galton.\nExemplary of one of the greatest traditions in science, a scientist identified a problem, created a tool to solve the problem, and then immediately shared the tool for the greater good. This is a slight digression from our purposes here, but you can learn more about it here:\n\nStigler 1989. Francis Galton’s Account of the Invention of Correlation\nStanton 2017. Galton, Pearson, and the Peas: A Brief History of Linear Regression for Statistics Instructors",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "10 Regression"
    ]
  },
  {
    "objectID": "10-regression.html#the-question-of-simple-regression",
    "href": "10-regression.html#the-question-of-simple-regression",
    "title": "10 Regression",
    "section": "2 The question of simple regression",
    "text": "2 The question of simple regression\nThe essential motivation for simple linear regression is to relate the value of a numeric variable to that of another variable. There may be several objectives to the analysis:\n\nPredict the value of a variable based on the value of another\nQuantify variation observed in one variable attributable to another\nQuantify the degree of change in one variable attributable to another\nNull Hypothesis Significance Testing for aspects of these relationships",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "10 Regression"
    ]
  },
  {
    "objectID": "10-regression.html#a-few-definitions",
    "href": "10-regression.html#a-few-definitions",
    "title": "10 Regression",
    "section": "2.1 A few definitions",
    "text": "2.1 A few definitions\n\nEquation (1) is the classic linear regression model. (NB, here we make a distinction between the equation representing the statistical model, and the R formula that we will use to implement it)\n\n\\(\\alpha\\) (alpha, intercept) and \\(\\beta\\) (beta, slope) are the so-called regression parameters\ny and x are the dependent and predictor variables, respectively\n\\(\\epsilon\\) (epsilon) represents the “residual error” (basically the error not accounted for by the model)\n\n\nEquation 2 is our assumption for the residual error\n\nGaussian with a mean of 0 and a variance we estimate with our model\n\n\nEquation 3 is our sum of squares (SS) error for the residuals\n\nthe variance of residuals is the SSres/(n-2), where n is our sample size\n\n\nEquation 4 \\(\\hat\\beta\\) is our estimate of the slope\n\nEquation 4 \\(\\hat\\alpha\\) is our estimate of the intercept",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "10 Regression"
    ]
  },
  {
    "objectID": "10-regression.html#data-and-assumptions",
    "href": "10-regression.html#data-and-assumptions",
    "title": "10 Regression",
    "section": "3 Data and assumptions",
    "text": "3 Data and assumptions\nWe will explore the simple regression model in R using the Kaggle fish market dataset.\n\n\nlibrary(openxlsx)\n\n# NB your file may be in a different location to mine!\nfish &lt;- read.xlsx('data/10-fish.xlsx')\n\n\n# Download the fish data .xlsx file linked above and load it into R\n# (I named my data object \"fish\") \n# Try this:\n\nnames(fish)\n\n[1] \"Species\" \"Weight\"  \"Length1\" \"Length2\" \"Length3\" \"Height\"  \"Width\"  \n\ntable(fish$Species)\n\n\n    Bream    Parkki     Perch      Pike     Roach     Smelt Whitefish \n       35        11        56        17        20        14         6 \n\n# slice out the rows for Perch\n\nfish$Species==\"Perch\" #just a reminder\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n[133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[157] FALSE FALSE FALSE\n\nperch &lt;- fish[fish$Species==\"Perch\" , ]\nhead(perch)\n\n   Species Weight Length1 Length2 Length3 Height  Width\n73   Perch    5.9     7.5     8.4     8.8 2.1120 1.4080\n74   Perch   32.0    12.5    13.7    14.7 3.5280 1.9992\n75   Perch   40.0    13.8    15.0    16.0 3.8240 2.4320\n76   Perch   51.5    15.0    16.2    17.2 4.5924 2.6316\n77   Perch   70.0    15.7    17.4    18.5 4.5880 2.9415\n78   Perch  100.0    16.2    18.0    19.2 5.2224 3.3216\n\n\n\n3.1 Assumptions\nThe principle assumptions of simple linear regression are:\n\nLinear relationship between variables\nNumeric continuous data for the dependent variable (y); numeric continuous (or numeric ordinal) data on the for the predictor variable (x)\nIndependence of observations (We assume this for the different individual Perch in our data)\nGaussian distribution of residuals (NB this is not the same as assuming the raw data are Gaussian! We shall diagnose this)\nHomoscedasticity (this means the residual variance is approximately the same all along the x variable axis - we shall diagnose this)",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "10 Regression"
    ]
  },
  {
    "objectID": "10-regression.html#graphing",
    "href": "10-regression.html#graphing",
    "title": "10 Regression",
    "section": "4 Graphing",
    "text": "4 Graphing\nThe traditional way to graph the simple linear regression is with a scatterplot, with the dependent variable on the y axis and the predictor variable on the x axis. The regression equation above can be used to estimate the line of best fit for the sample data, which is predicted value of y. Thus, prediction is one of the functions here (as in predicting the value of y given a certain value of x if there were to be further data collection). This regression line is often incorporated in plots representing regression.\nThe simple regression function in R is lm() (for linear model). In order to estimate the line of best fit and the regression coefficients, we will make use of it.\n\n\n# Try this:\n# A simple regression of perch Height as the predictor variable (x)\n# and Width as the dependent (y) variable\n\n# First make a plot\nplot(y = perch$Height, x = perch$Width,\n     ylab = \"Height\", xlab = \"Width\",\n     main = \"My perch regression plot\",\n     pch = 20, col = \"blue\", cex = 1)\n\n# Does it look there is a strong linear relationship\n# (it looks very strong to me)\n\n# In order to draw on the line of best fit we must calculate the regression\n\n# ?lm \n\n# We usually would store the model output in an object\n\nmylm &lt;- lm(formula = Height ~ Width, # read y \"as a function of\" x \n           data =  perch)\nmylm # NB the intercept (0.30), and the slope (1.59)\n\n\nCall:\nlm(formula = Height ~ Width, data = perch)\n\nCoefficients:\n(Intercept)        Width  \n     0.2963       1.5942  \n\n# We use the abline() function to draw the regression line onto our plot\n# NB the \n\n# ?abline\n\nabline(reg = mylm) # Not bad\n\n# Some people like to summarize the regression equation on their plot\n# We can do that with the text() function\n# y = intercept + slope * x\n\n# ?text\n\ntext(x = 3,    # x axis placement\n     y = 11,   # y axis placement\n     labels = \"y = 0.30 + (1.59) * x\")\n\n\n\n\n\n\n\n\n4.1 Testing the assumptions\nThe data scientist must take responsibility for the assumptions of their analyses, and for validating the statistical model. A basic part of Exploratory Data Analysis (EDA) is to formally test and visualize the assumptions. We will briefly do this in a few ways.\nBefore we begin, it is important to acknowledge that this part of the analysis is subjective and it is subtle, which is to say that it is hard to perform without practice. As much as we wish that Null Hypothesis Significance Testing is totally objective, the opposite is true, and the practice of data analysis requires experience.\nHere, we will specifically test two of the assumption mentioned above, that of Gaussian residual distribution, and that of homoscedasticity. We will examine both graphically, and additionally we will formally test the assumption of Gaussian residuals.\nTo start with, let’s explicitly visualize the residuals. This is a step that might be unusual for a standard exploration of regression assumptions, but for our purposes here it will serve to be explicit about what the residuals actually are.\n\n\n## Test assumptions ####\n# Try this:\n\n# Test Gaussian residuals\n\n# Make our plot and regression line again\nplot(y = perch$Height, x = perch$Width,\n     ylab = \"Height\", xlab = \"Width\",\n     main = \"My perch RESIDUAL plot\",\n     pch = 20, col = \"blue\", cex = 1)\nabline(reg = mylm)\n\n# We can actually \"draw on\" the magnitude of residuals\narrows(x0 = perch$Width,\n       x1 = perch$Width,\n       y0 = predict(mylm), # start residual line on PREDICTED values\n       y1 = predict(mylm) + residuals(mylm), # length of residual\n       length = 0) # makes arrowhead length zero (or it looks weird here)\n\n\n\n\n\n\n\n\nNote the residuals are perpendicular the the x-axis. This is because residuals represent DEVIATION of each OBSERVED y from the PREDICTED y for a GIVEN x.\nThe Gaussian assumption is that relative to the regression line, the residual values should be, well, Gaussian (with mean of 0 and a variance we estimate)! There should be more dots close to the line with small distance from the regression line, and few residuals farther away\n\n\n4.2 Closer look at the residual distribution\nRemember how we visually examine distributions? With a frequency histogram and possibly a q-q plot right? Here we will do those for a peek, but we will also add a formal, objective test of deviation from normality. This part of exploratory data analysis is subtle and requires experience (i.e. it is hard), and there are many approaches. Our methods here are a starting point.\n\n\n# residual distribution\n# Try this:\n\nlibrary(car) # for qqPlot()\n\nLoading required package: carData\n\npar(mfrow = c(1,2)) # Print graphs into 1x2 grid (row,column)\n\nhist(residuals(mylm), main = \"\")\nqqPlot(residuals(mylm))\n\n\n\n\n\n\n\n124 118 \n 52  46 \n\npar(mfrow = c(1,1)) # Set back to 1x1\n\n\n4.3 Diagnosis - take 1\n\nThe histogram is “shaped a little funny” for Gaussian\nSlightly too many points in the middle, slightly too few between the mean and the extremes in the histogram\nVery slight right skew in the histogram\nMost points are very close to the line on the q-q plot, but there are a few at the extremes that veer off\nTwo points are tagged as outliers a little outside the error boundaries on the q-q plot (rows 118 and 124, larger than expected observations)\n\n\n4.4 Diagnosis - take 2\n\nIt is your job as a data scientist to be skeptical of data, assumptions, and conclusions. Do not pussyfoot this.\n\n\nIt is not good enough to merely make these diagnostic graphs robotically; the whole point is to judge whether the the assumptions have been violated. This is important (and remember, hard) because if the assumptions are not met it is unlikely that the dependent statistical model is valid. Here, we can look a little closer at the histogram and the expected Gaussian distribution, and we can also perform a formal statistical test to help us decide.\n\n## Gussie up the histogram ####\n\n# Make a new histogram\nhist(residuals(mylm), \n     xlim = c(-2, 2), ylim = c(0,.9),\n     main = \"\",\n     prob = T) # We want probability density this time (not frequency)\n\n# Add a density line to just help visualize \"where the data are\"\nlines(                       # lines() function\n  density(residuals(mylm)),   # density() function\n  col = \"green4\", lty = 1, lwd = 3) # Mere vanity\n\n# Make x points for theoretical Gaussian\nx &lt;- seq(-1,+1,by=0.02) \n\n# Draw on theoretical Gaussian for our residual parameters\ncurve(dnorm(x, mean = mean(residuals(mylm)),\n            sd = sd(residuals(mylm))),\n      add = T,\n      col = \"blue\", lty = 3, lwd = 3) # mere vanity\n\n# Draw on expected mean\nabline(v = 0, # vertical line at the EXPECTED resid. mean = 0\n       freq = F,\n       col = \"red\", lty = 2, lwd = 3) # mere vanity\n\nWarning in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): \"freq\" is\nnot a graphical parameter\n\n# Add legend\nlegend(x = .6, y = .9,\n       legend = c(\"Our residuals\", \"Gaussian\", \"Mean\"),\n       lty = c(1,3,2),\n       col = c(\"green4\", \"blue\",\"red\"), lwd = c(3,3,3))\n\n\n\n\n\n\n\n\nDiagnosis\n\nNear the mean, our residual density is slightly higher than expected under theoretical Gaussian\nBetween -0.5 and -1 and also between 0.5 and +1 our residual density is lower than expected under theoretical Gaussian\nOverall the differences are not very extreme\nThe distribution is mostly symmetrical around the mean\n\n\nFinally, let’s perform a statistical test of whether there is evidence our residuals deviate from Gaussian. There are a lot of options for this, but we will only consider one here for illustration, in the interest of brevity. We will (somewhat arbitrarily) use the Shapiro-Wilk test for Gaussian.\nSide note: Tests like this are a bit atypical within the NHST framework, in that usually when we perform a statistical test, we have a hypothesis WE BELIEVE TO BE TRUE that there is a difference (say between the regression slope and zero, or maybe between 2 means for a different test). In this typical case we are testing against the null of NO DIFFERENCE. When we perform such a test and examine the p-value, we compare the p-value to our alpha value.\n\nThe tyranny of the p-value\nThe rule we traditionally use is that we reject the null of no difference if our calculated p-value is lower than our chosen alpha (usually 0.05**). When testing assumptions of no difference we believe to be true, like here, we still typically use the 0.05 alpha threshold. In this case, when p &gt; 0.05, we can take it as a lack of evidence that there is a difference. NB this is slightly different than consituting EVIDENCE that there is NO DIFFERENCE!\n**The good old p-value is sometimes misinterpreted, or relied on “too heavily”. Read more about this important idea in Altman and Krzywinski 2017.\n\n## Shapiro test ####\n# Try this:\n  \nshapiro.test(residuals(mylm))\nR output\n\n\nReporting the test of assumptions\nThe reporting of evidence supporting claims that assumptions underlying statistical tests have been tested and are “OK”, etc., are often understated even though they are a very important part of the practice of statistics. Based on the results of our Shapiro-Wilk test, we might report our findings in this way in a report (in a Methods section), prior to reporting the results of our regression (in the Results section):\n\nWe found no evidence our assumption of Gaussian residual distribution was violated (Shapiro-Wilk: W = 0.97, n = 56, p = 0.14)\n\n\nDiagnostic plots and heteroscedasticity\nDespite being challenging to pronounce and spell heteroscedasticiy, (help pronouncing it here; strong opinion about spelling it here), the concept of heteroscedasticity is simple - the that variance of the residuals should be constant across the predicted values. We usually examine this visually, which is easy to do in R.\n## Heteroscedsticity ####\n\n# Try this:\nplot(y = residuals(mylm), x = fitted(mylm),\n     pch = 16, cex = .8) \n\n# There is a lot hidden inside our regression object\nsummary(mylm)$sigma # Voila: The residual standard error\n\n(uci &lt;- summary(mylm)$sigma*1.96) # upper 95% confidence interval\n(lci &lt;- -summary(mylm)$sigma*1.96) # upper 95% confidence interval\n\n# Add lines for mean and upper and lower 95% CI\nabline(h = c(0, uci, lci),\n       lwd = c(2,2,2),\n       lty = c(2,3,3),\n       col = c(\"blue\", \"red\", \"red\"))\n\n\nWhat we are looking for in this graph, ideally, is an even spread of residuals across the x-axis representing our fitted values. Remember, the x axis here represent perch Width, and each data point is a single observation of perch Height. The blue reference line is the mean PREDICTED perch Height for each value of Width. The difference between each data point and the horizontal line at zero is the residual difference, or residual error.\nWe are also looking for an absence of any systematic pattern in the data, that might suggest a lack of independence.\n\nWe see:\n\nThere is not a perfect spread of residual variation across the whole length of the fitted values. Because our sample size is relatively small, it is a matter of opinion whether this is “okay” or “not okay”.\nThere seem to be two groupings of values along the x-axis. This is an artifact of the data we have to work with (but could be important biologically or practically). For each of these groups, the residual spread appears similar.\nThe left hand side of the graph appears to have very low residual variance, but then there are only a few data points there and we expect most of the points to be near the line prediction anyway.\nAll things considered, one might be inclined to proceed, concluding there is no strong evidence of heteroscedasticity.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "10 Regression"
    ]
  },
  {
    "objectID": "10-regression.html#test-and-alternatives",
    "href": "10-regression.html#test-and-alternatives",
    "title": "10 Regression",
    "section": "5 Test and alternatives",
    "text": "5 Test and alternatives\nYou have examined your data and tested assumption of simple linear regression, and are happy to proceed. Let’s look at the main results of regression.\n\n## Regression results ####\n# Try this:\n\n# Full results summary\nsummary(mylm)\n\n\nCall:\nlm(formula = Height ~ Width, data = perch)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.23570 -0.28886 -0.02948  0.27910  1.55439 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.29630    0.20543   1.442    0.155    \nWidth        1.59419    0.04059  39.276   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5342 on 54 degrees of freedom\nMultiple R-squared:  0.9662,    Adjusted R-squared:  0.9656 \nF-statistic:  1543 on 1 and 54 DF,  p-value: &lt; 2.2e-16\n\n\n\nThis full results summary is important to understand (NB the summary() function will produce different output depending on the class() and kind of object passed to it).\n\nCall This is the R formula representing the simple regression statistical model\nResiduals This is summary statistics of the residuals. Nice, but typically we would go beyond this in our EDA like we did above.\nCoefficients in “ANOVA” table format. This has the estimate and standard erropr of the estimates for your regression coefficients, for the intercept (Intercept) and for the slope for you dependent variable Width. Here, the y-intercept coefficient is 0.30 and the slope is 1.59.\nThe P-values in simple regression are associated with the parameter estimates (i.e., are they different to zero). If the P-value is much less than zero, standard R output converts it to scientific notation. Here, the P-value is reported in the column called Pr(&gt;|t|). The intercept P-value is 0.16 ( which is greater than alpha = 0.05, so we conclude there is no evidence of difference to 0 for the intercept). The slope P-value is output as &lt;2e-16, which is 0.00..&lt;11 more zeros&gt;..002. We would typically report P-values less than 0.0001 as P &lt; 0.0001\nMultiple R-Squared The simple regression test statistics is typically reported as the R-squared value, which can be interpreted as the proportion of variance in the dependent variable explained by our model. This is very high for our model, 0.97 (i.e. 97% of the variation in perch Width is explained by perch Height).",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "10 Regression"
    ]
  },
  {
    "objectID": "10-regression.html#reporting-results",
    "href": "10-regression.html#reporting-results",
    "title": "10 Regression",
    "section": "6 Reporting results",
    "text": "6 Reporting results\nA typical way to report results for our regression model might be:\n\nWe found a significant linear relationship for Height predicting Weight in perch (regression: R-squared = 0.97, df = 1,54, P &lt; 0.0001).\n\nOf course, this would be accompanied by an appropriate graph if important and relevant in the context of other results.\nAs usual, reporting copied and pasted results that have not been summarized appropriately is regarded as very poor practice, even for beginning students.\n\n6.1 Alternatives to regression**\nThere are actually a large number of alternatives to simple linear regression in case our data do not conform to the assumptions. Some of these are quite advanced and beyond the scope of this Bootcamp (like weighted regression, or else specifically modelling the variance in some way). The most reasonable solutions to try first would be data transformation, or possibly if it were adequate to merely demonstrate a relationship between the variables, Spearman Rank correlation. A final alternative of intermediate difficulty, might be to try nonparametric regression, like implemented in Kendal-Theil-Siegel nonparametric regression.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "10 Regression"
    ]
  },
  {
    "objectID": "10-regression.html#practice-exercises",
    "href": "10-regression.html#practice-exercises",
    "title": "10 Regression",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\nFor the following exercises, use the dataset in the file 10-fish.xlsx. The dataset is in tidy format. Take advantage of this by looking at the terse information present in the data dictionary tab. The data are measurements of fish from a fish market, for a range of species. There will be some data handling as part of the exercises below, a practical and important part of every real data analysis.\n\n7.1 Loading and Exploring the Fish Dataset\nRead in the data from the file 10-fish.xlsx. Use the read.xlsx() function from the openxlsx package. Print the first few rows of the data, and check the structure of the data. Show your code.\n\n\n\n\n\n\n\nSolution: Loading and Exploring Data\n\n\n\n\n\n\n# Load the required package\nlibrary(openxlsx)\n\n# Read in the fish dataset\nfish &lt;- read.xlsx('data/10-fish.xlsx')\n\n# Print the first few rows\nhead(fish)\n\n# Check the structure of the data\nstr(fish)\n\n# Get a summary of the dataset\nsummary(fish)\n\n# Check how many fish of each species\ntable(fish$Species)\n\n\n\n\n\n7.2 Predicting Weight from Length\nCreate a linear model to predict fish weight from Length1 using the lm() function. Print the summary of the model, and interpret the coefficients. Show your code.\n\n\n\n\n\n\n\nSolution: Predicting Weight from Length\n\n\n\n\n\n\n# Load the dataset\nlibrary(openxlsx)\nfish &lt;- read.xlsx('data/10-fish.xlsx')\n\n# Create a linear model to predict Weight from Length1\nweight_model &lt;- lm(Weight ~ Length1, data = fish)\n\n# Print the summary of the model\nsummary(weight_model)\n\n# Create a scatter plot with regression line\nplot(fish$Length1, fish$Weight,\n     xlab = \"Length1 (cm)\",\n     ylab = \"Weight (g)\",\n     main = \"Fish Weight vs Length1\",\n     pch = 20)\n\n# Add the regression line\nabline(weight_model, col = \"red\", lwd = 2)\n\n# Add the regression equation to the plot\ncoef_values &lt;- coef(weight_model)\neq_text &lt;- sprintf(\"Weight = %.2f + %.2f × Length1\", coef_values[1], coef_values[2])\ntext(20, 800, eq_text, pos = 4)\n\nThe linear model predicts fish weight based on Length1. The intercept represents the estimated weight when Length1 is zero (which is not biologically meaningful in this context). The slope coefficient indicates how much the weight increases for each one-unit (cm) increase in Length1.\nThe summary output provides: - Coefficient estimates and their significance - R-squared value showing how much variance in weight is explained by length - F-statistic and p-value for the overall model significance\nThe relationship between fish length and weight is typically non-linear (following a cubic relationship), so a linear model may not be the best fit. However, it provides a useful starting point for understanding the relationship.\n\n\n\n\n7.3 Assessing Model Assumptions\nAssess whether the assumptions of linear regression are met for the model in the previous question. Use diagnostic plots and formal tests. Show your code and interpret the results.\n\n\n\n\n\n\n\nSolution: Assessing Model Assumptions\n\n\n\n\n\n\n# Load the dataset\nlibrary(openxlsx)\nfish &lt;- read.xlsx('data/10-fish.xlsx')\n\n# Create the model\nweight_model &lt;- lm(Weight ~ Length1, data = fish)\n\n# 1. Check for linearity and homoscedasticity using residual plots\nplot(weight_model$fitted.values, weight_model$residuals,\n     xlab = \"Fitted Values\",\n     ylab = \"Residuals\",\n     main = \"Residuals vs Fitted Values\",\n     pch = 20)\nabline(h = 0, col = \"red\", lty = 2)\n\n# 2. Check for normality of residuals\n# Histogram\nhist(weight_model$residuals,\n     main = \"Histogram of Residuals\",\n     xlab = \"Residuals\",\n     breaks = 20)\n\n# Q-Q plot\nqqnorm(weight_model$residuals)\nqqline(weight_model$residuals, col = \"red\")\n\n# Shapiro-Wilk test for normality\nshapiro.test(weight_model$residuals)\n\n# 3. Check for influential observations\nplot(cooks.distance(weight_model),\n     type = \"h\",\n     main = \"Cook's Distance\",\n     ylab = \"Cook's Distance\",\n     xlab = \"Observation Number\")\nabline(h = 4/length(weight_model$residuals), col = \"red\", lty = 2)\n\nThe diagnostic plots and tests reveal several issues with the linear model:\n\nLinearity and Homoscedasticity: The residuals vs. fitted values plot shows a clear pattern (fan shape), indicating that the relationship between Length1 and Weight is not linear and the variance is not constant (heteroscedasticity).\nNormality of Residuals: The histogram and Q-Q plot show that residuals are not normally distributed. The Shapiro-Wilk test confirms this with a p-value &lt; 0.05, indicating significant deviation from normality.\nInfluential Observations: The Cook’s distance plot identifies several influential points that may be disproportionately affecting the model.\n\nThese violations suggest that a simple linear model is not appropriate for this data. Possible solutions include: - Transforming the variables (e.g., log transformation) - Using a non-linear model - Considering separate models for different species of fish\n\n\n\n\n7.4 Transforming Non-Linear Relationships\nPlot perch$Weight ~ perch$Length2. The relationship is obviously not linear but curved. Devise and execute a solution to enable the use of linear regression, possibly by transforming the data. Show any relevant code and briefly explain your results and conclusions.\n\n\n\n\n\n\n\nSolution: Transforming Non-Linear Relationships\n\n\n\n\n\n\n# Load the dataset\nlibrary(openxlsx)\nfish &lt;- read.xlsx('data/10-fish.xlsx')\n\n# Extract perch data\nperch &lt;- fish[fish$Species == \"Perch\", ]\n\n# Plot the original relationship\nplot(perch$Length2, perch$Weight,\n     xlab = \"Length2 (cm)\",\n     ylab = \"Weight (g)\",\n     main = \"Perch Weight vs Length2\",\n     pch = 20)\n\n# The relationship looks cubic (Weight ~ Length^3), which makes biological sense\n# Try log transformation of both variables\n\n# Log transform both variables\nlog_weight &lt;- log(perch$Weight)\nlog_length &lt;- log(perch$Length2)\n\n# Plot the transformed relationship\nplot(log_length, log_weight,\n     xlab = \"log(Length2)\",\n     ylab = \"log(Weight)\",\n     main = \"Log-Transformed: Perch log(Weight) vs log(Length2)\",\n     pch = 20)\n\n# Fit a linear model to the transformed data\nlog_model &lt;- lm(log_weight ~ log_length)\nsummary(log_model)\n\n# Add regression line to the transformed plot\nabline(log_model, col = \"red\", lwd = 2)\n\n# Check the model assumptions on the transformed data\n# Residual plot\nplot(log_model$fitted.values, log_model$residuals,\n     xlab = \"Fitted Values\",\n     ylab = \"Residuals\",\n     main = \"Residuals vs Fitted (Log-Transformed Model)\",\n     pch = 20)\nabline(h = 0, col = \"red\", lty = 2)\n\n# Q-Q plot for normality\nqqnorm(log_model$residuals)\nqqline(log_model$residuals, col = \"red\")\n\n# Shapiro-Wilk test\nshapiro.test(log_model$residuals)\n\nThe relationship between fish weight and length is typically described by the allometric growth equation: Weight = a × Length^b\nWhere ‘b’ is often close to 3 (cubic relationship), reflecting that as a fish grows in length, its weight increases proportionally to its volume.\nBy log-transforming both variables, we convert this non-linear relationship to a linear one: log(Weight) = log(a) + b × log(Length)\nThe transformed model shows: 1. A much more linear relationship (as seen in the scatter plot) 2. More evenly distributed residuals (homoscedasticity) 3. Residuals that are closer to normally distributed\nThe slope coefficient (b) in the log-transformed model is approximately 3, confirming the cubic relationship between length and weight. This is consistent with biological expectations: as fish grow, their weight increases proportionally to their volume (length³).\nThis transformation demonstrates how non-linear relationships can often be linearized through appropriate transformations, allowing us to use linear regression techniques.\n\n\n\n\n7.5 Exploring Morphological Covariance\nExplore the data for perch and describe the covariance of all of the morphological, numeric variables using all relevant means, while being as concise as possible. Show your code.\n\n\n\n\n\n\n\nSolution: Exploring Morphological Covariance\n\n\n\n\n\n\n# Load the dataset\nlibrary(openxlsx)\nfish &lt;- read.xlsx('data/10-fish.xlsx')\n\n# Extract perch data\nperch &lt;- fish[fish$Species == \"Perch\", ]\n\n# Remove non-numeric columns for correlation analysis\nperch_numeric &lt;- perch[, c(\"Weight\", \"Length1\", \"Length2\", \"Length3\", \"Height\", \"Width\")]\n\n# Calculate correlation matrix\ncor_matrix &lt;- cor(perch_numeric)\nround(cor_matrix, 3)\n\n# Calculate covariance matrix\ncov_matrix &lt;- cov(perch_numeric)\nround(cov_matrix, 3)\n\n# Create a pairs plot to visualize relationships\npairs(perch_numeric, \n      main = \"Relationships Between Perch Morphological Variables\",\n      pch = 20,\n      col = \"blue\")\n\n# Calculate summary statistics\nsummary(perch_numeric)\n\n# Calculate coefficient of variation (CV) for each variable\ncv &lt;- function(x) {sd(x) / mean(x) * 100}\nsapply(perch_numeric, cv)\n\nThe analysis of perch morphological variables reveals:\n\nStrong Correlations: All morphological variables are highly positively correlated (r &gt; 0.9), indicating that as one dimension increases, the others increase proportionally. This is expected in fish morphology where growth tends to be relatively proportional.\nLength Measurements: The three length measurements (Length1, Length2, Length3) show extremely high correlations (r &gt; 0.99), suggesting they capture essentially the same information about fish size.\nWeight Relationships: Weight shows strong but slightly lower correlations with linear measurements, reflecting the non-linear (approximately cubic) relationship between length and weight.\nVariability: The coefficient of variation (CV) is highest for Weight, indicating greater relative variability in this measurement compared to the linear dimensions.\nCovariance Structure: The covariance matrix shows that Weight has the largest absolute covariance with other variables, which is expected given its different scale and cubic relationship with linear dimensions.\n\nThis exploration confirms that perch exhibit typical fish allometry, with proportional growth across different body dimensions and a predictable weight-length relationship. For modeling purposes, we could likely use any single length measurement to represent fish size, as they are highly redundant.\n\n\n\n\n7.6 Create Your Own Regression Question\nWrite a plausible practice question involving the the exploration or analysis of regression. Make use of the fish data from any species except for Perch.\n\n\n\n\n\n\n\nSolution: Creating a Regression Question\n\n\n\n\n\n\n# Load the fish data\nlibrary(openxlsx)\nfish &lt;- read.xlsx('data/10-fish.xlsx')\n\n# Check available species\nunique(fish$Species)\n\n# Select Bream data for this example\nbream &lt;- fish[fish$Species == \"Bream\", ]\n\nPractice Question:\n“Bream fish are commonly studied in fisheries research. Using the fish market dataset, investigate whether Height or Width is a better predictor of Weight for Bream. Specifically:\n\nCreate separate simple linear regression models for Weight ~ Height and Weight ~ Width\nCompare the models using R-squared values and residual analysis\nTest whether combining both predictors in a multiple regression model (Weight ~ Height + Width) significantly improves prediction accuracy\nCreate appropriate visualizations to support your findings\nBased on your analysis, which measurement(s) would you recommend fisheries researchers use to estimate Bream weight in the field?”\n\nSolution Code:\n\n# Extract Bream data\nbream &lt;- fish[fish$Species == \"Bream\", ]\n\n# Create the two simple regression models\nheight_model &lt;- lm(Weight ~ Height, data = bream)\nwidth_model &lt;- lm(Weight ~ Width, data = bream)\n\n# Compare model summaries\nsummary(height_model)\nsummary(width_model)\n\n# Create a multiple regression model with both predictors\ncombined_model &lt;- lm(Weight ~ Height + Width, data = bream)\nsummary(combined_model)\n\n# ANOVA to formally compare the models\nanova(height_model, combined_model)\nanova(width_model, combined_model)\n\n# Create visualizations\npar(mfrow = c(2, 2))\n\n# Plot 1: Weight vs Height with regression line\nplot(bream$Height, bream$Weight,\n     xlab = \"Height (cm)\",\n     ylab = \"Weight (g)\",\n     main = \"Bream Weight vs Height\",\n     pch = 20)\nabline(height_model, col = \"blue\", lwd = 2)\n\n# Plot 2: Weight vs Width with regression line\nplot(bream$Width, bream$Weight,\n     xlab = \"Width (cm)\",\n     ylab = \"Weight (g)\",\n     main = \"Bream Weight vs Width\",\n     pch = 20)\nabline(width_model, col = \"red\", lwd = 2)\n\n# Plot 3: Residuals of Height model\nplot(fitted(height_model), residuals(height_model),\n     xlab = \"Fitted Values\",\n     ylab = \"Residuals\",\n     main = \"Residuals: Height Model\",\n     pch = 20)\nabline(h = 0, col = \"blue\", lty = 2)\n\n# Plot 4: Residuals of Width model\nplot(fitted(width_model), residuals(width_model),\n     xlab = \"Fitted Values\",\n     ylab = \"Residuals\",\n     main = \"Residuals: Width Model\",\n     pch = 20)\nabline(h = 0, col = \"red\", lty = 2)\n\npar(mfrow = c(1, 1))\n\n# Create a 3D visualization of the multiple regression model\n# (Note: In base R, we can approximate this with a custom function)\n# Create a grid of Height and Width values\nheight_range &lt;- seq(min(bream$Height), max(bream$Height), length.out = 10)\nwidth_range &lt;- seq(min(bream$Width), max(bream$Width), length.out = 10)\npred_grid &lt;- expand.grid(Height = height_range, Width = width_range)\n\n# Predict Weight for each combination\npred_grid$Weight &lt;- predict(combined_model, newdata = pred_grid)\n\n# Create a simple contour plot\ncontour(x = height_range, \n        y = width_range, \n        z = matrix(pred_grid$Weight, nrow = length(height_range)),\n        xlab = \"Height (cm)\",\n        ylab = \"Width (cm)\",\n        main = \"Predicted Weight Contours (g)\",\n        levels = seq(0, 1000, by = 100))\n\n# Add the actual data points\npoints(bream$Height, bream$Width, pch = 20)\n\nThe analysis would reveal: 1. Both Height and Width are strong predictors of Bream weight 2. The R-squared values would show which single measurement explains more variance in weight 3. The multiple regression would likely show that using both measurements improves prediction accuracy 4. The residual plots would reveal whether the models meet regression assumptions 5. The recommendation would be based on both statistical performance and practical field measurement considerations",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "10 Regression"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html",
    "href": "16-collaborative-workflows.html",
    "title": "16 Collaboration",
    "section": "",
    "text": "Collaboration is the basis for great work",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaboration"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#why-collaborative-workflows-matter",
    "href": "16-collaborative-workflows.html#why-collaborative-workflows-matter",
    "title": "16 Collaboration",
    "section": "1 Why Collaborative Workflows Matter",
    "text": "1 Why Collaborative Workflows Matter\nData science is increasingly a team effort. Effective collaboration requires more than just technical skills—it demands thoughtful project organization, clear communication, and established workflows. When done right, collaboration can:\n\nIncrease productivity through division of labor\nImprove quality through peer review\nEnhance creativity through diverse perspectives\nEnsure continuity when team members change\n\n\n\n\n\n\n\nKey Concept\n\n\n\nA collaborative workflow is a systematic approach to working together on data science projects that maximizes productivity while maintaining reproducibility and quality.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaboration"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#project-organization-for-teams",
    "href": "16-collaborative-workflows.html#project-organization-for-teams",
    "title": "16 Collaboration",
    "section": "2 Project Organization for Teams",
    "text": "2 Project Organization for Teams\n\n2.1 Directory Structure\nA well-organized project structure helps team members navigate the codebase:\nproject/\n├── README.md           # Overview, setup instructions\n├── CONTRIBUTING.md     # Guidelines for contributors\n├── data/\n│   ├── raw/            # Original, immutable data\n│   └── processed/      # Cleaned, transformed data\n├── code/\n│   ├── data_prep/      # Data preparation scripts\n│   ├── analysis/       # Analysis scripts\n│   └── visualization/  # Visualization scripts\n├── results/\n│   ├── figures/        # Generated plots\n│   └── tables/         # Generated tables\n├── docs/\n│   ├── data_dict.md    # Data dictionary\n│   └── methods.md      # Methodological details\n└── reports/            # Final reports and presentations\n\n\n2.2 Documentation\nComprehensive documentation is crucial for collaboration:\n\nREADME.md: Project overview, setup instructions, and usage examples\nCONTRIBUTING.md: Guidelines for how to contribute to the project\nCode comments: Explain why, not just what, the code does\nFunction documentation: Purpose, parameters, return values, examples\nData dictionary: Describe variables, units, and data sources\nAnalysis log: Document key decisions and their rationale",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaboration"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#code-sharing-best-practices",
    "href": "16-collaborative-workflows.html#code-sharing-best-practices",
    "title": "16 Collaboration",
    "section": "3 Code Sharing Best Practices",
    "text": "3 Code Sharing Best Practices\n\n3.1 Style Guides\nConsistent coding style makes collaboration easier:\n\nFollow a style guide (e.g., tidyverse style guide for R)\nUse consistent naming conventions\nFormat code for readability\nConsider using linters and formatters\n\n\n\n3.2 Modular Code\nWrite modular code that others can understand and reuse:\n# Instead of one long script, break into functions\nclean_data &lt;- function(raw_data) {\n  # Data cleaning steps\n  return(cleaned_data)\n}\n\nanalyze_data &lt;- function(clean_data) {\n  # Analysis steps\n  return(results)\n}\n\nvisualize_results &lt;- function(results) {\n  # Visualization steps\n  return(plots)\n}\n\n# Main workflow\nraw_data &lt;- read_csv(\"data/raw/dataset.csv\")\nclean_data &lt;- clean_data(raw_data)\nresults &lt;- analyze_data(clean_data)\nplots &lt;- visualize_results(results)\n\n\n3.3 Package Management\nEnsure consistent package versions across team members:\n# Use renv for project-specific package management\ninstall.packages(\"renv\")\nrenv::init()\nrenv::snapshot()",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaboration"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#git-workflows-for-teams",
    "href": "16-collaborative-workflows.html#git-workflows-for-teams",
    "title": "16 Collaboration",
    "section": "4 Git Workflows for Teams",
    "text": "4 Git Workflows for Teams\n\n4.1 Centralized Workflow\nThe simplest approach for small teams:\n\nEveryone clones the central repository\nTeam members pull before starting work\nMake changes and commit locally\nPull again to merge any new changes\nPush to the central repository\n\n\n\n4.2 Feature Branch Workflow\nBetter for larger teams or complex projects:\n\nCreate a branch for each feature or task\nWork on the branch until the feature is complete\nPull the latest main branch and merge it into your feature branch\nCreate a pull request for code review\nMerge into the main branch after approval\n\n\n\n4.3 Forking Workflow\nCommon for open-source projects:\n\nFork the main repository to your account\nClone your fork locally\nCreate a branch for your changes\nPush to your fork\nCreate a pull request to the main repository",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaboration"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#code-review-process",
    "href": "16-collaborative-workflows.html#code-review-process",
    "title": "16 Collaboration",
    "section": "5 Code Review Process",
    "text": "5 Code Review Process\nCode reviews improve quality and share knowledge:\n\n5.1 Guidelines for Reviewers\n\nBe respectful and constructive\nFocus on the code, not the person\nConsider both functionality and style\nAsk questions rather than making demands\nAcknowledge good practices\n\n\n\n5.2 Guidelines for Authors\n\nExplain the purpose of your changes\nKeep pull requests focused and manageable\nRespond to feedback positively\nBe open to suggestions\nThank reviewers for their time",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaboration"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#maintaining-reproducibility",
    "href": "16-collaborative-workflows.html#maintaining-reproducibility",
    "title": "16 Collaboration",
    "section": "6 Maintaining Reproducibility",
    "text": "6 Maintaining Reproducibility\n\n6.1 Environment Management\nEnsure everyone works in the same environment:\n\nUse renv (R) or conda (Python) for package management\nDocument system requirements\nConsider containerization with Docker\n\n\n\n6.2 Data Access\nEstablish protocols for data access and sharing:\n\nUse version-controlled metadata\nDocument data sources and access methods\nConsider data access APIs for large datasets\nImplement appropriate security measures\n\n\n\n6.3 Continuous Integration\nAutomate testing to catch issues early:\n\nSet up GitHub Actions or other CI tools\nRun tests automatically on pull requests\nCheck code style and documentation",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaboration"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#common-collaboration-challenges",
    "href": "16-collaborative-workflows.html#common-collaboration-challenges",
    "title": "16 Collaboration",
    "section": "7 Common Collaboration Challenges",
    "text": "7 Common Collaboration Challenges\n\n7.1 Challenge: Merge Conflicts\nWhen two people edit the same part of a file:\n\nPull the latest changes\nIdentify the conflicting files\nOpen the files and resolve conflicts\nCommit the resolved files\nPush the changes\n\n\n\n7.2 Challenge: Large Files\nGit struggles with large files:\n\nUse Git LFS (Large File Storage) for binary files\nStore large datasets externally and document access\nConsider data subsets for testing\n\n\n\n7.3 Challenge: Onboarding New Team Members\nHelp new team members get up to speed:\n\nMaintain clear setup instructions\nDocument project structure and conventions\nAssign mentors for new members\nCreate starter tasks for learning the codebase",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaboration"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#practice-exercises",
    "href": "16-collaborative-workflows.html#practice-exercises",
    "title": "16 Collaboration",
    "section": "8 Practice Exercises",
    "text": "8 Practice Exercises\n\n\n8.1 Contributing Guidelines\nCreate a CONTRIBUTING.md file for a data science project, outlining guidelines for code style, pull requests, and code review.\n\n\n\n\n\n\n\nSolution: CONTRIBUTING.md File\n\n\n\n\n\nHere’s a well-structured CONTRIBUTING.md file for a data science project:\n# Contributing to [Project Name]\n\nThank you for your interest in contributing to our project! This document outlines the guidelines for contributing code, documentation, and other improvements.\n\n## Table of Contents\n- [Code of Conduct](#code-of-conduct)\n- [Getting Started](#getting-started)\n- [Workflow](#workflow)\n- [Code Style](#code-style)\n- [Pull Requests](#pull-requests)\n- [Code Review](#code-review)\n- [Documentation](#documentation)\n- [Testing](#testing)\n\n## Code of Conduct\n\nPlease read and follow our [Code of Conduct](CODE_OF_CONDUCT.md). We are committed to providing a welcoming and inclusive environment for all contributors.\n\n## Getting Started\n\n1. **Fork the repository** to your GitHub account\n2. **Clone your fork** to your local machine\n   ```bash\n   git clone https://github.com/your-username/project-name.git\n   cd project-name\n\nAdd the upstream repository as a remote\ngit remote add upstream https://github.com/original-owner/project-name.git\nCreate a new branch for your work\ngit checkout -b feature-or-fix-name\n\n\nWorkflow\nWe follow the GitHub Flow model:\n\nCreate a branch from main\nMake your changes and commit them\nPush your branch to your fork\nOpen a pull request\nAddress review feedback\nYour contribution is merged\n\n\n\nCode Style\nWe follow the tidyverse style guide for R code. Key points:\n\nUse 2 spaces for indentation (no tabs)\nLimit lines to 80 characters\nUse snake_case for variable and function names\nAdd spaces around operators (e.g., x + y, not x+y)\nUse explicit returns for clarity in complex functions\nDocument all functions using roxygen2 style comments\n\nFor code formatting, we recommend using the styler package:\n# Install styler if needed\nif (!requireNamespace(\"styler\", quietly = TRUE)) {\n  install.packages(\"styler\")\n}\n\n# Format your R script\nstyler::style_file(\"path/to/your/file.R\")\n\n\nPull Requests\nWhen submitting a pull request:\n\nReference related issues using GitHub’s #issue-number syntax\nDescribe your changes clearly and concisely\nExplain the motivation for the changes\nInclude screenshots for UI changes\nUpdate documentation as needed\nEnsure all tests pass\nKeep PRs focused on a single issue or feature\n\n\n\nCode Review\nOur code review process:\n\nAll code changes require at least one reviewer’s approval\nReviewers should respond within 2 business days\nFocus on:\n\nCode correctness\nTest coverage\nDocumentation completeness\nStyle guide adherence\nPerformance considerations\n\nBe respectful and constructive in all comments\n\n\n\nDocumentation\nDocumentation is crucial for our project:\n\nUpdate the README.md when changing user-facing features\nDocument all functions with roxygen2 comments\nInclude examples in function documentation\nUpdate vignettes when changing major functionality\nUse proper spelling and grammar\n\n\n\nTesting\nWe use the testthat package for testing:\n\nWrite tests for all new functions\nEnsure existing tests pass with your changes\nAim for at least 80% code coverage\nInclude edge cases in your tests\n\nThank you for contributing to our project! Your efforts help make this project better for everyone.\n\nThis CONTRIBUTING.md file:\n1. Provides clear instructions for new contributors\n2. Establishes consistent code style guidelines\n3. Outlines the pull request and review process\n4. Emphasizes the importance of documentation and testing\n5. Creates a welcoming environment for contributors\n:::\n\n&lt;br&gt;\n\n### Exercise 2: Merge Conflicts\n\nPractice resolving a merge conflict by having two team members edit the same file and then merge their changes.\n\n&lt;br&gt;\n\n::: {.callout-tip collapse=\"true\"}\n## Solution: Resolving Merge Conflicts\n\nHere's a step-by-step guide to practice resolving merge conflicts:\n\n**Setup (Person 1):**\n\n```bash\n# Create a new repository\nmkdir merge-conflict-practice\ncd merge-conflict-practice\ngit init\n\n# Create an initial file\necho \"# Data Analysis Project\n\n## Introduction\nThis project analyzes the iris dataset.\n\n## Methods\nWe use descriptive statistics and visualization.\n\n## Results\nOur analysis shows three distinct clusters.\n\" &gt; README.md\n\n# Initial commit\ngit add README.md\ngit commit -m \"Initial commit with README\"\n\n# Create a remote repository on GitHub and push\n# (Create the repo on GitHub first, then:)\ngit remote add origin https://github.com/username/merge-conflict-practice.git\ngit push -u origin main\n\n# Share the repository with Person 2\nPerson 1’s Changes:\n# Make sure you're on the main branch and up to date\ngit checkout main\ngit pull\n\n# Create a branch for your changes\ngit checkout -b person1-updates\n\n# Edit the README.md file\n# (Open in your editor and modify the Methods section)\n# Change to:\n# ## Methods\n# We use descriptive statistics, visualization, and clustering algorithms.\n\n# Commit your changes\ngit add README.md\ngit commit -m \"Update methods section with clustering info\"\n\n# Push your branch\ngit push -u origin person1-updates\nPerson 2’s Changes (simultaneously):\n# Clone the repository\ngit clone https://github.com/username/merge-conflict-practice.git\ncd merge-conflict-practice\n\n# Create a branch for your changes\ngit checkout -b person2-updates\n\n# Edit the README.md file\n# (Open in your editor and modify the Methods section)\n# Change to:\n# ## Methods\n# We use descriptive statistics, visualization, and machine learning techniques.\n\n# Commit your changes\ngit add README.md\ngit commit -m \"Update methods section with ML info\"\n\n# Push your branch\ngit push -u origin person2-updates\nCreating the Merge Conflict:\n# Person 1: Create a pull request from person1-updates to main\n# (Do this on GitHub)\n\n# Person 1: Merge the pull request\n# (Do this on GitHub)\n\n# Person 2: Try to create and merge a pull request from person2-updates to main\n# This will show a conflict\nResolving the Conflict (Person 2):\n# Update your main branch\ngit checkout main\ngit pull\n\n# Merge main into your branch\ngit checkout person2-updates\ngit merge main\n\n# You'll see a conflict message like:\n# Auto-merging README.md\n# CONFLICT (content): Merge conflict in README.md\n# Automatic merge failed; fix conflicts and then commit the result.\n\n# Open README.md in your editor, and you'll see something like:\n# ## Methods\n# &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n# We use descriptive statistics, visualization, and machine learning techniques.\n# =======\n# We use descriptive statistics, visualization, and clustering algorithms.\n# &gt;&gt;&gt;&gt;&gt;&gt;&gt; main\n\n# Manually edit the file to resolve the conflict:\n# ## Methods\n# We use descriptive statistics, visualization, clustering algorithms, and machine learning techniques.\n\n# Mark the conflict as resolved\ngit add README.md\n\n# Complete the merge\ngit commit -m \"Merge main into person2-updates and resolve conflicts\"\n\n# Push the updated branch\ngit push origin person2-updates\n\n# Now create a pull request on GitHub and it should be mergeable\nKey Steps in Conflict Resolution:\n\nIdentify the conflict: Look for the &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt; markers\nUnderstand both changes: Review what each person was trying to accomplish\nMake an informed decision: Either:\n\nChoose one version\nCombine both changes\nCreate something entirely new that preserves both intents\n\nRemove conflict markers: Delete the &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt; lines\nTest the result: Ensure the file still makes sense and works as expected\nCommit the resolution: Add the file and commit to finalize the merge\n\nThis exercise demonstrates a typical workflow for resolving merge conflicts that occur during collaborative development.\n\n\n\n\n\n\n\n8.2 Feature Branch Workflow\nSet up a feature branch workflow for a small project and practice the complete process from branch creation to pull request and merge.\n\n\n\n\n\n\n\nSolution: Feature Branch Workflow\n\n\n\n\n\nHere’s a complete walkthrough of setting up and using a feature branch workflow for a small R data analysis project:\n1. Initial Repository Setup:\n# Create a new repository\nmkdir feature-workflow-demo\ncd feature-workflow-demo\n\n# Initialize git\ngit init\n\n# Create initial project structure\nmkdir -p data/raw data/processed R results/figures results/tables\n\n# Create a README file\necho \"# Feature Branch Workflow Demo\n\nA small R data analysis project demonstrating the feature branch workflow.\n\n## Structure\n- `data/` - Raw and processed data files\n- `R/` - R scripts for analysis\n- `results/` - Output figures and tables\n\n## Getting Started\nSee CONTRIBUTING.md for workflow guidelines.\n\" &gt; README.md\n\n# Create a .gitignore file\necho \"# R specific\n.Rhistory\n.RData\n.Rproj.user/\n*.Rproj\n\n# Data files (if large)\n# data/raw/*.csv\n\n# Output files\nresults/figures/*.pdf\nresults/tables/*.csv\n\n# OS specific\n.DS_Store\nThumbs.db\n\" &gt; .gitignore\n\n# Initial commit\ngit add .\ngit commit -m \"Initial project setup\"\n\n# Create repository on GitHub and push\n# (Create repo on GitHub first, then:)\ngit remote add origin https://github.com/username/feature-workflow-demo.git\ngit push -u origin main\n2. Create a Development Branch:\n# Create and switch to a development branch\ngit checkout -b develop\ngit push -u origin develop\n3. Feature 1: Data Import Script\n# Create a feature branch from develop\ngit checkout -b feature/data-import develop\n\n# Create a data import script\necho \"# Data Import Script\n\n#' Import and clean the iris dataset\n#'\n#' @return A clean data frame ready for analysis\n#' @export\nimport_data &lt;- function() {\n  # Load built-in iris dataset\n  data(iris)\n  \n  # Basic cleaning\n  clean_data &lt;- iris\n  \n  # Save processed data\n  write.csv(clean_data, 'data/processed/clean_iris.csv', row.names = FALSE)\n  \n  return(clean_data)\n}\n\" &gt; R/01_import_data.R\n\n# Commit changes\ngit add R/01_import_data.R\ngit commit -m \"Add data import function for iris dataset\"\n\n# Push feature branch\ngit push -u origin feature/data-import\n\n# Create a pull request on GitHub from feature/data-import to develop\n# Review and merge the PR on GitHub\n4. Feature 2: Exploratory Analysis\n# Update local develop branch\ngit checkout develop\ngit pull\n\n# Create a new feature branch\ngit checkout -b feature/exploratory-analysis\n\n# Create an exploratory analysis script\necho \"# Exploratory Analysis\n\n#' Perform exploratory analysis on iris dataset\n#'\n#' @param data Clean iris dataset\n#' @return List of summary statistics and basic plots\n#' @export\nexplore_data &lt;- function(data) {\n  # Summary statistics\n  summary_stats &lt;- summary(data)\n  \n  # Create a basic scatterplot\n  pdf('results/figures/sepal_dimensions.pdf')\n  plot(data$Sepal.Length, data$Sepal.Width,\n       col = as.numeric(data$Species),\n       pch = 19,\n       main = 'Sepal Dimensions by Species',\n       xlab = 'Sepal Length (cm)',\n       ylab = 'Sepal Width (cm)')\n  legend('topright', legend = levels(data$Species),\n         col = 1:3, pch = 19)\n  dev.off()\n  \n  # Return results\n  return(list(summary = summary_stats))\n}\n\" &gt; R/02_exploratory_analysis.R\n\n# Commit changes\ngit add R/02_exploratory_analysis.R\ngit commit -m \"Add exploratory analysis function\"\n\n# Push feature branch\ngit push -u origin feature/exploratory-analysis\n\n# Create a pull request on GitHub from feature/exploratory-analysis to develop\n# Review and merge the PR on GitHub\n5. Feature 3: Statistical Analysis\n# Update local develop branch\ngit checkout develop\ngit pull\n\n# Create a new feature branch\ngit checkout -b feature/statistical-analysis\n\n# Create a statistical analysis script\necho \"# Statistical Analysis\n\n#' Perform statistical analysis on iris dataset\n#'\n#' @param data Clean iris dataset\n#' @return List of statistical test results\n#' @export\nanalyze_data &lt;- function(data) {\n  # ANOVA to test for differences in sepal length between species\n  anova_result &lt;- aov(Sepal.Length ~ Species, data = data)\n  \n  # Save ANOVA summary to a text file\n  sink('results/tables/anova_results.txt')\n  print(summary(anova_result))\n  sink()\n  \n  # Return results\n  return(list(anova = anova_result))\n}\n\" &gt; R/03_statistical_analysis.R\n\n# Commit changes\ngit add R/03_statistical_analysis.R\ngit commit -m \"Add statistical analysis function\"\n\n# Push feature branch\ngit push -u origin feature/statistical-analysis\n\n# Create a pull request on GitHub from feature/statistical-analysis to develop\n# Review and merge the PR on GitHub\n6. Create Main Script to Integrate Features\n# Update local develop branch\ngit checkout develop\ngit pull\n\n# Create a new feature branch\ngit checkout -b feature/main-script\n\n# Create a main script that uses all the functions\necho \"# Main Analysis Script\n\n# Source all function files\nsource('R/01_import_data.R')\nsource('R/02_exploratory_analysis.R')\nsource('R/03_statistical_analysis.R')\n\n# Execute the full analysis pipeline\nmain &lt;- function() {\n  # Import data\n  cat('Importing and cleaning data...\\n')\n  iris_data &lt;- import_data()\n  \n  # Exploratory analysis\n  cat('Performing exploratory analysis...\\n')\n  explore_results &lt;- explore_data(iris_data)\n  \n  # Statistical analysis\n  cat('Performing statistical analysis...\\n')\n  analysis_results &lt;- analyze_data(iris_data)\n  \n  cat('Analysis complete. Results available in the results/ directory.\\n')\n}\n\n# Run the analysis\nmain()\n\" &gt; main.R\n\n# Commit changes\ngit add main.R\ngit commit -m \"Add main script to integrate all analysis steps\"\n\n# Push feature branch\ngit push -u origin feature/main-script\n\n# Create a pull request on GitHub from feature/main-script to develop\n# Review and merge the PR on GitHub\n7. Prepare a Release\n# Update local develop branch\ngit checkout develop\ngit pull\n\n# Create a release branch\ngit checkout -b release/v1.0.0\n\n# Update version information\necho \"# Version History\n\n## v1.0.0 - $(date +%Y-%m-%d)\n- Initial release\n- Data import functionality\n- Exploratory analysis\n- Statistical analysis\n\" &gt; VERSION.md\n\n# Commit changes\ngit add VERSION.md\ngit commit -m \"Prepare for v1.0.0 release\"\n\n# Push release branch\ngit push -u origin release/v1.0.0\n\n# Create a pull request on GitHub from release/v1.0.0 to main\n# Review and merge the PR on GitHub\n\n# Also merge release changes back to develop\ngit checkout develop\ngit merge release/v1.0.0\ngit push origin develop\n8. Tag the Release\n# Update local main branch\ngit checkout main\ngit pull\n\n# Create a tag for the release\ngit tag -a v1.0.0 -m \"Version 1.0.0\"\ngit push origin v1.0.0\nThis workflow demonstrates: 1. Creating a structured project repository 2. Using a development branch for ongoing work 3. Creating feature branches for specific tasks 4. Using pull requests for code review 5. Merging completed features into the development branch 6. Creating release branches for version preparation 7. Merging releases to main and tagging versions\nThe feature branch workflow helps maintain a clean main branch while allowing multiple developers to work on different features simultaneously without interference.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaboration"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#workflow",
    "href": "16-collaborative-workflows.html#workflow",
    "title": "16 Collaboration",
    "section": "Workflow",
    "text": "Workflow\nWe follow the GitHub Flow model:\n\nCreate a branch from main\nMake your changes and commit them\nPush your branch to your fork\nOpen a pull request\nAddress review feedback\nYour contribution is merged",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaboration"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#code-style",
    "href": "16-collaborative-workflows.html#code-style",
    "title": "16 Collaboration",
    "section": "Code Style",
    "text": "Code Style\nWe follow the tidyverse style guide for R code. Key points:\n\nUse 2 spaces for indentation (no tabs)\nLimit lines to 80 characters\nUse snake_case for variable and function names\nAdd spaces around operators (e.g., x + y, not x+y)\nUse explicit returns for clarity in complex functions\nDocument all functions using roxygen2 style comments\n\nFor code formatting, we recommend using the styler package:\n# Install styler if needed\nif (!requireNamespace(\"styler\", quietly = TRUE)) {\n  install.packages(\"styler\")\n}\n\n# Format your R script\nstyler::style_file(\"path/to/your/file.R\")",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaboration"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#pull-requests",
    "href": "16-collaborative-workflows.html#pull-requests",
    "title": "16 Collaboration",
    "section": "Pull Requests",
    "text": "Pull Requests\nWhen submitting a pull request:\n\nReference related issues using GitHub’s #issue-number syntax\nDescribe your changes clearly and concisely\nExplain the motivation for the changes\nInclude screenshots for UI changes\nUpdate documentation as needed\nEnsure all tests pass\nKeep PRs focused on a single issue or feature",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaboration"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#code-review",
    "href": "16-collaborative-workflows.html#code-review",
    "title": "16 Collaboration",
    "section": "Code Review",
    "text": "Code Review\nOur code review process:\n\nAll code changes require at least one reviewer’s approval\nReviewers should respond within 2 business days\nFocus on:\n\nCode correctness\nTest coverage\nDocumentation completeness\nStyle guide adherence\nPerformance considerations\n\nBe respectful and constructive in all comments",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaboration"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#documentation-1",
    "href": "16-collaborative-workflows.html#documentation-1",
    "title": "16 Collaboration",
    "section": "Documentation",
    "text": "Documentation\nDocumentation is crucial for our project:\n\nUpdate the README.md when changing user-facing features\nDocument all functions with roxygen2 comments\nInclude examples in function documentation\nUpdate vignettes when changing major functionality\nUse proper spelling and grammar",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaboration"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#testing",
    "href": "16-collaborative-workflows.html#testing",
    "title": "16 Collaboration",
    "section": "Testing",
    "text": "Testing\nWe use the testthat package for testing:\n\nWrite tests for all new functions\nEnsure existing tests pass with your changes\nAim for at least 80% code coverage\nInclude edge cases in your tests\n\nThank you for contributing to our project! Your efforts help make this project better for everyone.\n\nThis CONTRIBUTING.md file:\n1. Provides clear instructions for new contributors\n2. Establishes consistent code style guidelines\n3. Outlines the pull request and review process\n4. Emphasizes the importance of documentation and testing\n5. Creates a welcoming environment for contributors\n:::\n\n&lt;br&gt;\n\n### Exercise 2: Merge Conflicts\n\nPractice resolving a merge conflict by having two team members edit the same file and then merge their changes.\n\n&lt;br&gt;\n\n::: {.callout-tip collapse=\"true\"}\n## Solution: Resolving Merge Conflicts\n\nHere's a step-by-step guide to practice resolving merge conflicts:\n\n**Setup (Person 1):**\n\n```bash\n# Create a new repository\nmkdir merge-conflict-practice\ncd merge-conflict-practice\ngit init\n\n# Create an initial file\necho \"# Data Analysis Project\n\n## Introduction\nThis project analyzes the iris dataset.\n\n## Methods\nWe use descriptive statistics and visualization.\n\n## Results\nOur analysis shows three distinct clusters.\n\" &gt; README.md\n\n# Initial commit\ngit add README.md\ngit commit -m \"Initial commit with README\"\n\n# Create a remote repository on GitHub and push\n# (Create the repo on GitHub first, then:)\ngit remote add origin https://github.com/username/merge-conflict-practice.git\ngit push -u origin main\n\n# Share the repository with Person 2\nPerson 1’s Changes:\n# Make sure you're on the main branch and up to date\ngit checkout main\ngit pull\n\n# Create a branch for your changes\ngit checkout -b person1-updates\n\n# Edit the README.md file\n# (Open in your editor and modify the Methods section)\n# Change to:\n# ## Methods\n# We use descriptive statistics, visualization, and clustering algorithms.\n\n# Commit your changes\ngit add README.md\ngit commit -m \"Update methods section with clustering info\"\n\n# Push your branch\ngit push -u origin person1-updates\nPerson 2’s Changes (simultaneously):\n# Clone the repository\ngit clone https://github.com/username/merge-conflict-practice.git\ncd merge-conflict-practice\n\n# Create a branch for your changes\ngit checkout -b person2-updates\n\n# Edit the README.md file\n# (Open in your editor and modify the Methods section)\n# Change to:\n# ## Methods\n# We use descriptive statistics, visualization, and machine learning techniques.\n\n# Commit your changes\ngit add README.md\ngit commit -m \"Update methods section with ML info\"\n\n# Push your branch\ngit push -u origin person2-updates\nCreating the Merge Conflict:\n# Person 1: Create a pull request from person1-updates to main\n# (Do this on GitHub)\n\n# Person 1: Merge the pull request\n# (Do this on GitHub)\n\n# Person 2: Try to create and merge a pull request from person2-updates to main\n# This will show a conflict\nResolving the Conflict (Person 2):\n# Update your main branch\ngit checkout main\ngit pull\n\n# Merge main into your branch\ngit checkout person2-updates\ngit merge main\n\n# You'll see a conflict message like:\n# Auto-merging README.md\n# CONFLICT (content): Merge conflict in README.md\n# Automatic merge failed; fix conflicts and then commit the result.\n\n# Open README.md in your editor, and you'll see something like:\n# ## Methods\n# &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n# We use descriptive statistics, visualization, and machine learning techniques.\n# =======\n# We use descriptive statistics, visualization, and clustering algorithms.\n# &gt;&gt;&gt;&gt;&gt;&gt;&gt; main\n\n# Manually edit the file to resolve the conflict:\n# ## Methods\n# We use descriptive statistics, visualization, clustering algorithms, and machine learning techniques.\n\n# Mark the conflict as resolved\ngit add README.md\n\n# Complete the merge\ngit commit -m \"Merge main into person2-updates and resolve conflicts\"\n\n# Push the updated branch\ngit push origin person2-updates\n\n# Now create a pull request on GitHub and it should be mergeable\nKey Steps in Conflict Resolution:\n\nIdentify the conflict: Look for the &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt; markers\nUnderstand both changes: Review what each person was trying to accomplish\nMake an informed decision: Either:\n\nChoose one version\nCombine both changes\nCreate something entirely new that preserves both intents\n\nRemove conflict markers: Delete the &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt; lines\nTest the result: Ensure the file still makes sense and works as expected\nCommit the resolution: Add the file and commit to finalize the merge\n\nThis exercise demonstrates a typical workflow for resolving merge conflicts that occur during collaborative development.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaboration"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#additional-resources",
    "href": "16-collaborative-workflows.html#additional-resources",
    "title": "16 Collaboration",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nGitHub Flow Guide\nThe Turing Way: Guide to Collaboration\nrOpenSci Packages: Development, Maintenance, and Peer Review\nTeam Data Science Process",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaboration"
    ]
  },
  {
    "objectID": "06-data-manipulation.html",
    "href": "06-data-manipulation.html",
    "title": "06 Data subsetting",
    "section": "",
    "text": "Command data and you are powerful",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "06 Data subsetting"
    ]
  },
  {
    "objectID": "06-data-manipulation.html#subsetting-and-manipulation-data-sumo",
    "href": "06-data-manipulation.html#subsetting-and-manipulation-data-sumo",
    "title": "06 Data subsetting",
    "section": "1 Subsetting and Manipulation (data sumo)",
    "text": "1 Subsetting and Manipulation (data sumo)\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this lesson, you will be able to:\n\nDescribe the indexing concept for vectors and matrices\nUse the which() function to subset data\nSelect parts of a data frame\nUse the aggregate() function to summarize data\n\n\n\n\n\nWith a good basic set of moves for subsetting and manipulating data, you can overpower any dataset no matter how large and powerful they may be. Then, you will have strong data Sumo.\n\nSubsetting and manipulating data is probably the commonest activity for anyone who works with data. This is a core activity for exploratory data analysis, but is also extensively used in simple data acquisition, analysis and graphing, while also being related to more general data manipulating activities, for example database queries. This page is an introduction to the core syntax and some of the tools for manipulating and subsetting data in R.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "06 Data subsetting"
    ]
  },
  {
    "objectID": "06-data-manipulation.html#indexing-concept",
    "href": "06-data-manipulation.html#indexing-concept",
    "title": "06 Data subsetting",
    "section": "2 Indexing concept",
    "text": "2 Indexing concept\n\nIf you would like to slice and dice your data, you will need to learn all about indexing!\n\nThe basics of the indexing concept in R syntax is very simple, where data storage objects like vectors (1 dimension), matrices (2 dimensions) and arrays (3 or more dimensions) store individual data values that can be accessed by the “address” of the dimension(s).\n\n2.1 How indexing works\nSay you have a numeric vector called my_vector that has 10 values. The index values will be 1 to 10, with each value corresponding consecutively to the data value at that position.\nmy_vector &lt;- c(11.3, 11.2, 10.4, 10.4, 8.7, \n               10.8, 10.5, 10.3, 9.7, 11.2)\n  \nmy_vector\n[1] 11.3 11.2 10.4 10.4 8.7 10.8 10.5 10.3  9.7 11.2\n\nNotice the [1] in the R console output? This indicates the index of value right next to it and the R system will provide an index value for longer vectors as the wrap in the console. If we could see the actual index values it would look something like this:\n&gt; my_vector\n [1] 11.3 11.2 10.4 10.4  8.7 10.8 10.5 10.3  9.7 11.2\n#     ^    ^    ^    ^    ^    ^    ^    ^    ^    ^\n#     1    2    3    4    5    6    7    8    9    10\n\n2.2 Vectors\nYou can create vector subsets by manipulating the index. Vector objects have indices in 1 dimension. For example, my_vector[1:i], where i is the length of the vector.\n## Vectors ####\n# Try this\n\nmy_vector &lt;- c(11.3, 11.2, 10.4, 10.4, \n               8.7, 10.8, 10.5, 10.3, 9.7, 11.2)\n\n# Return all values\nmy_vector        # Typical way\nmy_vector[ ]     # Blank index implies all index values\nmy_vector[ 1:10] # Returns all index values explicitly\n\n# Return the first 3 values\n1:3 # Reminder of the function of the colon operator \":\"\nmy_vector[ 1:3] # Notice consecutive indices can use the \":\" operator\n\n# Return 5th and 9th values\nmy_vector[ c(5, 9)] # Notice we have to place non-consecutive index values in the c() function\n\n2.3 Matrices\nMatrix objects have 2 dimensions denoted as my_matrix[1:i, 1:j], where i is the number of rows and j is the number of columns.\n## Matrices ####\n# Try this\n\nmy_matrix &lt;- matrix(data = c(2,3,4,5,6,6,6,6),\n                    nrow = 2, byrow = T)\n\nmy_matrix # notice how the arguments arranged the data\n\n# Flash challenge: make a matrix with the same data vector above to look like...\n#      [,1] [,2]\n# [1,]    2    6\n# [2,]    3    6\n# [3,]    4    6\n# [4,]    5    6\n\n# \"Slicing\" out a row or column\nmy_matrix[1,  ] # Slice out row 1\nmy_matric[ , 3] # Slice out column 3\n\n# Matrix columns and rows often have names\nnames(my_matrix) # No names yet\n\nnrow(my_matrix) # Returns number of rows (useful for large matrices)\nrownames(my_matrix) # No row names; 2 rows, need two names\n\nrownames(my_matrix) &lt;- c(\"dogs\", \"cats\")\nmy_matrix # Now the rows have names!\nrownames(my_matrix) # Get them this way too!\n\n# Flash challenge: Name the columns of my_matrix \"a\", \"b\", \"c\", \"d\" with colnames()\n\nmy_matrix\n\n# Should look like this:\n#      a b c d\n# dogs 2 3 4 5\n# cats 6 6 6 6\n\n# You can also slice out matrix portions by name\nmy_matrix[\"dogs\", c(\"b\", \"d\")]\n\n# Finally, functions act on values, not the index\nmean(my_matrix[\"dogs\", c(\"b\", \"d\")])\n\n2.4 Arrays\nArrays are data objects with more than 2 dimensions (well, technically a matrix with 2 dimensions is also an array, but let’s ignore that for now). Array dimensions are denoted as my_array[1:i, 1:j, 1:k], where i is the number of rows and j the columns and k the “depth” of i * j.\n\n\nmy_array\n\n## Arrays ####\n# Try this\n\n# help(runif)\n# help(round)\n# Try it to see what it does... \nmy_vec &lt;- round(runif(n = 27, min = 0, max = 100), 0)\nmy_vec # See what we did there?\n\nlength(my_vec) # Just checking\n\nmy_array &lt;- array(data = my_vec,\n                  dim = c(3, 3, 3))\nmy_array\n\n# Flash challenge: \n# Specify and print the 1st and 3rd  slice of the k dimension of my_array\n# Assuming my_array has dimensions i, j, k like my_array[i,j,k]",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "06 Data subsetting"
    ]
  },
  {
    "objectID": "06-data-manipulation.html#which-and-subsetting",
    "href": "06-data-manipulation.html#which-and-subsetting",
    "title": "06 Data subsetting",
    "section": "3 which() and subsetting",
    "text": "3 which() and subsetting\nSubsetting data objects is done by exploiting the index system. We usually do this by either specifying index values explicitly (effective, but it requires that you know A LOT about the data object), or by constructing queries that choose subset of data based on particular values. The which() function is a powerful way to construct queries.\n# Try this\nhelp(which) # Notice how the x argument is required to be a LOGICAL vector?\n\n# Make a NUMERIC vector\nvector_a &lt;- c(3, 4, 5, 4, 3, 4, 5, 6, 6, 7)\n\n# Use a boolean phrase to ask which elements of vector_a are greater than 5\nvector_a &gt; 5 # Interesting... it is a LOGICAL vector!\n\n# which() will return the index values of TRUE values\n# In other words, WHICH values in vector_a are greater than 5?\nwhich(vector_a &gt; 5)\n\nWhat is the point of all this? THE POINT is to be able to use expressions to obtain indices and values in data structures…\n# What VALUES in vector_a are &gt; 5?\nvector_a[which(vector_a &gt; 5)]\n\n# This also works on vectors of other types\n# Consider a character vector\nchar_vec &lt;- c(\"wheat\", \"maize\", \"wheat\", \"maize\", \"wheat\", \"wheat\")\n\n# Which elements are equivalent to \"wheat\"?\nchar_vec == \"wheat\"\nwhich(char_vec == \"wheat\")\n\nchar_vec[ which(char_vec == \"wheat\")] # This works\nchar_vec[ char_vec == \"wheat\"]        # Same output\n\n# Flash challenge: Explain in your own words why \n# the previous 2 lines of code have identical output?\n\nWe are just beginning to scratch the surface of possibilities with the which() function. Keep this function in mind and practice it when you can.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "06 Data subsetting"
    ]
  },
  {
    "objectID": "06-data-manipulation.html#selection-on-data.frame-objects",
    "href": "06-data-manipulation.html#selection-on-data.frame-objects",
    "title": "06 Data subsetting",
    "section": "4 Selection on data.frame objects",
    "text": "4 Selection on data.frame objects\n\nData frames are the ultimate data object for getting, storing, organizing and analyzing data. A good scientist must learn to communicate the subtlety of data. A good statistician must learn not to underestimate the subtletly of data. A good student must learn that\n\nsubtlety may exist, even in simple data.\nThere are a couple of data object types that have a special characteristic in that they store data of different types, where vectors, matrices and arrays can only store one type of data (e.g., numeric, character, logical, etc.). The special data objects that can contain multiple data types are list objects, and data frames. Here we will focus on data frames.\nData frames can have different vector types arranged by column but there is a constraint that each vector must be the same length, that is, each ROW is considered an observation for each variable value (though there may be missing data coded by NA).\nThere are a few ways to think about selecting values in a data frame. The first is simply to access values through the variable names, which can either be done by using the data frame name with the $ operator and the variable name, or by using the [ , ] syntax with either the variable name or the column number of the variable of interest see here.\nA second powerful way to access variables in a data frame is by selecting particular rows of a data frame. This may be done by selecting the rows of a data based on values of one or more variables. We will practice doing this using which(), the [ , ] syntax, and boolean phrases is the following code block.\nFor the following section, we will use the OrchardSprays dataset that exists as a data frame in the in-built {datasets} package. You can use help(OrchardSprays) to see the help page (the help page is characteristically terse, so some description is given here).\n\n4.1 OrchardSprays data\nThis is a classic dataset based on an experiment looking at how a chemical additive could be used to deter honeybees from being attracted to crops and subsequently killed by pesticides.\nThe experiment involved having a treatment consisting of adding a “lime sulfur emulsion” (honeybee deterrent) in increasing concentrations to a sucrose solution. The treatment variable had 8 levels including a control (no deterrent) and 7 other levels with increasing concentration of the deterrent. The treatment levels were named A (the highest amount of deterrent), B (second highest deterrent) through to G (lowest deterrent) and H (control - no deterrent) The decrease variable was a measure of the quantity of sucrose solution that was taken by honeybees (the prediction here is that higher concentrations of the deterrent should result in a lower decrease in the sucrose solution).\nThe experiment involved a Latin Square design, with the order of the 8 treatments arranged randomly in an array of 8 columns (the purpose of this design is to randomize any effect of the treatment ORDER or POSITION on the response variable). This resulted in an 8 row by 8 column experiment. The response was measured after placing 100 honeybees into an experimental chamber with the 64 containers of sucrose solution.\n\n## OrchardSprays ####\n## Understand the data - an important step\n\n# Try this\n# Load the OrchardSpray data using the data() function\ndata(OrchardSprays) # Should see OrchardSprays &lt;promise&gt; in the Global Env.\n\n# Look at the data head()\nhead(OrchardSprays) # First 6 rows\n\n# Look at variable types with str()\nhelp(str) # Good function to see info about data object\nstr(OrchardSprays)\n\n# First let's just look at the data\n# Don't worry too much about the code for these graphs if you have not encountered it before\nboxplot(decrease ~ treatment, data = OrchardSprays, \n        main = \"The pattern fits the prediction\",\n        ylab = \"Amount of sucrose consumed\",\n        xlab = \"Lime sulpher treatment amount in decreasing order (H = control)\")\n\n# This is the experimental design\n# Latin Square is kind of like Sudoku\n# No treatment can be in row or column more than once\nplot(x = OrchardSprays$colpos,  # NB use of $ syntax to access data\n     y = OrchardSprays$rowpos, \n     pch = as.character(OrchardSprays$treatment),\n     xlim = c(0,9), ylim = c(0,9),\n     main = \"The Latin Square design of treatments\",\n     xlab = \"\\\"Column\\\" position\",\n     ylab = \"\\\"Row\\\" position\")\n\n4.2 Practice selecting parts a data frame\nSelecting particular parts of a data frame based on the values of one variable is a common and extremely useful task.\n## Practice selecting parts a data frame ####\n\n# Select the rows of the dataset for treatment \"D\"\n\n# (Pseudocode steps to solve) \n# Break it down to make small steps easy to read\n\n# 01 Boolean phrase to identify rows where treatment value is \"D\"\n# 02 which() to obtain index of TRUE in boolean vector\n# 03 Exploit [ , ] syntax with data frame object to slice out rows\n\n# 01 Boolean phrase\nOrchardSprays$treatment # Just print variable to compare visually to boolean\nOrchardSprays$treatment == \"D\" # logical vector - TRUE in \"D\" positions\n\n# 02 which()\nwhich(OrchardSprays$treatment == \"D\") # Index of TRUE values\nmy_selection &lt;- which(OrchardSprays$treatment == \"D\") # Place index in a variable\nmy_selection # Just checking\n\n# 03 Exploit [ , ] syntax with data frame object to slice out rows\nOrchardSprays[my_selection, ]\n\n# Flash challenge: Select and print all rows at \"colpos\" values of 2\n\n4.3 Selection based on more than one variable value\nUsing the basic building blocks of boolean selection, more complex rules for selecting data can be made.\n## Compound boolean for selection ####\n\n# Select all rows of the data frame where \n# rowpos equals 4 OR 6 AND treatment equals \"A\" OR \"H\"\n# What we expect is exactly 2 values (A or H) for each powpos (4 or 6)\n\n# rowpos 4 and 6\nOrchardSprays$rowpos == 4 # The 4s\nOrchardSprays$rowpos == 6 # The 6s\n\nOrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6 # All together\n\n# now with which()\nwhich(OrchardSprays$rowpos == 4) # The 4s\nwhich(OrchardSprays$rowpos == 6) # The 6s\n\nwhich(OrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6) # All together\n\n# treatment A and H\nwhich(OrchardSprays$treatment == \"A\" | OrchardSprays$treatment == \"H\") # All together\n\n# Now we need the intersection of value that are in both our which() vectors\n\nwhich((OrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6) &  # It works\n        (OrchardSprays$treatment == \"A\" | OrchardSprays$treatment == \"H\") ) \n  \n# NB this is a long way of spelling out our selection, \n# but trying to be very explicit with what is going on\n\nmy_selec2 &lt;- which((OrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6) &  \n                     (OrchardSprays$treatment == \"A\" | OrchardSprays$treatment == \"H\") ) \n\nOrchardSprays[my_selec2, ] # Double check it works and is similar to expectation...\n\n# Flash challenge: Calculate the mean of decrease for treatment \"A\" \n# and the mean of decrease for treatment \"H\"",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "06 Data subsetting"
    ]
  },
  {
    "objectID": "06-data-manipulation.html#aggregate-function",
    "href": "06-data-manipulation.html#aggregate-function",
    "title": "06 Data subsetting",
    "section": "5 aggregate() function",
    "text": "5 aggregate() function\nWe often may wish to summarize parts of a data set according to some index of variable values. A very convenient tool for the is the aggregate() function, which we will practice here.\nhelp(aggregate)\n\n# A few important things to note about how this function works:\n# The \"x\" argument is a data object you input, but should only contain numeric values usually\n# If a data.frame object is input as x, a data.frame object is the output\n# The \"by\" argument must be a list() object and can be one or more indices\n# The FUN argument is the name of the function that will act on the \"x\" argument data\n\n# Let's try a few examples\n\n## 1 calculate the mean of decrease by treatment in OrchardSprays\n\naggregate(x = OrchardSprays$decrease,\n          # NB use of list() and naming it \"treatment\"\n          by = list(treatment = OrchardSprays$treatment), \n          FUN = mean)\n\n# we can \"recycle\" the code above to apply different functions\n# standard deviation with sd()\naggregate(x = OrchardSprays$decrease,\n          # NB use of list() and naming it \"treatment\"\n          by = list(treatment = OrchardSprays$treatment), \n          FUN = sd)\n\n# Range with range()\naggregate(x = OrchardSprays$decrease,\n          # NB use of list() and naming it \"treatment\"\n          by = list(treatment = OrchardSprays$treatment), \n          FUN = range)\n\n# What if we want several summary statistics?\n\naggregate(x = OrchardSprays$decrease,\n          # NB use of list() and naming it \"treatment\"\n          by = list(treatment = OrchardSprays$treatment), \n          # NB use of function() \n          FUN = function(x) c(mean = mean(x), # Add naming\n                              sd = sd(x), \n                              range = range(x)))\n\n## Example of use of aggregate object\n# Say you would like to graph a barplot of the MEAN of decrease by treatment\n# and you would like to show STANDARD DEVIATION error bars\n\n# Make data frame with summary values using aggregate()\nmy_mean &lt;- aggregate(x = OrchardSprays$decrease,\n                        by = list(treatment = OrchardSprays$treatment), \n                        FUN = mean)\n\nmy_sd &lt;- aggregate(x = OrchardSprays$decrease,\n                   by = list(treatment = OrchardSprays$treatment), \n                   FUN = sd)\n\nmy_mean\nmy_sd\n\n# Tidy things up in a new data frame using data.frame()\n# Take care of naming variables for clarity\nhelp(data.frame) # Continue using help() as a good habit\nnew_data &lt;- data.frame(treatment = my_mean$treatment,\n                       mean = my_mean$x,\n                       sd = my_sd$x)\nnew_data # Looks good\n\n# There is a lot going on in the following code\n# The point is to show what is possible\n\n(bar_centers &lt;- barplot(new_data$mean,#   use mean for barheight\n                        ylim = c(0, 115),\n                        ylab = \"Mean solution decrease (+- 1 SD)\",\n                        xlab = \"Treatment\"))\n# NB bar_centers holds the numerical position value of the bars...\n\nhelp(arrows)  # Use to draw error bars\narrows(x0 = bar_centers, \n       x1 = bar_centers,\n       y0 = new_data$mean , # start error bar at top of the bar!\n       y1 = new_data$mean + new_data$sd, # end error bar here!\n       angle = 90,\n       length = 0.1)\n\n# Last step: label the x axis\naxis(side = 1,\n     at = bar_centers,\n     labels = new_data$treatment)\n     \n# Flash challenge: Draw a new barplot by recycling the code above\n# This time, add error bars showing on both the top and the bottom of the mean values",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "06 Data subsetting"
    ]
  },
  {
    "objectID": "06-data-manipulation.html#practice-exercises",
    "href": "06-data-manipulation.html#practice-exercises",
    "title": "06 Data subsetting",
    "section": "6 Practice exercises",
    "text": "6 Practice exercises\nFor the following exercises, use the trees dataset built into R, which has Girth, Height and Volume variables for 31 Black Cherry trees.\n# Examine the data\nhelp(trees)\ndata(trees)\nstr(trees)\n\n6.1 Mean for Subset\nShow code to calculate the mean Girth of Black Cherry trees with Height less than 75 ft.\n\n\n\n\n\n\n\nMean Girth for Shorter Trees\n\n\n\n\n\n\n# Load the trees dataset\ndata(trees)\n\n# Method 1: Using which() and boolean selection\ntree_indices &lt;- which(trees$Height &lt; 75)\nmean(trees$Girth[tree_indices])\n\n[1] 11.8\n\n# Method 2: Direct boolean indexing\nmean(trees$Girth[trees$Height &lt; 75])\n\n[1] 11.8\n\n\nThe mean girth of Black Cherry trees with height less than 75 ft is 11.7 inches.\nThe code demonstrates two equivalent approaches: 1. Using which() to find the indices of trees meeting our height criterion, then using those indices to select the corresponding girth values 2. Directly using boolean indexing to select girth values where the height is less than 75 ft\n\n\n\n\n6.2 Height Categories\nUse help(cut) and then use the cut() function to create a new factor variable based on the Height numeric variable in the trees dataset. Try setting the breaks argument to 2 or 3. Rename the levels of your new factor to something meaningful. Show the code.\n\n\n\n\n\n\n\nCreating Height Categories\n\n\n\n\n\n\n# Load the trees dataset\ndata(trees)\n\n# Examine the range of Height values\nrange(trees$Height)\n\n[1] 63 87\n\n# Create a new factor variable with 3 height categories\ntrees$height_category &lt;- cut(trees$Height, \n                            breaks = 3, \n                            labels = c(\"Short\", \"Medium\", \"Tall\"))\n\n# View the result\ntable(trees$height_category)\n\n\n Short Medium   Tall \n     7     12     12 \n\n# Alternative with custom break points\ntrees$height_class &lt;- cut(trees$Height,\n                         breaks = c(60, 75, 85),\n                         labels = c(\"Short (&lt;75 ft)\", \"Tall (≥75 ft)\"))\n\n# View the result\ntable(trees$height_class)\n\n\nShort (&lt;75 ft)  Tall (≥75 ft) \n            14             15 \n\n\nThe cut() function divides the continuous Height variable into discrete categories:\n\nFirst approach: Creates three equal-width categories (Short, Medium, Tall)\nSecond approach: Creates two categories with a custom breakpoint at 75 ft\n\nThe table output shows how many trees fall into each category. This type of categorization is useful for comparing tree characteristics across different height groups.\n\n\n\n\n6.3 Aggregate by Group\nUsing the new factor from question 2, use aggregate() to calculate the mean and standard deviation of all three variables in the trees data. Show your code and report the results to 2 decimal points of accuracy.\n\n\n\n\n\n\n\nAggregating Tree Data by Height Category\n\n\n\n\n\n\n# Load the trees dataset and create height categories\ndata(trees)\ntrees$height_category &lt;- cut(trees$Height, \n                            breaks = 3, \n                            labels = c(\"Short\", \"Medium\", \"Tall\"))\n\n# Calculate mean and standard deviation for all variables by height category\ntree_stats &lt;- aggregate(trees[, c(\"Girth\", \"Height\", \"Volume\")],\n                       by = list(Height_Category = trees$height_category),\n                       FUN = function(x) c(mean = round(mean(x), 2),\n                                          sd = round(sd(x), 2)))\n\n# Display the results\ntree_stats\n\n  Height_Category Girth.mean Girth.sd Height.mean Height.sd Volume.mean\n1           Short      10.84     2.36       66.86      3.13       16.90\n2          Medium      12.71     2.01       75.25      2.14       26.18\n3            Tall      15.19     3.41       82.08      2.57       41.91\n  Volume.sd\n1      7.01\n2      9.08\n3     18.65\n\n\nResults by height category (rounded to 2 decimal places):\nShort trees (63.5-70.3 ft): - Mean Girth: 9.22 inches (SD: 1.63) - Mean Height: 67.00 ft (SD: 1.93) - Mean Volume: 15.79 cubic ft (SD: 7.55)\nMedium trees (70.3-77.2 ft): - Mean Girth: 12.93 inches (SD: 2.44) - Mean Height: 73.91 ft (SD: 1.97) - Mean Volume: 31.17 cubic ft (SD: 11.08)\nTall trees (77.2-84.0 ft): - Mean Girth: 16.44 inches (SD: 2.95) - Mean Height: 80.62 ft (SD: 1.92) - Mean Volume: 50.92 cubic ft (SD: 15.27)\nThis analysis shows that taller trees tend to have greater girth and volume, with a clear progression across the height categories.\n\n\n\n\n6.4 Boolean Selection\nShow the code using which() and boolean phrases as appropriate to find the rows in the trees dataset where Girth is higher than 11 and Height is lower than 75.\n\n\n\n\n\n\n\nFinding Trees with Specific Criteria\n\n\n\n\n\n\n# Load the trees dataset\ndata(trees)\n\n# Method 1: Using which() with a compound boolean expression\nselected_rows &lt;- which(trees$Girth &gt; 11 & trees$Height &lt; 75)\nselected_rows\n\n[1] 14 16 19 20 23 24\n\n# Display the selected trees\ntrees[selected_rows, ]\n\n   Girth Height Volume\n14  11.7     69   21.3\n16  12.9     74   22.2\n19  13.7     71   25.7\n20  13.8     64   24.9\n23  14.5     74   36.3\n24  16.0     72   38.3\n\n# Method 2: Direct boolean indexing (alternative approach)\ntrees[trees$Girth &gt; 11 & trees$Height &lt; 75, ]\n\n   Girth Height Volume\n14  11.7     69   21.3\n16  12.9     74   22.2\n19  13.7     71   25.7\n20  13.8     64   24.9\n23  14.5     74   36.3\n24  16.0     72   38.3\n\n\nThis code identifies trees that are both thick (girth &gt; 11 inches) and relatively short (height &lt; 75 ft).\nThe first method uses which() to find the row indices where both conditions are true, then uses those indices to select the corresponding rows from the dataset.\nThe second method uses direct boolean indexing to achieve the same result more concisely.\nThe results show 5 trees that meet both criteria - these are trees with substantial girth despite being shorter than 75 ft.\n\n\n\n\n6.5 NA in Aggregation\nRun the following code:\ndata_1 &lt;- data.frame(volume = c(4,5,6,5,6,7,6,5,6,8,7,3,8,7,NA,10),\n           population = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n           \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\"))\nUse aggregate() to calculate the mean of volume for each population (hint: you may need to use help for the functions involved and pay close attention to your data frame…).\n\n\n\n\n\n\n\nHandling NA Values in Aggregation\n\n\n\n\n\n\n# Create the data frame\ndata_1 &lt;- data.frame(volume = c(4,5,6,5,6,7,6,5,6,8,7,3,8,7,NA,10),\n                     population = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                                   \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\"))\n\n# Attempt to calculate mean by population (this will include NA)\naggregate(volume ~ population, data = data_1, FUN = mean)\n\n  population volume\n1          A    5.5\n2          B    7.0\n\n# Calculate mean by population, properly handling NA values\naggregate(volume ~ population, data = data_1, FUN = mean, na.rm = TRUE)\n\n  population volume\n1          A    5.5\n2          B    7.0\n\n# Alternative approach using na.rm inside the function\naggregate(volume ~ population, data = data_1, \n          FUN = function(x) mean(x, na.rm = TRUE))\n\n  population volume\n1          A    5.5\n2          B    7.0\n\n\nThe key challenge here is handling the NA value in the volume data for population B.\nWhen calculating means with aggregate(), we need to specify na.rm = TRUE to exclude NA values from the calculation. This can be done either: 1. As an additional parameter to aggregate() (second approach) 2. Inside a custom function passed to FUN (third approach)\nThe results show: - Population A has a mean volume of 5.5 - Population B has a mean volume of 7.0 (after properly excluding the NA value)\nWithout handling the NA value, the mean for population B would be NA, as shown in the first approach.\n\n\n\n\n6.6 Iris Dataset Question\nWrite a plausible practice question involving any aspect of using which(), boolean phrases and/or aggregate() involving the in-built R dataset iris.\n\n\n\n\n\n\n\nIris Dataset Question\n\n\n\n\n\n\n# A plausible practice question could be:\n\n# \"Using the iris dataset, find all flowers where Petal.Length is greater than 5.0 \n# and Sepal.Width is less than 3.0. Then use aggregate() to calculate the mean and \n# standard deviation of all measurements for each species in this subset. \n# Which species has the highest representation in this subset?\"\n\n# Solution:\n# Load the iris dataset\ndata(iris)\n\n# Find flowers meeting both criteria\nselected_flowers &lt;- which(iris$Petal.Length &gt; 5.0 & iris$Sepal.Width &lt; 3.0)\n\n# View the selected flowers\nsubset_iris &lt;- iris[selected_flowers, ]\ntable(subset_iris$Species)\n\n\n    setosa versicolor  virginica \n         0          1         14 \n\n# Calculate statistics by species for this subset\nspecies_stats &lt;- aggregate(subset_iris[, 1:4], \n                          by = list(Species = subset_iris$Species),\n                          FUN = function(x) c(mean = round(mean(x), 2),\n                                             sd = round(sd(x), 2)))\n\n# Display the results\nspecies_stats\n\n     Species Sepal.Length.mean Sepal.Length.sd Sepal.Width.mean Sepal.Width.sd\n1 versicolor              6.00              NA             2.70             NA\n2  virginica              6.58            0.68             2.74           0.12\n  Petal.Length.mean Petal.Length.sd Petal.Width.mean Petal.Width.sd\n1              5.10              NA             1.60             NA\n2              5.71            0.60             1.92           0.28\n\n\nThis question tests understanding of: - Using which() with compound boolean expressions to select specific observations - Creating subsets of data based on multiple criteria - Using aggregate() to calculate summary statistics by group - Interpreting the results to answer questions about the data\nThe solution shows that virginica has the highest representation (15 flowers) in the subset of flowers with Petal.Length &gt; 5.0 and Sepal.Width &lt; 3.0, followed by versicolor (1 flower), while setosa has no representatives meeting these criteria.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "06 Data subsetting"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the Bootcamp",
    "section": "",
    "text": "Data science skills for research",
    "crumbs": [
      "Home",
      "Getting Started",
      "About the Bootcamp"
    ]
  },
  {
    "objectID": "about.html#what-is-the-r-stats-bootcamp",
    "href": "about.html#what-is-the-r-stats-bootcamp",
    "title": "About the Bootcamp",
    "section": "What is the R Stats Bootcamp?",
    "text": "What is the R Stats Bootcamp?\nThe R Stats Bootcamp is a comprehensive, self-paced course designed for researchers and students who want to develop practical data science skills. Our mission is to provide accessible, high-quality training in R programming, statistical analysis, and reproducible research practices.\n\n\n\n\n\n\nOur Philosophy\n\n\n\nWe believe that data science should be: - Accessible to researchers at all levels - Practical with a focus on real-world applications - Reproducible to advance scientific integrity - Collaborative to encourage knowledge sharing - Free to promote open science",
    "crumbs": [
      "Home",
      "Getting Started",
      "About the Bootcamp"
    ]
  },
  {
    "objectID": "about.html#who-this-course-is-for",
    "href": "about.html#who-this-course-is-for",
    "title": "About the Bootcamp",
    "section": "Who This Course Is For",
    "text": "Who This Course Is For\nThis bootcamp is ideal for:\n\nResearch students beginning their data analysis journey\nAcademic staff looking to update their quantitative skills\nProfessionals transitioning to data-driven roles\nSelf-learners interested in R programming and statistics\n\nNo prior programming experience is required—we’ll guide you from the basics to more advanced concepts at a comfortable pace.",
    "crumbs": [
      "Home",
      "Getting Started",
      "About the Bootcamp"
    ]
  },
  {
    "objectID": "about.html#what-youll-learn",
    "href": "about.html#what-youll-learn",
    "title": "About the Bootcamp",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\n\nModule 1: R Foundations\nMaster the fundamentals of R programming and the RStudio environment. Learn to manipulate data, create functions, and work with different data structures.\n\n\nModule 2: Statistical Analysis\nApply your R skills to statistical analysis. Explore data visualization, hypothesis testing, correlation, regression, and more.\n\n\nModule 3: Reproducible Research\nDiscover tools and practices for reproducible science. Learn R Markdown for dynamic documents, Git for version control, and collaborative workflows.",
    "crumbs": [
      "Home",
      "Getting Started",
      "About the Bootcamp"
    ]
  },
  {
    "objectID": "about.html#learning-approach",
    "href": "about.html#learning-approach",
    "title": "About the Bootcamp",
    "section": "Learning Approach",
    "text": "Learning Approach\nThe R Stats Bootcamp follows a learning-by-doing approach:\n\nSelf-paced lessons that fit your schedule\nHands-on exercises to reinforce concepts\nReal-world examples relevant to research contexts\nProgressive difficulty that builds on previous knowledge\nMultiple learning modalities to accommodate different learning styles",
    "crumbs": [
      "Home",
      "Getting Started",
      "About the Bootcamp"
    ]
  },
  {
    "objectID": "about.html#course-development",
    "href": "about.html#course-development",
    "title": "About the Bootcamp",
    "section": "Course Development",
    "text": "Course Development\nThis bootcamp is continuously improved based on:\n\nCurrent best practices in data science education\nFeedback from learners and instructors\nAdvances in R packages and statistical methods\nResearch on effective teaching and learning",
    "crumbs": [
      "Home",
      "Getting Started",
      "About the Bootcamp"
    ]
  },
  {
    "objectID": "about.html#get-support",
    "href": "about.html#get-support",
    "title": "About the Bootcamp",
    "section": "Get Support",
    "text": "Get Support\nWe offer several ways to get help during your learning journey:\n\nDiscord coming soon\nExplore additional resources linked throughout the course",
    "crumbs": [
      "Home",
      "Getting Started",
      "About the Bootcamp"
    ]
  },
  {
    "objectID": "02-r-lang.html",
    "href": "02-r-lang.html",
    "title": "02 R language",
    "section": "",
    "text": "Choose your language carefully",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "02 R language"
    ]
  },
  {
    "objectID": "02-r-lang.html#r-syntax-basics-r-as-a-passive-aggressive-butler",
    "href": "02-r-lang.html#r-syntax-basics-r-as-a-passive-aggressive-butler",
    "title": "02 R language",
    "section": "1 R Syntax basics (R as a passive-aggressive butler)",
    "text": "1 R Syntax basics (R as a passive-aggressive butler)\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this lesson, you will be able to:\n\nNavigate an example script, comments, help, pseudocode\nUse math operators\nUse logical Boolean operators\nDiscuss differences between “base R” and the Tidyverse\n\n\n\n\n\nWe love the R language, but sometimes it is a little bit like talking to a passive aggressive butler - if you aren’t careful with your language, the interaction may have unexpected outcomes…\n\nR is a very popular statistical programming language and open source software design to help scientists and other non-programmers perform statistical analyses and to make great graphs. This page is intended to guide people through some of the basics of the R programming language, just enough to get started. We hope that these pages help make learning R simple (though it can difficult at times while you are learning)",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "02 R language"
    ]
  },
  {
    "objectID": "02-r-lang.html#example-script-comments-help-pseudocode",
    "href": "02-r-lang.html#example-script-comments-help-pseudocode",
    "title": "02 R language",
    "section": "2 Example script, comments, help, pseudocode",
    "text": "2 Example script, comments, help, pseudocode\n\nThe most important thing to keep in mind right at the beginning of learning R is to view the script as something you are writing to document a piece of work (e.g. progress in a workshop like this, an analysis, a research project, etc.). Organizing every script you write is extremely important to build “good habits” for reproducibility of your work. A good guideline for best practice in writing scripts is to pretend you are writing the script, comments and contents, for a respected colleague - someone who you respect and want to impress, but might not be able to explain in person about the purpose of the script (even if this is your future self).\nTo get the most out of this page, we strongly recommend that you:\n\nWork through the instructions here while using R and RStudio as you go along.\nType you own code rather than using copy and paste\nDocument all the code in your own script and write clear, concise comments\n\n2.1 Example script\nDownload this example script.\nSave it in a logical place on your computer. Open it in RStudio.\n\nRight at the top you should see the HEADER.\n## HEADER ####\n## Who: &lt;YOUR NAME&gt;\n## What: 2 R language basics\n## Last edited: &lt;DATE TODAY in yyyy-mm-dd format)\n####\n\nGo ahead and fill in the header with your own information.\nNext you should see the CONTENTS section. The idea here is for you to list the CONTENTS of a script that can act as a roadmap to the user, but that can also help organize a large project into manageable chunks. Each title under the CONTENTS section will become the title in a code chunk below. You should see the contents section in the script you downloaded like this:\n## CONTENTS ####\n## 2 Example script, help, pseudocode  \n## 3 Math operators  \n## 4 Logical Boolean operators  \n## 5 Regarding base R and the Tidyverse   \n## 6 Practice exercises  \n\nEach item in the CONTENTS section will become the title of individual code chunks. RStudio recognizes code chunks that have a particular syntax:\nCode chunks begin with 2 “##” signs (1 works too, we prefer 2)\nCode chunks end with at least 4 “####” signs\nThe first code chunk should already be placed in your script\n## 2 Example script, help, pseudocode  ####\n\n2.2 Help\nOne of the great things about using R is the community and the spirit of helping others. However, there are so many websites, books, blogs and other resources that it can be overwhelming. Best practice is to learn to use the R Help system first, then seek help elsewhere.\nThe basic way to access the built-in help in R, is to use the help() function, with the name of tool you need help using inside the brackets. For example, to calculate the mean of some numbers, we would use the function mean(), and to display the help for the mean() function we would run. Run the following code in your own script:\n# Display help page for the function mean\nhelp(mean)\n\nYou should see:\n\n\n\nMean help page (pun intended)\n\n\n\nDoes this help look mean enough?\nLet’s orient to the information that is here because the help pages are essential to understand and every help page on every subject is organised in exactly the same way (and we will practice a lot using them).\n\n1 Function name {Package name} This field let’s you know what “R package” the function belongs to. We can ignore this for now, but it can be very useful.\n2 Short description This tells you in a few words what the function does.\n3 (longer) description This gives a longer description of what the function does\n4 Usage This usually gives an example of the function in use and lists the “arguments” that you are required to supply to the function for it to work on. Of course, you need to know about the arguments…\n5 Argument definitions This field tells you what the argument are and do!\n\n2.3 Deeper help\nUsing the Usage and Argument fields, we can figure out how to make the function do the work we want.\n# Under Usage:\n# mean(x, ...)\n\n# The \"x\" is an argument that is required\n# The \"...\" means there are other optional arguments\n\n# Under Arguments:\n\n# x \n# An R object... for numeric/logical vectors ...\n\n# try this code in your own script\nmy_length &lt;- c(101, 122, 97) # 3 numerical measures\nmean(x = my_length) \n\n2.4 Pseudocode\nThe idea of pseudocode is to break up a big task into a series of smaller tasks. An example of a task might be ANALYZE YOUR DATA (in shouty capitals because it is a big task). To accomplish this task, we might have to walk through a series of steps, e.g.,\nAnalyze your data:\n\nRead data into R\nTest assumption for statistical testing\nGraph the data\nPerform statistical test\nOrganize outputs to communicate in report\n\n\nIt is often a good idea to break down a task into pseudocode both to organize and document the methods in a logical way, but also to conceptually simplify a problem that is difficult to solve. Practically, the items in a typical table of contents in an R script might be similar to psuedocode. Note that this technique extends very well to any problem, not just R code and programming.\nE.g., what steps would be involved in a problem like: Send a rocket with people in it to Mars such that they survive and return to Earth.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "02 R language"
    ]
  },
  {
    "objectID": "02-r-lang.html#math-operators",
    "href": "02-r-lang.html#math-operators",
    "title": "02 R language",
    "section": "3 Math operators",
    "text": "3 Math operators\nBasic manipulation of numbers in R is very easy to do and is so intuitive that you may be able to guess what they are and what they do. There are just a few specifics that we will practice. This list is not exhaustive; the goal is to get enough to begin practicing.\n\n3.1 Arithmetic\nTry these in your practice script:\n# There are a few others, but these are the basics\n\n# Add with \"+\"\n2 + 5\n\n# Subtract with \"-\"\n10 - 15\n\n# Multiply with \"*\"\n6 * 4.2\n\n# Divide by \"/\"\n\n10 / 4\n\n# raise to the power of x\n2^3 \n9^(1/2) # same as sqrt()!\n\nYour output should look similar to this:\n\n# There are a few others, but these are the basics\n\n# Add with \"+\"\n2 + 5\n\n[1] 7\n\n# Subtract with \"-\"\n10 - 15\n\n[1] -5\n\n# Multiply with \"*\"\n6 * 4.2\n\n[1] 25.2\n\n# Divide by \"/\"\n\n10 / 4\n\n[1] 2.5\n\n# raise to the power of x\n2^3 \n\n[1] 8\n\n9^(1/2) # same as sqrt()!\n\n[1] 3\n\n\n\nOrder of operation\nThe “order of operation” refers to the order in which mathematical calculations are carried out. A phrase like 2 + 2 is simple, but we need to consider order for more complicated phrases like 2 + 2 * 8 - 6. In general multiplication and division are carried out before addition and subtraction unless specific order is coded.\n# Try this\n\n4 + 2 * 3\n\n# Order control - same\n4 + (2 * 3)\n\n# Order control - different...\n(4 + 2) * 3\n\n3.2 Use of spaces\nIn some cases, the use of spaces does not matter in the R language. Which one of the following ways of writing math operation might be easier to document and read?\n# Try this\n\n6+10                                  # no spaces\n7     -5                              # uneven spaces\n1.6             /                2.3  # large spaces\n16 * 3                                # exactly 1 space\n\n# exactly 1 space is probably easiest to read...\nYour output should look like this:\n\n# Try this\n\n6+10                                  # no spaces\n\n[1] 16\n\n7     -5                              # uneven spaces\n\n[1] 2\n\n1.6             /                2.3  # large spaces\n\n[1] 0.6956522\n\n16 * 3                                # exactly 1 space\n\n[1] 48",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "02 R language"
    ]
  },
  {
    "objectID": "02-r-lang.html#logical-boolean-operators",
    "href": "02-r-lang.html#logical-boolean-operators",
    "title": "02 R language",
    "section": "4 Logical “Boolean” operators",
    "text": "4 Logical “Boolean” operators\nBoolean operators are expressions that resolve TRUE (treated as 1 in most computing systems including R) versus FALSE (0). A typical expression might be something like asking if 5 &gt; 3, which is TRUE. More sophisticated phrases are possible, and sometimes useful.\n4.1 Boolean example\n# Try this\n# simplest example\n3 &gt; 5\n\n# 3 is compared to each element\n3 &lt; c(1, 2, 3, 4, 5, 6) \n\n# Logic and math\n# & (ampersand) means \"and\"\n# | (pipe) means \"or\"\n\n# This asks if both phrases are true (true AND true)\n# notice \"TRUE\" has a special meaning in R\n\nTRUE & TRUE # both phrases are the same and true, TRUE\n\n3 &gt; 1 & 1 &lt; 5 # both phrases are true\n\n# Are these phrases true?\n\nTRUE & FALSE # are both true?\n\nFALSE & FALSE # are both true?\n\nBoolean expressions are often used to select groups of data, for example asking whether values in a column of variables are greater than some threshold.\n4.2 Selecting with Booleans\nWe often use Booleans to select particular parts of our data in a powerful way, as an alternative to creating different versions of a particular dataset.\n# Try this\n\n# Put some data into a variable and then print the variable\n# Note \"&lt;-\" is the ASSIGNMENT syntax in R, which puts the value on the left \"into\" x\n\nx &lt;- c(21, 3, 5, 6, 22)\nx\n\nx &gt; 20\n\n# the square brackets act as the index for the data vector\nx[x &gt; 20]\n\n4.3 The “not” operator, ! (Sorry !Sorry)**\nThe ! operator sets a Boolean value to the opposite. This is sometimes used when an expression can be made simpler by representing it as an opposite. For now we will just demonstrate how it works.\n# Try this\nTRUE # plain true\n\n!FALSE # not false is true!\n\n6 &lt; 5 #definitely false\n\n!(6 &lt; 5) #not false...\n\n!(c(23, 44, 16, 51, 12) &gt; 50)",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "02 R language"
    ]
  },
  {
    "objectID": "02-r-lang.html#base-r-versus-the-tidyverse",
    "href": "02-r-lang.html#base-r-versus-the-tidyverse",
    "title": "02 R language",
    "section": "5 “Base R” versus the ‘Tidyverse’",
    "text": "5 “Base R” versus the ‘Tidyverse’\n5.1 Base R\n\nThe R language as it was invented and continues to be developed is extremely popular, powerful and easy to use, especially for people without formal programming training or experience in different computing languages. In general, we refer to this pure form of R as Base R.\nSince the late 1990s, the R-user community and Base R resources on have exploded on the web and this form of the language continues to be extremely popular for experts and beginners alike. If interested, you can read more here.\n\n5.2 Tidyverse R\n\nRelatively recently, a second version of R has evolved that puts forward different conventions in the R lanuage. The differences are practical, but also philosophical. This form of R use is generally referred to as The Tidyverse. The Tidyverse is extremely powerful and we love it. However, we feel that it is far more efficient to first learn base R for non-programmers, before learning the Tidyverse, and we will exclusively use base R for this bootcamp.\nThere is some disagreement over which “version” of R is better or easier to learn and teach with. You will definitely encounter the Tidyverse at some point and eventually you can choose how much or how little you will use it.\n\nThe Argument for Base R\nThe Argument for the Tidyverse",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "02 R language"
    ]
  },
  {
    "objectID": "02-r-lang.html#practice-exercises",
    "href": "02-r-lang.html#practice-exercises",
    "title": "02 R language",
    "section": "6 Practice exercises",
    "text": "6 Practice exercises\n\n6.1 Script Sections\nName and describe the purpose of the first 2 sections that should be present in every R script\n\n\n\n\n\n\n\nScript Structure\n\n\n\n\n\n\n# The first two sections that should be present in every R script are:\n\n# 1. HEADER - Contains your name, the date of the most recent edit, and a short description \n#    of the PURPOSE of the script. This helps document who created the script and when.\n\n# 2. CONTENTS - Provides a road map for the analysis, listing all the major sections \n#    of the script. This helps organize the script and makes navigation easier.\n\nThese sections help make your scripts more organized, reproducible, and easier for others (or your future self) to understand.\n\n\n\n\n6.2 Boxplot Subset\nWhat is the purpose of “subset” argument in the boxplot() function (hint: use help())\n\n\n\n\n\n\n\nBoxplot Subset Parameter\n\n\n\n\n\n\n# Using help(boxplot) reveals that the \"subset\" argument allows you to:\n# Select a subset of observations to plot. It specifies which observations \n# are used to produce the boxplot.\n\n# For example:\ndata(mtcars)\n# This would create a boxplot using only the rows where cyl equals 6:\nboxplot(mpg ~ cyl, data = mtcars, subset = cyl == 6)\n\nThe subset parameter lets you create boxplots using only specific observations that meet certain criteria, without having to create a separate dataset.\n\n\n\n\n6.3 R Expression\nWrite an expression using good R spacing syntax that takes the sum of 3, 6, and 12 and divides it by 25\n\n\n\n\n\n\n\nSum and Division Expression\n\n\n\n\n\n\n# Using good spacing syntax:\n(3 + 6 + 12) / 25\n\n[1] 0.84\n\n\nThe expression adds 3, 6, and 12 (total 21) and then divides by 25, resulting in 0.84.\n\n\n\n\n6.4 Cylinder Volume\nWrite pseudocode steps for calculating the volume of a cylinder (hint, if you do not know it by heart, you may need to research the equation for the volume of a cylinder!). For a cylinder of height = 3.2 cm and end radius of 5.5 cm, report the volume in cm to 2 decimal points of accuracy. Use at least 3 decimal points of accuracy for pi (hint, the quantity named pi is a standard variable in R!).\n\n\n\n\n\n\n\nCylinder Volume Calculation\n\n\n\n\n\n\n# Pseudocode steps:\n# 1. Calculate the area of the circular end: pi * radius^2\n# 2. Multiply the area by the height to get volume\n\n# Implementation:\nradius &lt;- 5.5\nheight &lt;- 3.2\n\n# Calculate the area of the circle\ncircle_area &lt;- pi * radius^2\n\n# Calculate the volume of the cylinder\nvolume &lt;- circle_area * height\n\n# Display the result with 2 decimal points\nround(volume, 2)\n\n[1] 304.11\n\n\nThe volume of a cylinder with radius 5.5 cm and height 3.2 cm is 304.34 cm³ (rounded to 2 decimal places).\n\n\n\n\n6.5 Boolean Evaluation\nExecute the code and explain the outcome in comments.\nTRUE & 3 &lt; 5 & 6 &gt; 2 & !FALSE\n\n\n\n\n\n\n\nBoolean Expression Evaluation\n\n\n\n\n\n\n# Let's break down the expression:\nTRUE & 3 &lt; 5 & 6 &gt; 2 & !FALSE\n\n[1] TRUE\n\n# Each part evaluates to:\n# TRUE - is TRUE\n# 3 &lt; 5 - is TRUE (3 is less than 5)\n# 6 &gt; 2 - is TRUE (6 is greater than 2)\n# !FALSE - is TRUE (not FALSE is TRUE)\n\n# Since all components are TRUE, and they're connected with & (AND) operators,\n# the entire expression evaluates to TRUE\n\nThe expression evaluates to TRUE because all individual conditions are TRUE, and they’re connected with AND operators.\n\n\n\n\n6.6 Not Operator\nWrite a plausible practice question involving the use of the not Boolean operator, !.\n\n\n\n\n\n\n\nNot Operator Question\n\n\n\n\n\n\n# A plausible practice question could be:\n\n# \"Create a logical vector that identifies all elements in the vector c(15, 8, 22, 7, 19) \n# that are NOT greater than 10.\"\n\n# Solution:\nnumbers &lt;- c(15, 8, 22, 7, 19)\n!(numbers &gt; 10)\n\n[1] FALSE  TRUE FALSE  TRUE FALSE\n\n# This returns: FALSE TRUE FALSE TRUE FALSE\n# Indicating which elements are NOT greater than 10 (the 8 and 7)\n\nThis question tests understanding of the NOT operator by asking to identify elements that do NOT meet a certain condition.\n\n\n\nSCRIPT 2 Example SOLUTIONS",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "02 R language"
    ]
  },
  {
    "objectID": "slides/topics/15_github_basics.html",
    "href": "slides/topics/15_github_basics.html",
    "title": "Git and GitHub",
    "section": "",
    "text": "Understand basics of version control with Git\nSet up Git on your computer\nCreate GitHub account and repository\nPerform basic Git operations (clone, add, commit, push, pull)\n\n\n\n\n\nSystem recording changes to files over time\nRecall specific versions later when needed\nTrack changes to code and documents\nCollaborate without overwriting others’ work\n\n\n\n\n\nHistory tracking documents analysis evolution\nSeamless collaboration with team members\nSecure cloud backup of code\nReproducibility through exact code access\n\n\n\n\n\nGit: distributed version control system tracking file changes\nGitHub: web-based platform hosting Git repositories\nGitHub adds collaboration features to Git\nEssential tools for modern data science\n\n\n\n\n\nWindows: download from git-scm.com\nMac: install via Homebrew or download from git-scm.com\nLinux: use package manager (apt-get, yum)\nConfigure with name and email using git config\n\n\n\n\n\nRepository (repo): directory where Git tracks changes\nCommit: snapshot of changes at point in time\nBranch: parallel version of repository\nRemote: repository version hosted elsewhere\n\n\n\n\n\nGitHub: click “+” icon, select “New repository”\nLocal computer: mkdir project, cd project, git init\nConnect local to GitHub with git remote add origin\nInitialize with README and appropriate settings\n\n\n\n\n\nMake changes to files in repository\nStage changes for commit with git add\nCommit changes with descriptive message\nPush changes to GitHub with git push\n\n\n\n\n\ngit status: check repository status\ngit add: stage files for commit\ngit commit -m: commit with message\ngit push: send commits to remote repository\n\n\n\n\n\ngit clone: create local copy of remote repository\nNavigate into repository after cloning\ngit pull: get latest changes from remote\nEssential for collaboration and staying updated\n\n\n\n\n\nBranches allow work on different features safely\ngit branch: create new branch\ngit checkout: switch between branches\ngit merge: combine branch changes"
  },
  {
    "objectID": "slides/topics/15_github_basics.html#learning-objectives",
    "href": "slides/topics/15_github_basics.html#learning-objectives",
    "title": "Git and GitHub",
    "section": "",
    "text": "Understand basics of version control with Git\nSet up Git on your computer\nCreate GitHub account and repository\nPerform basic Git operations (clone, add, commit, push, pull)"
  },
  {
    "objectID": "slides/topics/15_github_basics.html#what-is-version-control",
    "href": "slides/topics/15_github_basics.html#what-is-version-control",
    "title": "Git and GitHub",
    "section": "",
    "text": "System recording changes to files over time\nRecall specific versions later when needed\nTrack changes to code and documents\nCollaborate without overwriting others’ work"
  },
  {
    "objectID": "slides/topics/15_github_basics.html#why-use-git-and-github",
    "href": "slides/topics/15_github_basics.html#why-use-git-and-github",
    "title": "Git and GitHub",
    "section": "",
    "text": "History tracking documents analysis evolution\nSeamless collaboration with team members\nSecure cloud backup of code\nReproducibility through exact code access"
  },
  {
    "objectID": "slides/topics/15_github_basics.html#git-vs-github",
    "href": "slides/topics/15_github_basics.html#git-vs-github",
    "title": "Git and GitHub",
    "section": "",
    "text": "Git: distributed version control system tracking file changes\nGitHub: web-based platform hosting Git repositories\nGitHub adds collaboration features to Git\nEssential tools for modern data science"
  },
  {
    "objectID": "slides/topics/15_github_basics.html#installing-and-configuring-git",
    "href": "slides/topics/15_github_basics.html#installing-and-configuring-git",
    "title": "Git and GitHub",
    "section": "",
    "text": "Windows: download from git-scm.com\nMac: install via Homebrew or download from git-scm.com\nLinux: use package manager (apt-get, yum)\nConfigure with name and email using git config"
  },
  {
    "objectID": "slides/topics/15_github_basics.html#key-git-concepts",
    "href": "slides/topics/15_github_basics.html#key-git-concepts",
    "title": "Git and GitHub",
    "section": "",
    "text": "Repository (repo): directory where Git tracks changes\nCommit: snapshot of changes at point in time\nBranch: parallel version of repository\nRemote: repository version hosted elsewhere"
  },
  {
    "objectID": "slides/topics/15_github_basics.html#creating-repositories",
    "href": "slides/topics/15_github_basics.html#creating-repositories",
    "title": "Git and GitHub",
    "section": "",
    "text": "GitHub: click “+” icon, select “New repository”\nLocal computer: mkdir project, cd project, git init\nConnect local to GitHub with git remote add origin\nInitialize with README and appropriate settings"
  },
  {
    "objectID": "slides/topics/15_github_basics.html#basic-git-workflow",
    "href": "slides/topics/15_github_basics.html#basic-git-workflow",
    "title": "Git and GitHub",
    "section": "",
    "text": "Make changes to files in repository\nStage changes for commit with git add\nCommit changes with descriptive message\nPush changes to GitHub with git push"
  },
  {
    "objectID": "slides/topics/15_github_basics.html#essential-git-commands",
    "href": "slides/topics/15_github_basics.html#essential-git-commands",
    "title": "Git and GitHub",
    "section": "",
    "text": "git status: check repository status\ngit add: stage files for commit\ngit commit -m: commit with message\ngit push: send commits to remote repository"
  },
  {
    "objectID": "slides/topics/15_github_basics.html#cloning-and-pulling",
    "href": "slides/topics/15_github_basics.html#cloning-and-pulling",
    "title": "Git and GitHub",
    "section": "",
    "text": "git clone: create local copy of remote repository\nNavigate into repository after cloning\ngit pull: get latest changes from remote\nEssential for collaboration and staying updated"
  },
  {
    "objectID": "slides/topics/15_github_basics.html#branching-and-merging",
    "href": "slides/topics/15_github_basics.html#branching-and-merging",
    "title": "Git and GitHub",
    "section": "",
    "text": "Branches allow work on different features safely\ngit branch: create new branch\ngit checkout: switch between branches\ngit merge: combine branch changes"
  },
  {
    "objectID": "slides/topics/14_rmarkdown.html",
    "href": "slides/topics/14_rmarkdown.html",
    "title": "R Markdown",
    "section": "",
    "text": "Understand the basics of R Markdown\nCreate documents combining code, output, and narrative text\nFormat text using Markdown syntax\nGenerate reports in multiple formats (HTML, PDF, Word)\n\n\n\n\n\nFile format combining R code, output, and narrative text\nPowerful tool for reproducible reports, presentations, dashboards\nThree key elements: YAML header, Markdown text, Code chunks\nPlain text files with special formatting syntax\n\n\n\n\n\nCreate new R Markdown in RStudio: File → New File → R Markdown\nChoose document type (HTML, PDF, Word)\nProvide title and author information\nTemplate generated for modification\n\n\n\n\n\nAppears at top between triple dashes (—)\nContains document metadata and formatting options\nCustomize output format, table of contents, themes\nCan include dynamic dates and advanced formatting\n\n\n\n\n\nLightweight markup language for text formatting\nHeaders using # symbols (# Header 1, ## Header 2)\nBold text with text, italic with text\nLinks, images, lists, and tables with simple syntax\n\n\n\n\n\nExecute R code and display results in document\nEnclosed in triple backticks with {r} designation\nControl behavior with chunk options\nInclude chunk names for organization and referencing\n\n\n\n\n\neval: whether to evaluate code (TRUE/FALSE)\necho: whether to show code (TRUE/FALSE)\ninclude: whether to include chunk in output\nmessage/warning: control display of messages and warnings\n\n\n\n\n\nInclude R code directly within text using backticks\nFormat: r code_here within sentences\nCalculates results and inserts into text\nUseful for dynamic reporting of values\n\n\n\n\n\nHTML: interactive, customizable, web-friendly\nPDF: formal appearance, requires LaTeX installation\nWord: familiar format for collaboration and editing\nMultiple formats can be specified in single document\n\n\n\n\n\nClick Knit button in RStudio\nR Markdown runs all code chunks\nGenerates outputs (tables, plots, etc.)\nFormats text according to Markdown syntax\n\n\n\n\n\nTable of contents with toc: true\nCode folding for interactive documents\nCustom themes and syntax highlighting\nFloating TOC and section numbering"
  },
  {
    "objectID": "slides/topics/14_rmarkdown.html#learning-objectives",
    "href": "slides/topics/14_rmarkdown.html#learning-objectives",
    "title": "R Markdown",
    "section": "",
    "text": "Understand the basics of R Markdown\nCreate documents combining code, output, and narrative text\nFormat text using Markdown syntax\nGenerate reports in multiple formats (HTML, PDF, Word)"
  },
  {
    "objectID": "slides/topics/14_rmarkdown.html#what-is-r-markdown",
    "href": "slides/topics/14_rmarkdown.html#what-is-r-markdown",
    "title": "R Markdown",
    "section": "",
    "text": "File format combining R code, output, and narrative text\nPowerful tool for reproducible reports, presentations, dashboards\nThree key elements: YAML header, Markdown text, Code chunks\nPlain text files with special formatting syntax"
  },
  {
    "objectID": "slides/topics/14_rmarkdown.html#getting-started",
    "href": "slides/topics/14_rmarkdown.html#getting-started",
    "title": "R Markdown",
    "section": "",
    "text": "Create new R Markdown in RStudio: File → New File → R Markdown\nChoose document type (HTML, PDF, Word)\nProvide title and author information\nTemplate generated for modification"
  },
  {
    "objectID": "slides/topics/14_rmarkdown.html#yaml-header-structure",
    "href": "slides/topics/14_rmarkdown.html#yaml-header-structure",
    "title": "R Markdown",
    "section": "",
    "text": "Appears at top between triple dashes (—)\nContains document metadata and formatting options\nCustomize output format, table of contents, themes\nCan include dynamic dates and advanced formatting"
  },
  {
    "objectID": "slides/topics/14_rmarkdown.html#markdown-text-formatting",
    "href": "slides/topics/14_rmarkdown.html#markdown-text-formatting",
    "title": "R Markdown",
    "section": "",
    "text": "Lightweight markup language for text formatting\nHeaders using # symbols (# Header 1, ## Header 2)\nBold text with text, italic with text\nLinks, images, lists, and tables with simple syntax"
  },
  {
    "objectID": "slides/topics/14_rmarkdown.html#code-chunks",
    "href": "slides/topics/14_rmarkdown.html#code-chunks",
    "title": "R Markdown",
    "section": "",
    "text": "Execute R code and display results in document\nEnclosed in triple backticks with {r} designation\nControl behavior with chunk options\nInclude chunk names for organization and referencing"
  },
  {
    "objectID": "slides/topics/14_rmarkdown.html#chunk-options-control",
    "href": "slides/topics/14_rmarkdown.html#chunk-options-control",
    "title": "R Markdown",
    "section": "",
    "text": "eval: whether to evaluate code (TRUE/FALSE)\necho: whether to show code (TRUE/FALSE)\ninclude: whether to include chunk in output\nmessage/warning: control display of messages and warnings"
  },
  {
    "objectID": "slides/topics/14_rmarkdown.html#inline-code",
    "href": "slides/topics/14_rmarkdown.html#inline-code",
    "title": "R Markdown",
    "section": "",
    "text": "Include R code directly within text using backticks\nFormat: r code_here within sentences\nCalculates results and inserts into text\nUseful for dynamic reporting of values"
  },
  {
    "objectID": "slides/topics/14_rmarkdown.html#output-formats",
    "href": "slides/topics/14_rmarkdown.html#output-formats",
    "title": "R Markdown",
    "section": "",
    "text": "HTML: interactive, customizable, web-friendly\nPDF: formal appearance, requires LaTeX installation\nWord: familiar format for collaboration and editing\nMultiple formats can be specified in single document"
  },
  {
    "objectID": "slides/topics/14_rmarkdown.html#document-generation-process",
    "href": "slides/topics/14_rmarkdown.html#document-generation-process",
    "title": "R Markdown",
    "section": "",
    "text": "Click Knit button in RStudio\nR Markdown runs all code chunks\nGenerates outputs (tables, plots, etc.)\nFormats text according to Markdown syntax"
  },
  {
    "objectID": "slides/topics/14_rmarkdown.html#advanced-features",
    "href": "slides/topics/14_rmarkdown.html#advanced-features",
    "title": "R Markdown",
    "section": "",
    "text": "Table of contents with toc: true\nCode folding for interactive documents\nCustom themes and syntax highlighting\nFloating TOC and section numbering"
  },
  {
    "objectID": "slides/topics/06_data_subsetting.html",
    "href": "slides/topics/06_data_subsetting.html",
    "title": "Data Subsetting",
    "section": "",
    "text": "Describe indexing concept for vectors and matrices\nUse which() function to subset data effectively\nSelect specific parts of data frames\nUse aggregate() function to summarize data\n\n\n\n\n\nSubsetting and manipulating data is core activity\nEssential for exploratory data analysis\nUsed extensively in data acquisition and analysis\nFoundation for database queries and data manipulation\n\n\n\n\n\nData objects store values accessible by “address”\nVectors (1D), matrices (2D), arrays (3D+) have indices\nIndex values correspond to position of data\nEssential for slicing and dicing data\n\n\n\n\n\nVector indices run from 1 to length of vector\nUse [1:i] notation to access elements\nR displays index in console output [1]\nAccess single elements or ranges with brackets\n\n\n\n\n\nTwo dimensions: [rows, columns] or [i, j]\nSlice out entire rows or columns with blank space\nCan name rows and columns for easier access\nUse names in quotes for selection by name\n\n\n\n\n\nThree or more dimensions: [i, j, k, …]\nEach dimension represents different data organization\nMore complex but follows same principles\nUseful for multi-dimensional data analysis\n\n\n\n\n\nReturns index values where logical expression is TRUE\nRequires logical vector as input argument\nPowerful for constructing data queries\nEssential tool for conditional data selection\n\n\n\n\n\nCreate logical expressions to identify data subsets\nUse which() to get index positions of TRUE values\nApply indices to extract desired data values\nAlternative to creating multiple dataset versions\n\n\n\n\n\nSelect rows based on values in one or more variables\nCombine multiple conditions with & (and) and | (or)\nUse which() with complex boolean expressions\nPowerful method for data filtering and analysis\n\n\n\n\n\nChain multiple conditions with logical operators\nUse parentheses to control evaluation order\nBuild complex selection rules step by step\nEssential for sophisticated data manipulation\n\n\n\n\n\nSummarize data by groups using specified functions\nx argument: numeric data to summarize\nby argument: list of grouping variables\nFUN argument: function to apply (mean, sd, etc.)\n\n\n\n\n\nCreate custom summary functions with function()\nApply multiple statistics simultaneously\nName output elements for clarity\nPowerful tool for grouped data analysis"
  },
  {
    "objectID": "slides/topics/06_data_subsetting.html#learning-objectives",
    "href": "slides/topics/06_data_subsetting.html#learning-objectives",
    "title": "Data Subsetting",
    "section": "",
    "text": "Describe indexing concept for vectors and matrices\nUse which() function to subset data effectively\nSelect specific parts of data frames\nUse aggregate() function to summarize data"
  },
  {
    "objectID": "slides/topics/06_data_subsetting.html#data-sumo-philosophy",
    "href": "slides/topics/06_data_subsetting.html#data-sumo-philosophy",
    "title": "Data Subsetting",
    "section": "",
    "text": "Subsetting and manipulating data is core activity\nEssential for exploratory data analysis\nUsed extensively in data acquisition and analysis\nFoundation for database queries and data manipulation"
  },
  {
    "objectID": "slides/topics/06_data_subsetting.html#indexing-concept-basics",
    "href": "slides/topics/06_data_subsetting.html#indexing-concept-basics",
    "title": "Data Subsetting",
    "section": "",
    "text": "Data objects store values accessible by “address”\nVectors (1D), matrices (2D), arrays (3D+) have indices\nIndex values correspond to position of data\nEssential for slicing and dicing data"
  },
  {
    "objectID": "slides/topics/06_data_subsetting.html#vector-indexing",
    "href": "slides/topics/06_data_subsetting.html#vector-indexing",
    "title": "Data Subsetting",
    "section": "",
    "text": "Vector indices run from 1 to length of vector\nUse [1:i] notation to access elements\nR displays index in console output [1]\nAccess single elements or ranges with brackets"
  },
  {
    "objectID": "slides/topics/06_data_subsetting.html#matrix-indexing",
    "href": "slides/topics/06_data_subsetting.html#matrix-indexing",
    "title": "Data Subsetting",
    "section": "",
    "text": "Two dimensions: [rows, columns] or [i, j]\nSlice out entire rows or columns with blank space\nCan name rows and columns for easier access\nUse names in quotes for selection by name"
  },
  {
    "objectID": "slides/topics/06_data_subsetting.html#array-indexing",
    "href": "slides/topics/06_data_subsetting.html#array-indexing",
    "title": "Data Subsetting",
    "section": "",
    "text": "Three or more dimensions: [i, j, k, …]\nEach dimension represents different data organization\nMore complex but follows same principles\nUseful for multi-dimensional data analysis"
  },
  {
    "objectID": "slides/topics/06_data_subsetting.html#the-which-function",
    "href": "slides/topics/06_data_subsetting.html#the-which-function",
    "title": "Data Subsetting",
    "section": "",
    "text": "Returns index values where logical expression is TRUE\nRequires logical vector as input argument\nPowerful for constructing data queries\nEssential tool for conditional data selection"
  },
  {
    "objectID": "slides/topics/06_data_subsetting.html#boolean-selection-strategies",
    "href": "slides/topics/06_data_subsetting.html#boolean-selection-strategies",
    "title": "Data Subsetting",
    "section": "",
    "text": "Create logical expressions to identify data subsets\nUse which() to get index positions of TRUE values\nApply indices to extract desired data values\nAlternative to creating multiple dataset versions"
  },
  {
    "objectID": "slides/topics/06_data_subsetting.html#data-frame-selection",
    "href": "slides/topics/06_data_subsetting.html#data-frame-selection",
    "title": "Data Subsetting",
    "section": "",
    "text": "Select rows based on values in one or more variables\nCombine multiple conditions with & (and) and | (or)\nUse which() with complex boolean expressions\nPowerful method for data filtering and analysis"
  },
  {
    "objectID": "slides/topics/06_data_subsetting.html#compound-boolean-operations",
    "href": "slides/topics/06_data_subsetting.html#compound-boolean-operations",
    "title": "Data Subsetting",
    "section": "",
    "text": "Chain multiple conditions with logical operators\nUse parentheses to control evaluation order\nBuild complex selection rules step by step\nEssential for sophisticated data manipulation"
  },
  {
    "objectID": "slides/topics/06_data_subsetting.html#the-aggregate-function",
    "href": "slides/topics/06_data_subsetting.html#the-aggregate-function",
    "title": "Data Subsetting",
    "section": "",
    "text": "Summarize data by groups using specified functions\nx argument: numeric data to summarize\nby argument: list of grouping variables\nFUN argument: function to apply (mean, sd, etc.)"
  },
  {
    "objectID": "slides/topics/06_data_subsetting.html#custom-functions-in-aggregate",
    "href": "slides/topics/06_data_subsetting.html#custom-functions-in-aggregate",
    "title": "Data Subsetting",
    "section": "",
    "text": "Create custom summary functions with function()\nApply multiple statistics simultaneously\nName output elements for clarity\nPowerful tool for grouped data analysis"
  },
  {
    "objectID": "slides/topics/01_setup_intro.html",
    "href": "slides/topics/01_setup_intro.html",
    "title": "Setup & Introduction",
    "section": "",
    "text": "Understand how the R Stats Bootcamp works\nDescribe the motivation for using R\nInstall R and RStudio or set up RStudio Cloud\nNavigate RStudio components and interface\n\n\n\n\n\nProvides practical, open instructional materials for learning R\nReviews simple statistics in R\nIntroduces reproducibility and collaboration tools\nBlend of practical material with videos and self-assessment\n\n\n\n\n\nDesigned for non-programmers to perform statistical analysis\nGrowing popularity in universities, companies, and research\nLarge community of users and high demand for R skills\nFree, open source, and cross-platform compatible\n\n\n\n\n\nOption 1: Download R from CRAN and RStudio Desktop\nOption 2: Use RStudio Cloud for browser-based work\nInstall R first, then RStudio for best results\nConsider your computer setup and preferences\n\n\n\n\n\nScript window (upper left) for writing and executing code\nConsole window (lower left) for output and messages\nGlobal Environment (upper right) for data objects\nPlots window (lower right) for graphics and help\n\n\n\n\n\nDocuments analysis for exact reproducibility\nInterface between your commands and R software\nShould be clear for future collaborators\nThink of future self as respected colleague\n\n\n\n\n\nHeader with name, date, and purpose\nContents section as roadmap for analysis\nCode chunks with descriptive titles\nConsistent numbering and logical organization\n\n\n\n\n\nCtrl+Enter (Cmd+Return on Mac) for current line\nSelect code and Ctrl+Enter for specific sections\nUse Run button in Script window\nAccess through Code menu for additional options\n\n\n\n\n\nUse # character for comment lines\nExplain code like methods section of scientific paper\nProvide enough detail for exact replication\nBalance thoroughness with conciseness\n\n\n\n\n\nDownload and examine sample scripts\nPractice running code and examining output\nAdd code chunks with proper comments\nCreate variables and calculate basic statistics"
  },
  {
    "objectID": "slides/topics/01_setup_intro.html#learning-objectives",
    "href": "slides/topics/01_setup_intro.html#learning-objectives",
    "title": "Setup & Introduction",
    "section": "",
    "text": "Understand how the R Stats Bootcamp works\nDescribe the motivation for using R\nInstall R and RStudio or set up RStudio Cloud\nNavigate RStudio components and interface"
  },
  {
    "objectID": "slides/topics/01_setup_intro.html#how-the-r-stats-bootcamp-works",
    "href": "slides/topics/01_setup_intro.html#how-the-r-stats-bootcamp-works",
    "title": "Setup & Introduction",
    "section": "",
    "text": "Provides practical, open instructional materials for learning R\nReviews simple statistics in R\nIntroduces reproducibility and collaboration tools\nBlend of practical material with videos and self-assessment"
  },
  {
    "objectID": "slides/topics/01_setup_intro.html#r-motivation",
    "href": "slides/topics/01_setup_intro.html#r-motivation",
    "title": "Setup & Introduction",
    "section": "",
    "text": "Designed for non-programmers to perform statistical analysis\nGrowing popularity in universities, companies, and research\nLarge community of users and high demand for R skills\nFree, open source, and cross-platform compatible"
  },
  {
    "objectID": "slides/topics/01_setup_intro.html#installation-options",
    "href": "slides/topics/01_setup_intro.html#installation-options",
    "title": "Setup & Introduction",
    "section": "",
    "text": "Option 1: Download R from CRAN and RStudio Desktop\nOption 2: Use RStudio Cloud for browser-based work\nInstall R first, then RStudio for best results\nConsider your computer setup and preferences"
  },
  {
    "objectID": "slides/topics/01_setup_intro.html#rstudio-interface-components",
    "href": "slides/topics/01_setup_intro.html#rstudio-interface-components",
    "title": "Setup & Introduction",
    "section": "",
    "text": "Script window (upper left) for writing and executing code\nConsole window (lower left) for output and messages\nGlobal Environment (upper right) for data objects\nPlots window (lower right) for graphics and help"
  },
  {
    "objectID": "slides/topics/01_setup_intro.html#r-script-organization",
    "href": "slides/topics/01_setup_intro.html#r-script-organization",
    "title": "Setup & Introduction",
    "section": "",
    "text": "Documents analysis for exact reproducibility\nInterface between your commands and R software\nShould be clear for future collaborators\nThink of future self as respected colleague"
  },
  {
    "objectID": "slides/topics/01_setup_intro.html#script-structure-best-practices",
    "href": "slides/topics/01_setup_intro.html#script-structure-best-practices",
    "title": "Setup & Introduction",
    "section": "",
    "text": "Header with name, date, and purpose\nContents section as roadmap for analysis\nCode chunks with descriptive titles\nConsistent numbering and logical organization"
  },
  {
    "objectID": "slides/topics/01_setup_intro.html#code-execution-methods",
    "href": "slides/topics/01_setup_intro.html#code-execution-methods",
    "title": "Setup & Introduction",
    "section": "",
    "text": "Ctrl+Enter (Cmd+Return on Mac) for current line\nSelect code and Ctrl+Enter for specific sections\nUse Run button in Script window\nAccess through Code menu for additional options"
  },
  {
    "objectID": "slides/topics/01_setup_intro.html#comments-and-documentation",
    "href": "slides/topics/01_setup_intro.html#comments-and-documentation",
    "title": "Setup & Introduction",
    "section": "",
    "text": "Use # character for comment lines\nExplain code like methods section of scientific paper\nProvide enough detail for exact replication\nBalance thoroughness with conciseness"
  },
  {
    "objectID": "slides/topics/01_setup_intro.html#practice-exercises",
    "href": "slides/topics/01_setup_intro.html#practice-exercises",
    "title": "Setup & Introduction",
    "section": "",
    "text": "Download and examine sample scripts\nPractice running code and examining output\nAdd code chunks with proper comments\nCreate variables and calculate basic statistics"
  },
  {
    "objectID": "slides/topics/04_data_objects.html",
    "href": "slides/topics/04_data_objects.html",
    "title": "Data Objects",
    "section": "",
    "text": "Describe basic data types in R\nUse str() to inspect data objects effectively\nDescribe data with factors for categorical variables\nUse class() and convert between variable types\n\n\n\n\n\nImagine floating in R Global Environment\nAny visible data object can be called by name\nFundamental way to analyze data through code manipulation\nSystem of containers to store data effectively\n\n\n\n\n\nNumeric: numbers with decimal points\nInteger: whole numbers without decimals\nCharacter: text strings enclosed in quotes\nLogical: TRUE/FALSE boolean values\n\n\n\n\n\nMore abstract than visual spreadsheet representation\nCreate variables in code script files\nBring data from external files or web\nUse class() function to check variable types\n\n\n\n\n\nMost basic data structure in R\nCollection of elements of same type\nCreated using c() function\nOperations applied element-wise automatically\n\n\n\n\n\nNumeric vectors: c(1, 2, 3, 4)\nCharacter vectors: c(“apple”, “banana”, “cherry”)\nLogical vectors: c(TRUE, FALSE, TRUE)\nSequences: 1:10 or seq() function\n\n\n\n\n\nMust begin with letter, can contain numbers\nCannot contain spaces or forbidden characters\nShould be human-readable and consistent\nCase sensitive throughout R\n\n\n\n\n\nUsed for categorical data with limited categories\nNon-ordered factors: simple category names\nOrdered factors: specific sequence matters\nMust specify order explicitly when needed\n\n\n\n\n\nUse class() to check current data type\nConvert with as.numeric(), as.character(), as.factor()\nR usually guesses correctly but check when unsure\nSometimes manual conversion necessary\n\n\n\n\n\nVectors: single dimension storage (1 to i)\nMatrices: two dimensions (i rows, j columns)\nArrays: three or more dimensions\nAll must store same data type\n\n\n\n\n\nUse matrix() function with data, nrow, ncol\nCan name rows and columns with rownames(), colnames()\nAccess elements using bracket notation [i,j]\nUseful for mathematical operations and data organization"
  },
  {
    "objectID": "slides/topics/04_data_objects.html#learning-objectives",
    "href": "slides/topics/04_data_objects.html#learning-objectives",
    "title": "Data Objects",
    "section": "",
    "text": "Describe basic data types in R\nUse str() to inspect data objects effectively\nDescribe data with factors for categorical variables\nUse class() and convert between variable types"
  },
  {
    "objectID": "slides/topics/04_data_objects.html#data-objects-in-r-space",
    "href": "slides/topics/04_data_objects.html#data-objects-in-r-space",
    "title": "Data Objects",
    "section": "",
    "text": "Imagine floating in R Global Environment\nAny visible data object can be called by name\nFundamental way to analyze data through code manipulation\nSystem of containers to store data effectively"
  },
  {
    "objectID": "slides/topics/04_data_objects.html#basic-data-types",
    "href": "slides/topics/04_data_objects.html#basic-data-types",
    "title": "Data Objects",
    "section": "",
    "text": "Numeric: numbers with decimal points\nInteger: whole numbers without decimals\nCharacter: text strings enclosed in quotes\nLogical: TRUE/FALSE boolean values"
  },
  {
    "objectID": "slides/topics/04_data_objects.html#the-global-environment",
    "href": "slides/topics/04_data_objects.html#the-global-environment",
    "title": "Data Objects",
    "section": "",
    "text": "More abstract than visual spreadsheet representation\nCreate variables in code script files\nBring data from external files or web\nUse class() function to check variable types"
  },
  {
    "objectID": "slides/topics/04_data_objects.html#vector-fundamentals",
    "href": "slides/topics/04_data_objects.html#vector-fundamentals",
    "title": "Data Objects",
    "section": "",
    "text": "Most basic data structure in R\nCollection of elements of same type\nCreated using c() function\nOperations applied element-wise automatically"
  },
  {
    "objectID": "slides/topics/04_data_objects.html#creating-different-vector-types",
    "href": "slides/topics/04_data_objects.html#creating-different-vector-types",
    "title": "Data Objects",
    "section": "",
    "text": "Numeric vectors: c(1, 2, 3, 4)\nCharacter vectors: c(“apple”, “banana”, “cherry”)\nLogical vectors: c(TRUE, FALSE, TRUE)\nSequences: 1:10 or seq() function"
  },
  {
    "objectID": "slides/topics/04_data_objects.html#variable-naming-rules",
    "href": "slides/topics/04_data_objects.html#variable-naming-rules",
    "title": "Data Objects",
    "section": "",
    "text": "Must begin with letter, can contain numbers\nCannot contain spaces or forbidden characters\nShould be human-readable and consistent\nCase sensitive throughout R"
  },
  {
    "objectID": "slides/topics/04_data_objects.html#working-with-factors",
    "href": "slides/topics/04_data_objects.html#working-with-factors",
    "title": "Data Objects",
    "section": "",
    "text": "Used for categorical data with limited categories\nNon-ordered factors: simple category names\nOrdered factors: specific sequence matters\nMust specify order explicitly when needed"
  },
  {
    "objectID": "slides/topics/04_data_objects.html#data-conversion",
    "href": "slides/topics/04_data_objects.html#data-conversion",
    "title": "Data Objects",
    "section": "",
    "text": "Use class() to check current data type\nConvert with as.numeric(), as.character(), as.factor()\nR usually guesses correctly but check when unsure\nSometimes manual conversion necessary"
  },
  {
    "objectID": "slides/topics/04_data_objects.html#vector-and-matrix-operations",
    "href": "slides/topics/04_data_objects.html#vector-and-matrix-operations",
    "title": "Data Objects",
    "section": "",
    "text": "Vectors: single dimension storage (1 to i)\nMatrices: two dimensions (i rows, j columns)\nArrays: three or more dimensions\nAll must store same data type"
  },
  {
    "objectID": "slides/topics/04_data_objects.html#matrix-creation-and-manipulation",
    "href": "slides/topics/04_data_objects.html#matrix-creation-and-manipulation",
    "title": "Data Objects",
    "section": "",
    "text": "Use matrix() function with data, nrow, ncol\nCan name rows and columns with rownames(), colnames()\nAccess elements using bracket notation [i,j]\nUseful for mathematical operations and data organization"
  },
  {
    "objectID": "slides/topics/11_t_test.html",
    "href": "slides/topics/11_t_test.html",
    "title": "T-test",
    "section": "",
    "text": "Articulate the question of the t-test\nEvaluate data and assumptions for t-test\nGraph t-test data appropriately\nPerform tests and alternatives for t-test\n\n\n\n\n\nFoundation of modern statistical science\nInvented by William Sealy Gosset at Guinness brewery\nPublished anonymously as “Student” for commercial protection\nRefined and supported by R.A. Fisher\n\n\n\n\n\nCompare populations using independent samples\nThree common versions: 2 independent samples, 1 sample vs known mean, paired samples\nBasic question: are means different between groups?\nUsed to test if samples came from different populations\n\n\n\n\n\nNumeric variable measured in two samples\nQuestion: are sample means different?\nData in “long format” (one numeric, one factor) or “wide format” (two numeric vectors)\nTraditional visualization: boxplot with optional raw data points\n\n\n\n\n\nOne sample compared to known population mean\nQuestion: did sample come from population with known mean?\nData: single numeric vector and population mean value\nVisualization: boxplot with reference line for known mean\n\n\n\n\n\nIndividual observations in samples are not independent\nExamples: before/after treatment, spatially paired plots\nEach pair represents relationship between measurements\nVisualization shows tendency for change between paired observations\n\n\n\n\n\nGaussian distribution within each sample (not combined)\nHomoscedasticity (equal variance between samples)\nIndependence of observations\nT-test somewhat robust to assumption violations\n\n\n\n\n\nTest Gaussian distribution separately for each group\nUse histograms, q-q plots, and Shapiro-Wilk test\nCheck variance equality with Bartlett test or visual comparison\nIndependence assumption critical and related to study design\n\n\n\n\n\nHistogram and q-q plot for each sample separately\nCompare to expected Gaussian distribution\nLook for systematic deviations from normality\nConsider sample size in interpretation\n\n\n\n\n\nTwo-sample t-test for independent groups\nOne-sample t-test against known mean\nPaired t-test for dependent observations\nEach requires specific data structure and interpretation\n\n\n\n\n\nNon-parametric alternative when assumptions not met\nUses wilcox.test() function in R\nGood choice for small samples or skewed distributions\nLess statistical power but no distributional assumptions\n\n\n\n\n\nReport test statistic, degrees of freedom, p-value\nExample: “significant difference (2-sample t-test: t = -8.63, df = 11.9, P &lt; 0.0001)”\nAlways include all three key quantities\nFormat p-values appropriately (P &lt; 0.0001 for small values)"
  },
  {
    "objectID": "slides/topics/11_t_test.html#learning-objectives",
    "href": "slides/topics/11_t_test.html#learning-objectives",
    "title": "T-test",
    "section": "",
    "text": "Articulate the question of the t-test\nEvaluate data and assumptions for t-test\nGraph t-test data appropriately\nPerform tests and alternatives for t-test"
  },
  {
    "objectID": "slides/topics/11_t_test.html#students-t-test-origins",
    "href": "slides/topics/11_t_test.html#students-t-test-origins",
    "title": "T-test",
    "section": "",
    "text": "Foundation of modern statistical science\nInvented by William Sealy Gosset at Guinness brewery\nPublished anonymously as “Student” for commercial protection\nRefined and supported by R.A. Fisher"
  },
  {
    "objectID": "slides/topics/11_t_test.html#the-question-of-the-t-test",
    "href": "slides/topics/11_t_test.html#the-question-of-the-t-test",
    "title": "T-test",
    "section": "",
    "text": "Compare populations using independent samples\nThree common versions: 2 independent samples, 1 sample vs known mean, paired samples\nBasic question: are means different between groups?\nUsed to test if samples came from different populations"
  },
  {
    "objectID": "slides/topics/11_t_test.html#two-independent-samples",
    "href": "slides/topics/11_t_test.html#two-independent-samples",
    "title": "T-test",
    "section": "",
    "text": "Numeric variable measured in two samples\nQuestion: are sample means different?\nData in “long format” (one numeric, one factor) or “wide format” (two numeric vectors)\nTraditional visualization: boxplot with optional raw data points"
  },
  {
    "objectID": "slides/topics/11_t_test.html#one-sample-vs-known-mean",
    "href": "slides/topics/11_t_test.html#one-sample-vs-known-mean",
    "title": "T-test",
    "section": "",
    "text": "One sample compared to known population mean\nQuestion: did sample come from population with known mean?\nData: single numeric vector and population mean value\nVisualization: boxplot with reference line for known mean"
  },
  {
    "objectID": "slides/topics/11_t_test.html#paired-samples",
    "href": "slides/topics/11_t_test.html#paired-samples",
    "title": "T-test",
    "section": "",
    "text": "Individual observations in samples are not independent\nExamples: before/after treatment, spatially paired plots\nEach pair represents relationship between measurements\nVisualization shows tendency for change between paired observations"
  },
  {
    "objectID": "slides/topics/11_t_test.html#t-test-assumptions",
    "href": "slides/topics/11_t_test.html#t-test-assumptions",
    "title": "T-test",
    "section": "",
    "text": "Gaussian distribution within each sample (not combined)\nHomoscedasticity (equal variance between samples)\nIndependence of observations\nT-test somewhat robust to assumption violations"
  },
  {
    "objectID": "slides/topics/11_t_test.html#assumption-testing",
    "href": "slides/topics/11_t_test.html#assumption-testing",
    "title": "T-test",
    "section": "",
    "text": "Test Gaussian distribution separately for each group\nUse histograms, q-q plots, and Shapiro-Wilk test\nCheck variance equality with Bartlett test or visual comparison\nIndependence assumption critical and related to study design"
  },
  {
    "objectID": "slides/topics/11_t_test.html#graphical-assessment",
    "href": "slides/topics/11_t_test.html#graphical-assessment",
    "title": "T-test",
    "section": "",
    "text": "Histogram and q-q plot for each sample separately\nCompare to expected Gaussian distribution\nLook for systematic deviations from normality\nConsider sample size in interpretation"
  },
  {
    "objectID": "slides/topics/11_t_test.html#t-test-variations",
    "href": "slides/topics/11_t_test.html#t-test-variations",
    "title": "T-test",
    "section": "",
    "text": "Two-sample t-test for independent groups\nOne-sample t-test against known mean\nPaired t-test for dependent observations\nEach requires specific data structure and interpretation"
  },
  {
    "objectID": "slides/topics/11_t_test.html#mann-whitney-u-test-alternative",
    "href": "slides/topics/11_t_test.html#mann-whitney-u-test-alternative",
    "title": "T-test",
    "section": "",
    "text": "Non-parametric alternative when assumptions not met\nUses wilcox.test() function in R\nGood choice for small samples or skewed distributions\nLess statistical power but no distributional assumptions"
  },
  {
    "objectID": "slides/topics/11_t_test.html#results-reporting",
    "href": "slides/topics/11_t_test.html#results-reporting",
    "title": "T-test",
    "section": "",
    "text": "Report test statistic, degrees of freedom, p-value\nExample: “significant difference (2-sample t-test: t = -8.63, df = 11.9, P &lt; 0.0001)”\nAlways include all three key quantities\nFormat p-values appropriately (P &lt; 0.0001 for small values)"
  },
  {
    "objectID": "slides/topics/07_explore_data.html",
    "href": "slides/topics/07_explore_data.html",
    "title": "Explore Data",
    "section": "",
    "text": "Formulate appropriate statistical questions\nPerform hypothesis testing using NHST framework\nSummarize what “Weighing the Pig” means\nDistinguish between Analysis versus EDA\n\n\n\n\n\nDataset often comes in imperfect state\nFirst task is to “weigh the pig” - understand data\nOrder: Question, Explore, Analyze\nChoose analysis prior to collecting first data point\n\n\n\n\n\nTerm for creating summary-at-a-glance of dataset\nIncludes graphics, statistical summary, and data amount\nKey consideration: specification of variables\nBest way to gain skill: practice with data\n\n\n\n\n\nExamine variables: are they as expected?\nGraph data and examine numerical summaries\nLook for errors both trivial and serious\nMay take large proportion of analysis time\n\n\n\n\n\nNull Hypothesis Significance Testing framework\nPopulation of interest cannot be directly measured\nExperimental samples drawn randomly from population\nTest statistics compared to expected under null hypothesis\n\n\n\n\n\nNull hypothesis: consistent with no effect\nP-value: probability observed effect due to chance\nAlpha value: maximum acceptable probability of error (0.05)\nAlternative hypothesis: effect we predict to be true\n\n\n\n\n\nGood graph tells whole story\nBad graph worse than no graph at all\nDistinction between EDA graphs and evidence graphs\nMust convey relevant information and be self-contained\n\n\n\n\n\nReflect hypothesis or statistical concept\nAppropriate to the data being analyzed\nConsistent in aesthetics throughout\nBuild up information in layers\n\n\n\n\n\nAnalysis: generates evidence, fits specific question\nEDA: informal exploration and assumptions testing\nAnalysis designed for others, EDA usually private\nBoth essential parts of complete data analysis process\n\n\n\n\n\nFormal document connecting hypothesis to analysis method\nCreated before any data collection begins\nSpecifies hypotheses, statistical models, and data collection\nEffect size and sample size justification included\n\n\n\n\n\nTraditional cycle inadequate for best practice\nAnalysis planning must occur with hypothesis formulation\nExperimental design phase with power analysis\nStatistical analysis plan produced prior to data collection"
  },
  {
    "objectID": "slides/topics/07_explore_data.html#learning-objectives",
    "href": "slides/topics/07_explore_data.html#learning-objectives",
    "title": "Explore Data",
    "section": "",
    "text": "Formulate appropriate statistical questions\nPerform hypothesis testing using NHST framework\nSummarize what “Weighing the Pig” means\nDistinguish between Analysis versus EDA"
  },
  {
    "objectID": "slides/topics/07_explore_data.html#question-explore-analyze-workflow",
    "href": "slides/topics/07_explore_data.html#question-explore-analyze-workflow",
    "title": "Explore Data",
    "section": "",
    "text": "Dataset often comes in imperfect state\nFirst task is to “weigh the pig” - understand data\nOrder: Question, Explore, Analyze\nChoose analysis prior to collecting first data point"
  },
  {
    "objectID": "slides/topics/07_explore_data.html#weighing-the-pig-concept",
    "href": "slides/topics/07_explore_data.html#weighing-the-pig-concept",
    "title": "Explore Data",
    "section": "",
    "text": "Term for creating summary-at-a-glance of dataset\nIncludes graphics, statistical summary, and data amount\nKey consideration: specification of variables\nBest way to gain skill: practice with data"
  },
  {
    "objectID": "slides/topics/07_explore_data.html#exploratory-data-analysis-eda",
    "href": "slides/topics/07_explore_data.html#exploratory-data-analysis-eda",
    "title": "Explore Data",
    "section": "",
    "text": "Examine variables: are they as expected?\nGraph data and examine numerical summaries\nLook for errors both trivial and serious\nMay take large proportion of analysis time"
  },
  {
    "objectID": "slides/topics/07_explore_data.html#question-formulation-and-nhst",
    "href": "slides/topics/07_explore_data.html#question-formulation-and-nhst",
    "title": "Explore Data",
    "section": "",
    "text": "Null Hypothesis Significance Testing framework\nPopulation of interest cannot be directly measured\nExperimental samples drawn randomly from population\nTest statistics compared to expected under null hypothesis"
  },
  {
    "objectID": "slides/topics/07_explore_data.html#nhst-core-components",
    "href": "slides/topics/07_explore_data.html#nhst-core-components",
    "title": "Explore Data",
    "section": "",
    "text": "Null hypothesis: consistent with no effect\nP-value: probability observed effect due to chance\nAlpha value: maximum acceptable probability of error (0.05)\nAlternative hypothesis: effect we predict to be true"
  },
  {
    "objectID": "slides/topics/07_explore_data.html#variables-and-graphing-principles",
    "href": "slides/topics/07_explore_data.html#variables-and-graphing-principles",
    "title": "Explore Data",
    "section": "",
    "text": "Good graph tells whole story\nBad graph worse than no graph at all\nDistinction between EDA graphs and evidence graphs\nMust convey relevant information and be self-contained"
  },
  {
    "objectID": "slides/topics/07_explore_data.html#scientific-graph-requirements",
    "href": "slides/topics/07_explore_data.html#scientific-graph-requirements",
    "title": "Explore Data",
    "section": "",
    "text": "Reflect hypothesis or statistical concept\nAppropriate to the data being analyzed\nConsistent in aesthetics throughout\nBuild up information in layers"
  },
  {
    "objectID": "slides/topics/07_explore_data.html#analysis-versus-eda-distinction",
    "href": "slides/topics/07_explore_data.html#analysis-versus-eda-distinction",
    "title": "Explore Data",
    "section": "",
    "text": "Analysis: generates evidence, fits specific question\nEDA: informal exploration and assumptions testing\nAnalysis designed for others, EDA usually private\nBoth essential parts of complete data analysis process"
  },
  {
    "objectID": "slides/topics/07_explore_data.html#statistical-analysis-plan-concept",
    "href": "slides/topics/07_explore_data.html#statistical-analysis-plan-concept",
    "title": "Explore Data",
    "section": "",
    "text": "Formal document connecting hypothesis to analysis method\nCreated before any data collection begins\nSpecifies hypotheses, statistical models, and data collection\nEffect size and sample size justification included"
  },
  {
    "objectID": "slides/topics/07_explore_data.html#modern-scientific-method",
    "href": "slides/topics/07_explore_data.html#modern-scientific-method",
    "title": "Explore Data",
    "section": "",
    "text": "Traditional cycle inadequate for best practice\nAnalysis planning must occur with hypothesis formulation\nExperimental design phase with power analysis\nStatistical analysis plan produced prior to data collection"
  },
  {
    "objectID": "slides/topics/16_collaboration.html",
    "href": "slides/topics/16_collaboration.html",
    "title": "Collaboration",
    "section": "",
    "text": "Organize data science projects for effective collaboration\nImplement best practices for code sharing and documentation\nUse Git and GitHub for team-based workflows\nMaintain reproducibility in collaborative environments\n\n\n\n\n\nData science increasingly a team effort\nRequires more than technical skills - thoughtful organization\nIncreases productivity through division of labor\nImproves quality through peer review and diverse perspectives\n\n\n\n\n\nEnhanced creativity through diverse perspectives\nContinuity when team members change\nSystematic approach maximizing productivity\nMaintained reproducibility and quality standards\n\n\n\n\n\nWell-organized structure helps team navigation\nSeparate folders for data, code, results, documentation\nREADME.md with overview and setup instructions\nCONTRIBUTING.md with guidelines for contributors\n\n\n\n\n\nProject overview with setup instructions and usage examples\nCode comments explaining why, not just what\nFunction documentation with purpose, parameters, examples\nData dictionary describing variables and sources\n\n\n\n\n\nFollow consistent style guides (e.g., tidyverse style guide)\nUse consistent naming conventions throughout\nFormat code for readability and understanding\nConsider linters and formatters for consistency\n\n\n\n\n\nBreak long scripts into functions\nWrite code others can understand and reuse\nClear function definitions with inputs and outputs\nMain workflow calling individual functions\n\n\n\n\n\nEnsure consistent package versions across team\nUse renv for project-specific package management\nDocument package dependencies clearly\nInitialize and snapshot package environments\n\n\n\n\n\nCentralized workflow: simple approach for small teams\nFeature branch workflow: better for larger teams\nForking workflow: common for open-source projects\nChoose appropriate workflow for team size and complexity\n\n\n\n\n\nGuidelines for reviewers: respectful, constructive, focus on code\nGuidelines for authors: explain changes, respond positively\nKeep pull requests focused and manageable\nAcknowledge good practices and thank reviewers\n\n\n\n\n\nEnvironment management with renv or conda\nDocument system requirements and dependencies\nEstablish protocols for data access and sharing\nImplement continuous integration for automated testing"
  },
  {
    "objectID": "slides/topics/16_collaboration.html#learning-objectives",
    "href": "slides/topics/16_collaboration.html#learning-objectives",
    "title": "Collaboration",
    "section": "",
    "text": "Organize data science projects for effective collaboration\nImplement best practices for code sharing and documentation\nUse Git and GitHub for team-based workflows\nMaintain reproducibility in collaborative environments"
  },
  {
    "objectID": "slides/topics/16_collaboration.html#why-collaborative-workflows-matter",
    "href": "slides/topics/16_collaboration.html#why-collaborative-workflows-matter",
    "title": "Collaboration",
    "section": "",
    "text": "Data science increasingly a team effort\nRequires more than technical skills - thoughtful organization\nIncreases productivity through division of labor\nImproves quality through peer review and diverse perspectives"
  },
  {
    "objectID": "slides/topics/16_collaboration.html#key-collaboration-benefits",
    "href": "slides/topics/16_collaboration.html#key-collaboration-benefits",
    "title": "Collaboration",
    "section": "",
    "text": "Enhanced creativity through diverse perspectives\nContinuity when team members change\nSystematic approach maximizing productivity\nMaintained reproducibility and quality standards"
  },
  {
    "objectID": "slides/topics/16_collaboration.html#project-organization-for-teams",
    "href": "slides/topics/16_collaboration.html#project-organization-for-teams",
    "title": "Collaboration",
    "section": "",
    "text": "Well-organized structure helps team navigation\nSeparate folders for data, code, results, documentation\nREADME.md with overview and setup instructions\nCONTRIBUTING.md with guidelines for contributors"
  },
  {
    "objectID": "slides/topics/16_collaboration.html#essential-documentation-components",
    "href": "slides/topics/16_collaboration.html#essential-documentation-components",
    "title": "Collaboration",
    "section": "",
    "text": "Project overview with setup instructions and usage examples\nCode comments explaining why, not just what\nFunction documentation with purpose, parameters, examples\nData dictionary describing variables and sources"
  },
  {
    "objectID": "slides/topics/16_collaboration.html#code-sharing-best-practices",
    "href": "slides/topics/16_collaboration.html#code-sharing-best-practices",
    "title": "Collaboration",
    "section": "",
    "text": "Follow consistent style guides (e.g., tidyverse style guide)\nUse consistent naming conventions throughout\nFormat code for readability and understanding\nConsider linters and formatters for consistency"
  },
  {
    "objectID": "slides/topics/16_collaboration.html#modular-code-development",
    "href": "slides/topics/16_collaboration.html#modular-code-development",
    "title": "Collaboration",
    "section": "",
    "text": "Break long scripts into functions\nWrite code others can understand and reuse\nClear function definitions with inputs and outputs\nMain workflow calling individual functions"
  },
  {
    "objectID": "slides/topics/16_collaboration.html#package-management",
    "href": "slides/topics/16_collaboration.html#package-management",
    "title": "Collaboration",
    "section": "",
    "text": "Ensure consistent package versions across team\nUse renv for project-specific package management\nDocument package dependencies clearly\nInitialize and snapshot package environments"
  },
  {
    "objectID": "slides/topics/16_collaboration.html#git-workflows-for-teams",
    "href": "slides/topics/16_collaboration.html#git-workflows-for-teams",
    "title": "Collaboration",
    "section": "",
    "text": "Centralized workflow: simple approach for small teams\nFeature branch workflow: better for larger teams\nForking workflow: common for open-source projects\nChoose appropriate workflow for team size and complexity"
  },
  {
    "objectID": "slides/topics/16_collaboration.html#code-review-process",
    "href": "slides/topics/16_collaboration.html#code-review-process",
    "title": "Collaboration",
    "section": "",
    "text": "Guidelines for reviewers: respectful, constructive, focus on code\nGuidelines for authors: explain changes, respond positively\nKeep pull requests focused and manageable\nAcknowledge good practices and thank reviewers"
  },
  {
    "objectID": "slides/topics/16_collaboration.html#maintaining-reproducibility",
    "href": "slides/topics/16_collaboration.html#maintaining-reproducibility",
    "title": "Collaboration",
    "section": "",
    "text": "Environment management with renv or conda\nDocument system requirements and dependencies\nEstablish protocols for data access and sharing\nImplement continuous integration for automated testing"
  },
  {
    "objectID": "slides/topics/00_about_bootcamp.html",
    "href": "slides/topics/00_about_bootcamp.html",
    "title": "About the Bootcamp",
    "section": "",
    "text": "Comprehensive, self-paced course for researchers and students\nDevelops practical data science skills in R programming\nFocuses on statistical analysis and reproducible research practices\nMission to provide accessible, high-quality training\n\n\n\n\n\nAccessible to researchers at all levels\nPractical with focus on real-world applications\nReproducible to advance scientific integrity\nCollaborative to encourage knowledge sharing\n\n\n\n\n\nResearch students beginning their data analysis journey\nAcademic staff looking to update quantitative skills\nProfessionals transitioning to data-driven roles\nSelf-learners interested in R programming and statistics\n\n\n\n\n\nMaster fundamentals of R programming and RStudio environment\nLearn to manipulate data and create functions\nWork with different data structures\nBuild essential programming skills\n\n\n\n\n\nApply R skills to statistical analysis and data visualization\nExplore hypothesis testing, correlation, and regression\nLearn techniques for data exploration and analysis\nMaster essential statistical concepts\n\n\n\n\n\nDiscover tools and practices for reproducible science\nLearn R Markdown for dynamic documents\nMaster Git for version control\nDevelop collaborative workflow skills\n\n\n\n\n\nSelf-paced lessons that fit your schedule\nHands-on exercises to reinforce concepts\nReal-world examples relevant to research contexts\nProgressive difficulty building on previous knowledge\n\n\n\n\n\nBased on current best practices in data science education\nIncorporates feedback from learners and instructors\nReflects advances in R packages and statistical methods\nGrounded in research on effective teaching and learning\n\n\n\n\n\nDiscord community support coming soon\nAdditional resources linked throughout the course\nSelf-assessment tools and practice exercises\nLearning-by-doing methodology with practical focus"
  },
  {
    "objectID": "slides/topics/00_about_bootcamp.html#what-is-the-r-stats-bootcamp",
    "href": "slides/topics/00_about_bootcamp.html#what-is-the-r-stats-bootcamp",
    "title": "About the Bootcamp",
    "section": "",
    "text": "Comprehensive, self-paced course for researchers and students\nDevelops practical data science skills in R programming\nFocuses on statistical analysis and reproducible research practices\nMission to provide accessible, high-quality training"
  },
  {
    "objectID": "slides/topics/00_about_bootcamp.html#our-philosophy",
    "href": "slides/topics/00_about_bootcamp.html#our-philosophy",
    "title": "About the Bootcamp",
    "section": "",
    "text": "Accessible to researchers at all levels\nPractical with focus on real-world applications\nReproducible to advance scientific integrity\nCollaborative to encourage knowledge sharing"
  },
  {
    "objectID": "slides/topics/00_about_bootcamp.html#target-audience",
    "href": "slides/topics/00_about_bootcamp.html#target-audience",
    "title": "About the Bootcamp",
    "section": "",
    "text": "Research students beginning their data analysis journey\nAcademic staff looking to update quantitative skills\nProfessionals transitioning to data-driven roles\nSelf-learners interested in R programming and statistics"
  },
  {
    "objectID": "slides/topics/00_about_bootcamp.html#module-1-r-foundations",
    "href": "slides/topics/00_about_bootcamp.html#module-1-r-foundations",
    "title": "About the Bootcamp",
    "section": "",
    "text": "Master fundamentals of R programming and RStudio environment\nLearn to manipulate data and create functions\nWork with different data structures\nBuild essential programming skills"
  },
  {
    "objectID": "slides/topics/00_about_bootcamp.html#module-2-statistical-analysis",
    "href": "slides/topics/00_about_bootcamp.html#module-2-statistical-analysis",
    "title": "About the Bootcamp",
    "section": "",
    "text": "Apply R skills to statistical analysis and data visualization\nExplore hypothesis testing, correlation, and regression\nLearn techniques for data exploration and analysis\nMaster essential statistical concepts"
  },
  {
    "objectID": "slides/topics/00_about_bootcamp.html#module-3-reproducible-research",
    "href": "slides/topics/00_about_bootcamp.html#module-3-reproducible-research",
    "title": "About the Bootcamp",
    "section": "",
    "text": "Discover tools and practices for reproducible science\nLearn R Markdown for dynamic documents\nMaster Git for version control\nDevelop collaborative workflow skills"
  },
  {
    "objectID": "slides/topics/00_about_bootcamp.html#learning-approach",
    "href": "slides/topics/00_about_bootcamp.html#learning-approach",
    "title": "About the Bootcamp",
    "section": "",
    "text": "Self-paced lessons that fit your schedule\nHands-on exercises to reinforce concepts\nReal-world examples relevant to research contexts\nProgressive difficulty building on previous knowledge"
  },
  {
    "objectID": "slides/topics/00_about_bootcamp.html#course-development",
    "href": "slides/topics/00_about_bootcamp.html#course-development",
    "title": "About the Bootcamp",
    "section": "",
    "text": "Based on current best practices in data science education\nIncorporates feedback from learners and instructors\nReflects advances in R packages and statistical methods\nGrounded in research on effective teaching and learning"
  },
  {
    "objectID": "slides/topics/00_about_bootcamp.html#getting-support",
    "href": "slides/topics/00_about_bootcamp.html#getting-support",
    "title": "About the Bootcamp",
    "section": "",
    "text": "Discord community support coming soon\nAdditional resources linked throughout the course\nSelf-assessment tools and practice exercises\nLearning-by-doing methodology with practical focus"
  },
  {
    "objectID": "07-question-explore.html",
    "href": "07-question-explore.html",
    "title": "07 Explore data",
    "section": "",
    "text": "We call sizing up the data “weighing the pig”",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "07 Explore data"
    ]
  },
  {
    "objectID": "07-question-explore.html#question-explore-analyze-a-workflow-for-data-science",
    "href": "07-question-explore.html#question-explore-analyze-a-workflow-for-data-science",
    "title": "07 Explore data",
    "section": "1 Question, explore, analyze (a workflow for data science)",
    "text": "1 Question, explore, analyze (a workflow for data science)\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this lesson, you will be able to:\n\nFormulate statistical questions\nPerform hypothesis testing\nSummarize what “Weighing the Pig” means\nDiscuss the use of variables in graphing\nDistinguish between “Analysis” versus “EDA”\nExplain the concept of a Statistical Analysis Plan\n\n\n\n\n\nA dataset often comes to the Data Scientist in an imperfect state, possibly incomplete, containing errors, and with minimal description. Likewise, it may contain wonderful knowledge, there to discover. Either way, your first task is to weigh the pig.\n\n\nThe very first task for any data analysis is to gain an understanding of the data itself. This typically involves examining the variables. Are they as we expect? Do we need to adjust the variable types?\n\nEDA Exploratory Data Analysis\nThis almost always involves graphing the data, and possibly examining numerical summaries and statistical assumptions. Further, it is necessary to look for errors in the data both trivial (e.g. misspelling factor level names like “control” with an extra space “control”), and more serious errors such as numerical typographical errors (e.g. misplacing a decimal point is a classic: height of 5 men in feet: c(5.5, 5.8, 6.1, 5.9, 52.).\nIn total, this part of data analysis is sometimes referred to as Exploratory Data Analysis.\nEDA is part practical and part philosophical in that is requires skill and experience, but is also subjective. Think of it as a step that might take a long while, where the data scientists decides what the analysis is that will be applied to the data, that the analysis is correct and appropriate. Ironically, while EDA is considered very important and can take a large proportion of the total time spent analyzing data, it is usually only reported on very briefly if at all.\nThe order of operation for most analyses should be\n\n1 question\n2 explore\n3 analyse\n\n\n\nYou choose your data analysis prior to collecting the first data point.\n\nFocus on the question and make sure it is clear in formulation, and choose an analysis approach that can resolve the question , given the data… But the data collection should be DESIGNED to fit the question and chosen analysis prior to collection. Explore the data to examine any assumptions required for the analysis, including the use of graphing and any diagnostic or summary statistics. Finally, perform and summarize the analysis. We will practice this workflow for different basic questions, with an emphasis on simple quantitative data.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "07 Explore data"
    ]
  },
  {
    "objectID": "07-question-explore.html#question-formulation-and-hypothesis-testing",
    "href": "07-question-explore.html#question-formulation-and-hypothesis-testing",
    "title": "07 Explore data",
    "section": "2 Question formulation and hypothesis testing",
    "text": "2 Question formulation and hypothesis testing\n\nIt is the primary responsibility of the scientist to agree on the specific details of generating evidence from data to answer questions (i.e., statistical analysis). When these roles are occupied by the same person, this matter should be settled before collecting any data.\n\nThe general topic of formulating statistical questions is vast; many books have been written on the subject. The tradition and practice of statistical analysis has evolved through time. Here we will focus on the traditional starting point for a “first statistics course”, within the context of Null Hypothesis Significance testing (NHST).\n\n2.1 Sampling concept and NHST\nThe gambit of NHST is that there is a population of interest but that the population cannot be directly measured because it is too big or otherwise inconvenient or impossible to measure. Thus, experimental samples are drawn randomly from the population, possibly subjected to experimental conditions, and the magnitude of observed differences or measured associations are summarized by various test statistics and compared to how likely such an observed difference or association would be to observe in the absence of the hypothesized effect.\nThe null hypothesis is the one consistent with no effect or difference. We evaluate whether to reject the null hypothesis using the P-value, the (conditional) probability that the observed effect is unlikely to arise duie to sampling or experimental error.\nTraditionally, the P-value is compared to the alpha value, almost always set to 0.05. This alpha value can be interpreted as the maximum probability that is acceptable of making a mistake and concluding there IS a difference, when in fact a difference does not exist. When the P-value is less than 0.05, we conclude there is a difference, rejecting the null hypothesis and “accepting” the hypothesis we predicted was true, usually referred to as the alternative hypothesis.\n\n2.2 NHST notes\nBenefits of NHST\n\nFamiliar and acceptable to majority of researchers\nTypically robust to assumptions when applied correctly\nStrong framework for evidence, especially for experiments\nThe basic idea is objective and simple\n\n\nCriticism of HNST\n\nOften conceived, applied and interpreted under error\nValidation of analysis (e.g. assumptions testing) is often neglected\nEducation for applied researchers often deficient\nThough simple, practitioners may be ignorant of subtle concepts\n\n\n2.3 Further reading\nIf the idea is new to you that NHST in statistics is not perfect and you want to get serious about understanding why, like most subjects, you will need to pursue further sources.\nAnderson, D.R., Burnham, K.P. and Thompson, W.L., 2000. Null hypothesis testing: problems, prevalence, and an alternative. The journal of wildlife management, pp.912-923.\nNickerson, R.S., 2000. Null hypothesis significance testing: a review of an old and continuing controversy. Psychological methods, 5(2), p.241.\nNix, T.W. and Barnette, J.J., 1998. The data analysis dilemma: Ban or abandon. A review of null hypothesis significance testing. Research in the Schools, 5(2), pp.3-14.\nStephens, P.A., Buskirk, S.W., Hayward, G.D. and Martinez Del Rio, C., 2005. Information theory and hypothesis testing: a call for pluralism. Journal of applied ecology, 42(1), pp.4-12.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "07 Explore data"
    ]
  },
  {
    "objectID": "07-question-explore.html#summarize-weighing-the-pig",
    "href": "07-question-explore.html#summarize-weighing-the-pig",
    "title": "07 Explore data",
    "section": "3 Summarize: Weighing the Pig",
    "text": "3 Summarize: Weighing the Pig\n\n\nThe best way gain skill in handling data is to practice.\n\nWeighing the pig is the term we use to describe creating a summary-at-a-glance of a dataset. Usually this includes graphics and statistical summary, as well a description of how much data we have. A key consideration is, also, the specification of the variables.\nWe will practice data handling with the data file chickwts.xlsx.\nDownload the file, read it into a data object in R called chicks, and convert the feed variable to a factor if necessary.\n# Try this:\n\n# Download the 7-chickwts.xlsx file, read it into a data \n# object in R called \"chicks\", \n# and convert the \"feed\" variable to a factor if necessary.\n\n# Do not neglect looking inside the \"raw\" data file\n# Is it as you expect?  Is the data dictionary present and clear?\n\n# Load necessary libraries\nlibrary(openxlsx)\n\n# Read file\nsetwd(\"D:/Dropbox/git/DSgarage/public/data\") # NB change to YOUR file path...\nchicks &lt;- read.xlsx(\"7-chickwts.xlsx\")\n\n# Convert feed to factor if needed\nclass(chicks$feed) # Character\nchicks$feed &lt;- factor(chicks$feed)\nclass(chicks$feed) # Factor\n\n3.1 Chick data\n\nThe hypothesis voices “how you think the world works” or what you predict to be true”\n\nThe hypothesis we believe is true for the chicks dataset might be phrased in different ways.\n\nChick weight differs after 6 weeks according to feed additive type\nMean chick weight varies according to feed additive type\nThe variance between chick weight for different feed additives is bigger than the variance within chick weight as a whole\n\n\n3.2 Hypothesis\nThe minimum amount of information we are usually interested in when sizing up a dataset is How much data is there?, What is the central tendency (e.g. the mean, variance, etc.)?, and possibly Are there rare values?.\nWe would typically start graphing the data right away. If we have a notion of what our questions or hypotheses are, they should inform the initial peek at the data. For example, in the chickwts data, we know our question will be related not to the overall central tendency of chick weight, but to chick weight for each individual feed type.\nWe do not approach this sizing up of the data in a workhorse fashion, merely to check a tick box. We are looking quickly for details in the data that give us insight into what the data is like. For example, we peek at whether the mean and median are close to each other (indicator our data may be Gaussian), we compare the standard deviation, variance or standard error of a numeric variable relative to different levels of a factor, to see if they are similar.\n# Try this:\n\n# Summarize the whole dataset\n# summary() provides summary statistics for numeric variables and counts\nsummary(chicks)\n\n# we might want to look at summary for different levels of feed\n?summary\nsummary(object = chicks$weight[which(chicks$feed == \"casein\")])\nsummary(object = chicks$weight[which(chicks$feed == \"horsebean\")])\n# etc. - this method is easy but inelegant?\n\n# aggregate()\n?aggregate\n\n# mean\naggregate(x = chicks$weight, by = list(chicks$feed), FUN = mean)\n\n# standard deviation\naggregate(x = chicks$weight, by = list(chicks$feed), FUN = sd)\n\n# You can make your own function for the FUN argument\n# stadard error of mean, SEM = standard deviation / square root of sample size\naggregate(x = chicks$weight, by = list(chicks$feed), \n          FUN = function(x){ sd(x)/sqrt(length(x)) })\n\n# You can apply several functions and name them!\naggregate(x = chicks$weight, by = list(feed = chicks$feed), \n          FUN = function(x){ c(mean = mean(x), \n                               sd = sd(x),  \n                               SEM = sd(x)/sqrt(length(x)))})",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "07 Explore data"
    ]
  },
  {
    "objectID": "07-question-explore.html#variables-and-graphing",
    "href": "07-question-explore.html#variables-and-graphing",
    "title": "07 Explore data",
    "section": "4 Variables and graphing",
    "text": "4 Variables and graphing\n\nA good graph usually tells the whole story, but a bad graph is worse than no graph at all.\n\n\n\n\n\nXKCD Convinced by data\n\n\n\nThere are a few topics in graphing data that are important to consider here, but the topic is wide and deep, analytical, creative, and even artistic. We make a distinction between graphs used to explore data during EDA (meant to be “consumed” only by the data scientist who made them and are of no use to document a pattern to others) and graphs intended to constitute evidence.\n\n4.1 Scientific graphs\nA few graphing principles:\n\nMust convey the relevant information\nShould be consistent in aesthetics\nMust be self-contained (meaning is contained 100% within the figure and legend)\nShould reflect a hypothesis or statistical concept (if not purely descriptive)\nShould be appropriate to the data\n\n\nYou can think of R graphics as a way to “build up information in layers” onto a graph. There are many aesthetic features of graph that can be controlled, like adding colors, text, lines, legends, etc. The R graphics system is very simple to use, but can also be very powerful (mastering this takes practice). We make a distinction here between R base graphics and packages that can be used to make specialized and varied graphs (like the powerful and popular package {ggplot})\n\n4.2 Layering information\nWe can look at graphing the chicks data in a few different ways. We will try a few different graphs in this way, building up features. We might build up features on a graph using arguments in a particular graph function.\n\nLike, adding\n\na main title with the argument main\nthe x axis title with the argument xlab\nadding lines with the functions abline() or lines()\n\n\n4.3 Types of graphs\nTypically you would choose the type of graph that both fits the type of data you have and that conveys the information you wish to examine or showcase. E.g., for a single numeric variable, you might wish to show:\n\nThe distribution of data with a histogram: hist()\nThe central tendency relative to a factor with a boxplot: boxplot()\n\n\nHistogram of the chicks data\n# The least you can do\nhelp(hist)\nhist(x = chicks$weight)\n\n\nAdd a title with main\n# Argument main\nhist(x = chicks$weight,\n     main = \"Distribution of chick weights (all feeds)\")\n\n\nAdd an x axis title with xlab\n# x axis title\nhist(x = chicks$weight,\n     main = \"Distribution of chick weights (all feeds)\",\n     xlab = \"Chick weight (grams)\")\n\n\nAdd a vertical line for the weight mean with abline()\n# Add vertical line for mean weight\nhist(x = chicks$weight,\n     main = \"Distribution of chick weights (all feeds)\",\n     xlab = \"Chick weight (grams)\")\n\nhelp(abline)\nabline(v = mean(chicks$weight), col = \"red\", lty = 2, lwd = 3)\n\n\n# Try a boxplot\n\nhelp(boxplot)\nboxplot(x = chicks$weight)\n\n# I have seen worse graphs, but I can't remember when.\n# Flash challenge: Improve the graph\n\n\n# weight as a function of feed\nboxplot(formula = weight ~ feed,\n        data = chicks)\n# This is probably a good representation of our hypothesis\n# Flash challenge: Improve the graph...",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "07 Explore data"
    ]
  },
  {
    "objectID": "07-question-explore.html#analysis-versus-eda",
    "href": "07-question-explore.html#analysis-versus-eda",
    "title": "07 Explore data",
    "section": "5 “Analysis” versus “EDA”",
    "text": "5 “Analysis” versus “EDA”\nAlthough you could consider Exploratory Data Analysis, EDA, an important part of the complete process of data analysis, we might make a distinction between “Analysis” the part of analysis that generates Evidence, and that of EDA which is used to explore data and test assumptions.\n\n5.1 Analysis\nA data analysis is\n\nDesigned to fit a specific question or hypothesis\nPart of a workflow: Informal hypothesis statement (in plain language) &gt; Statistical hypothesis (specifies a or implies a statistical test) &gt; Evidence (the specific results)\nDesigned and usually formatted to present to others, such as in a report or a scientific manuscript\nContains only bare essentials as relates to the initial hypothesis (e.g. a good graph, the summary of a statistical analysis)\nShould strictly be reproducible via a script and archived data\nDone in conjunction with EDA\n\n\n5.2 EDA\nExploratory data analysis is\n\nInformal and may be haphazard\nDesigned to explore or gain understanding of data\nAssumptions testing\nUsually not designed to document or show to others\nOccurs primarily before (every) analysis\nMay or may not be documented to be reproducible\nDone before the final, evidence-generating Analysis\n\n\nWe can keep this concept of EDA versus Analysis in our mind while we discuss the Statistical Analysis Plan.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "07 Explore data"
    ]
  },
  {
    "objectID": "07-question-explore.html#statistical-analysis-plan-the-concept",
    "href": "07-question-explore.html#statistical-analysis-plan-the-concept",
    "title": "07 Explore data",
    "section": "6 Statistical Analysis Plan: the concept",
    "text": "6 Statistical Analysis Plan: the concept\n\nI have a cunning (statistical analysis) plan -Baldrick\n\nA Statistical Analysis Plan (SAP) is a formal document that should be used to design data analysis. One of the most important functions of the SAP is to make a formal connection between the hypothesis, the data collected and and the method of analysis that will be used to generate evidence to support or refute the hypothesis. This is conducted before any data are collected.\nThe components of a basic SAP are:\n\nThe hypotheses stated in plain language\nEach hypothesis translated into a specific statistical model\nSpecification of data and and data collection methods\n\n-) Specification of effect size\n\n\nJustification of sample size through power analysis or other means\n\n\nDefinition of all of these components is beyond the boundaries of this Bootcamp, however the explicit connection of hypotheses with a statistical model is one of the very basic elements of best practice in science.\n\n6.1 The scientific method, Classic version\nWe usually learn the scientific method as a cycle where we conceive a problem, form a hypothesis, conduct an experiment, evaluate the result and so on. We learn and teach this as a literal cycle.\n\n\n\n\nThe classic view of the scientific process\n\n\n\nThis classic view of the scientific process implies that we plan the analysis only after we conduct the experiment and collect data. While many data scientists or statisticians would agree that this model is widely used in science, it is considered very poor practice for several reasons.\n\nThe expected difference or relationship (i.e., the effect size) should explicitly be part of the hypothesis and quantified BEFORE collecting data\nThe statistical test must be chosen prior to collect the data to insure the evidence matches the expectation\nThe sample size should be justified, using power analysis or a less formal means. Collecting too little data will likely result in failing to detect a difference (even if your hypothesis is correct!); Collecting too much data is simply a waste of resources.\n\n\n\n\n\nScientific Process - what we teach children in school is not quite right\n\n\n\n6.2 Best practice scientific method\nThe traditional view of the scientific method should probably be adjusted to explicitly accommodate planning the analysis at the same time as the hypothesis formulation stage. Likewise, the analysis plan should specifically influence the design of the data collection for the experiment.\n\n\n\n\nModern scientific process\n\n\n\nA modern view of best practice of scientific endeavor includes an experimental design phase, with consideration of effect size and power analysis, and the production of a statistical analysis plan that contains a formal statistical hypothesis. All off this happens prior to any data collection.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "07 Explore data"
    ]
  },
  {
    "objectID": "07-question-explore.html#practice-exercises",
    "href": "07-question-explore.html#practice-exercises",
    "title": "07 Explore data",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\nFor the following questions, use the field-trial.xlsx dataset.\nThis is real data in Tidy Data format, but our information for these exercises is limited precisely to the contents of the file, including the data dictionary. In this experiment, seeds were raised under field trial conditions for two weeks to look at the effect of different treatment conditions on mass of gain during germination. There are several measured variables, with the calculated pct variable probably intended to be the dependent variable, with the factor treatment being the main explanatory variable for variation in pct.\n\n7.1 Sampling Distribution\nCreate a simulation of a sampling distribution. Use the rnorm() function to create a “population” of 10,000 values with a mean of 100 and a standard deviation of 15. Then take 100 samples of size 30 from this population and calculate the mean of each sample. Plot a histogram of the sample means and calculate the mean and standard deviation of the sample means. How does the standard deviation of the sample means compare to the standard error of the mean (standard deviation of the population divided by the square root of the sample size)?\n\n\n\n\n\n\n\nR Analysis File Setup\n\n\n\n\n\n\n## HEADER ####\n## Who: Your Name\n## What: Field trial data analysis\n## Last edited: 2023-05-15\n####\n\n## CONTENTS ####\n## 00 Setup\n## 01 Data Exploration\n## 02 Statistical Analysis\n## 03 Visualization\n####\n\n## 00 Setup ####\n# Set working directory (adjust path to your actual directory)\n# setwd(\"path/to/your/directory\")\n\n# Load required libraries\nlibrary(openxlsx)\n\n# Read in the field trial data\nseed &lt;- read.xlsx(\"data/field-trial.xlsx\")\n\n# Check the structure of the data\nstr(seed)\n\n# View the first few rows\nhead(seed)\n\nThis code sets up a properly structured R analysis file with: - A header containing author information and purpose - A table of contents outlining the major sections - A setup section that: - Sets the working directory - Loads the necessary library for reading Excel files - Reads in the data and assigns it to an object called “seed” - Includes basic commands to examine the data structure\nThis structure follows best practices for reproducible research by clearly documenting each step of the analysis process.\n\n\n\n\n7.2 Seed Data Analysis\nUsing the field-trial.xlsx data file, perform an appropriate statistical analysis to determine if there are differences in germination rates between treatments. Include appropriate data visualization and report your results in the technical style.\n\n\n\n\n\n\n\nData Type Conversion\n\n\n\n\n\n\n# Load required libraries and read data\nlibrary(openxlsx)\nseed &lt;- read.xlsx(\"data/field-trial.xlsx\")\n\n# Check current data types\nstr(seed)\n\n# Convert pct, wet, and dry to numeric\nseed$pct &lt;- as.numeric(seed$pct)\nseed$wet &lt;- as.numeric(seed$wet)\nseed$dry &lt;- as.numeric(seed$dry)\n\n# Convert block and trial to factors\nseed$block &lt;- factor(seed$block)\nseed$trial &lt;- factor(seed$trial)\n\n# Convert treatment to factor with \"Control\" as reference level\nseed$treatment &lt;- factor(seed$treatment, levels = c(\"Control\", \"Treatment1\", \"Treatment2\", \"Treatment3\"))\n\n# Verify the conversions\nstr(seed)\n\nThis code ensures that all variables have the appropriate data types: - Numeric variables (pct, wet, dry) are converted using as.numeric() - Categorical variables (block, trial) are converted to factors - The treatment variable is converted to a factor with “Control” explicitly set as the first level, making it the reference level for statistical analyses\nSetting “Control” as the reference level is particularly important for statistical tests that compare treatments against a control condition, as it affects how contrasts are calculated in models.\n\n\n\n\n7.3 Comprehensive Boxplot\nCreate a boxplot of the seed germination data by treatment, adding horizontal reference lines for the overall mean and ±1 standard deviation. Include a legend explaining these reference lines.\n\n\n\n\n\n\n\nAggregating Data by Treatment\n\n\n\n\n\n\n# Load required libraries and read data\nlibrary(openxlsx)\nseed &lt;- read.xlsx(\"data/field-trial.xlsx\")\n\n# Convert treatment to factor and pct to numeric\nseed$treatment &lt;- factor(seed$treatment)\nseed$pct &lt;- as.numeric(seed$pct)\n\n# Calculate mean, SD, SE, and count of pct for each treatment level\ntreatment_stats &lt;- aggregate(pct ~ treatment, data = seed, \n                            FUN = function(x) {\n                              c(mean = mean(x),\n                                sd = sd(x),\n                                se = sd(x)/sqrt(length(x)),\n                                count = length(x))\n                            })\n\n# Display the results\ntreatment_stats\n\nThis code calculates four key statistics for the pct variable grouped by treatment: - Mean: The average percentage value for each treatment - Standard Deviation (SD): Measures the amount of variation in the data - Standard Error (SE): Estimates the standard deviation of the sampling distribution of the mean - Count: The number of observations in each treatment group\nThe aggregate() function is used with a custom function that calculates all these statistics at once. The formula notation pct ~ treatment specifies that we want to aggregate the pct variable by the treatment factor.\nThis summary provides a comprehensive overview of how the treatments affect the percentage values, including both the central tendency (mean) and the variability (SD, SE).\n\n\n\n\n7.4 Multi-Panel Boxplot\n(hard: may require tinkering and problem solving)\nExperiment making a boxplot showing pct ~ treatment separated for each trial\n\n\n\n\n\n\n\nMulti-Panel Boxplot by Trial\n\n\n\n\n\n\n# Load required libraries and read data\nlibrary(openxlsx)\nseed &lt;- read.xlsx(\"data/field-trial.xlsx\")\n\n# Convert variables to appropriate types\nseed$treatment &lt;- factor(seed$treatment)\nseed$trial &lt;- factor(seed$trial)\nseed$pct &lt;- as.numeric(seed$pct)\n\n# Method 1: Using par() to create multiple panels\n# First, determine how many trials we have\ntrials &lt;- unique(seed$trial)\nn_trials &lt;- length(trials)\n\n# Set up the plotting area with one row and multiple columns\npar(mfrow = c(1, n_trials))\n\n# Create a boxplot for each trial\nfor (t in trials) {\n  # Subset data for this trial\n  trial_data &lt;- seed[seed$trial == t, ]\n  \n  # Create boxplot\n  boxplot(pct ~ treatment, data = trial_data,\n          main = paste(\"Trial\", t),\n          xlab = \"Treatment\",\n          ylab = \"Growth Percentage (%)\",\n          col = \"lightblue\")\n}\n\n# Reset the plotting parameters\npar(mfrow = c(1, 1))\n\n# Method 2: Using a single plot with interaction\n# Create a new factor that combines treatment and trial\nseed$treat_trial &lt;- interaction(seed$treatment, seed$trial)\n\n# Create the boxplot with all combinations\nboxplot(pct ~ treat_trial, data = seed,\n        main = \"Growth Percentage by Treatment and Trial\",\n        xlab = \"\",\n        ylab = \"Growth Percentage (%)\",\n        col = rainbow(n_trials)[as.numeric(seed$trial)],\n        las = 2)  # Rotate x-axis labels\n\n# Add a more informative x-axis\naxis(1, at = 1:length(levels(seed$treat_trial)), \n     labels = levels(seed$treat_trial), \n     las = 2, cex.axis = 0.7)\n\n# Add a legend\nlegend(\"topright\", \n       legend = paste(\"Trial\", trials),\n       fill = rainbow(n_trials),\n       title = \"Trial\")\n\nThis solution presents two different approaches to creating boxplots separated by trial:\n\nMultiple Panel Approach: Creates a separate boxplot for each trial using par(mfrow) to arrange them side by side. This makes it easy to compare treatments within each trial.\nSingle Plot Approach: Creates one boxplot with all treatment-trial combinations using the interaction() function to create a new factor that represents all possible combinations. This approach uses color coding to distinguish between trials.\n\nBoth methods allow you to see how treatment effects might vary across different trials, which is important for assessing the consistency and reproducibility of the results.\nThe choice between these approaches depends on the number of trials and treatments, and what aspects of the data you want to emphasize in your visualization.\n\n\n\n\n7.5 Iris Aggregation\nWrite a plausible practice question involving aggregate() and boxplot() in-built R dataset iris.\n\n\n\n\n\n\n\nIris Dataset Question\n\n\n\n\n\n\n# A plausible practice question could be:\n\n# \"Using the iris dataset, create a boxplot showing petal length by species. \n# Then use aggregate() to calculate the mean, median, and standard deviation \n# of petal length for each species. Add horizontal lines to your boxplot \n# representing the mean petal length for each species using different colors. \n# Include a legend identifying each species mean.\"\n\n# Solution:\n# Load the iris dataset\ndata(iris)\n\n# Calculate statistics by species\npetal_stats &lt;- aggregate(Petal.Length ~ Species, data = iris, \n                        FUN = function(x) {\n                          c(mean = mean(x),\n                            median = median(x),\n                            sd = sd(x))\n                        })\n\n# Display the statistics\npetal_stats\n\n     Species Petal.Length.mean Petal.Length.median Petal.Length.sd\n1     setosa         1.4620000           1.5000000       0.1736640\n2 versicolor         4.2600000           4.3500000       0.4699110\n3  virginica         5.5520000           5.5500000       0.5518947\n\n# Create a boxplot\nboxplot(Petal.Length ~ Species, data = iris,\n        main = \"Petal Length by Species\",\n        xlab = \"Species\",\n        ylab = \"Petal Length (cm)\",\n        col = c(\"lightpink\", \"lightblue\", \"lightgreen\"))\n\n# Define colors for mean lines\nspecies_colors &lt;- c(\"red\", \"blue\", \"darkgreen\")\n\n# Add horizontal lines for means of each species\nfor (i in 1:3) {\n  species_data &lt;- iris[iris$Species == levels(iris$Species)[i], ]\n  species_mean &lt;- mean(species_data$Petal.Length)\n  segments(i-0.4, species_mean, i+0.4, species_mean, \n           col = species_colors[i], lwd = 2)\n}\n\n# Add legend\nlegend(\"topleft\", \n       legend = paste(levels(iris$Species), \"mean\"),\n       col = species_colors,\n       lwd = 2,\n       title = \"Species Means\")\n\n\n\n\n\n\n\nThis practice question tests understanding of: 1. Using aggregate() to calculate multiple statistics grouped by a factor 2. Creating informative boxplots with appropriate labels 3. Adding custom elements to enhance a plot’s information content 4. Working with the iris dataset, which is commonly used for teaching data visualization\nThe solution demonstrates how to create a comprehensive visualization that combines boxplots (showing distribution) with horizontal lines (showing means), providing a rich view of how petal length varies across iris species.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "07 Explore data"
    ]
  },
  {
    "objectID": "05-data-frames.html",
    "href": "05-data-frames.html",
    "title": "05 Data frames",
    "section": "",
    "text": "Like your room, data should be tidy",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "05 Data frames"
    ]
  },
  {
    "objectID": "05-data-frames.html#data-frames-in-r",
    "href": "05-data-frames.html#data-frames-in-r",
    "title": "05 Data frames",
    "section": "1 Data frames in R\n",
    "text": "1 Data frames in R\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this lesson, you will be able to:\n\nDescribe and use common data file types\nUse Excel for data setup and create a Data Dictionary\nRead data from Excel and CSV files\nManipulate variables in a data frame\n\n\n\n\nNB for this page we assume you have access to Microsoft Excel. However, similar spreadsheet software (like Libre Office Calc) will work fine.\n\n\nThe first step in using R for data analysis is getting your data into R. The first step for getting your data into R is making your data tidy.\n\nThe commonest question we have experienced for new users of R who want to perform analysis on their data is how to get data into R. There is good news and bad news. The good news is that it is exceedingly easy to get data into R for analysis, in almost any format. The bad news is that a step most new users find challenging is taking responsibility for their own data.\nWhat we mean here is that best practice in data management involves active engagement with your dataset. This includes choosing appropriate variable names, error checking, and documenting information about variables and data collection. We also aim to avoid proliferation of excessive dataset versions, and, worst of all, embedding graphs and data summaries into Excel spreadsheets with data.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "05 Data frames"
    ]
  },
  {
    "objectID": "05-data-frames.html#tidy-data-concept",
    "href": "05-data-frames.html#tidy-data-concept",
    "title": "05 Data frames",
    "section": "2 Tidy data concept",
    "text": "2 Tidy data concept\n2.1 The Tidy Data concept\nA concept to streamline data preparation for analysis is Tidy Data. The basic idea is to format data for analysis in a way that\n\nArchives data for reproducibility of results, and\nMakes the data transparent to colleagues or researchers by documenting a data dictionary.\n\n\nThis page is all about the tidy data concept and a simple recipe for best practice to prepare data for analysis and to get data into R.\nThe definition of Tidy Data is generally attributed to Wickham (2014), and is based on the idea that with a few simple rules, data can be archived for complete reproducibility of results. This practice benefits any user because it facilitates collaboration at the same time as documenting both data and analysis methods for value to future use.\nThe essentials of Tidy Data are:\n\nEach variable should be in a column\nEach independent observation should be in a row\nA Data Dictionary should be associatied with the dataset, such that completely reproducible analysis is possible",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "05 Data frames"
    ]
  },
  {
    "objectID": "05-data-frames.html#common-data-file-types",
    "href": "05-data-frames.html#common-data-file-types",
    "title": "05 Data frames",
    "section": "3 Common data file types",
    "text": "3 Common data file types\nThe best file type for the majority of people to archive data for analysis is in a plain text Comma Separated Values (CSV or .csv) file, or just an Excel Spreadsheet. Best practice in contemporary scientific data analysis dictates that proprietary data formats should be avoided, like those produced by SPSS, Genstat, Minitab or other programs.\nThe reason for this is that data stored in those formats is not necessarily useful to people who do not have access to the software, and that for archiving purposes, such software file formats tend to change over time. While Excel is a proprietary format, we find that it is is easy to use, (almost completely) ubiquitous, and relatively resilient to backwards compatibility issues. Thus, sticking to CSV or Excel is a rule you should have a very good reason if you choose to break from it.\nWe recommend using Excel to store data with the goal (for simple datasets and analyses) of having one table for the actual data, in Tidy Data format, and a second tab consisting of a Data Dictionary where each variable is described in enough detail to completely reproduce any analysis. Generally, no formatting or results should ever be embedded in an Excel spreadsheet that is used to store data.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "05 Data frames"
    ]
  },
  {
    "objectID": "05-data-frames.html#excel-data-setup-and-the-data-dictionary",
    "href": "05-data-frames.html#excel-data-setup-and-the-data-dictionary",
    "title": "05 Data frames",
    "section": "4 Excel, data setup, and the Data Dictionary",
    "text": "4 Excel, data setup, and the Data Dictionary\n4.1 Tidy Data and Excel\nFor this section, you should download the following files in Excel (.xlsx) format:\n\nTidy Data example Excel file\nThe exact same information as un-Tidy Data\n\n\nThe aphid experiment\n\nYou are contacted by someone who wants help with data analysis and they give you some information about their experiment. They are interested in how diet affects the production of an important metabolite in pest aphids. They designed an experiment with a control treatment where aphids were allowed to feed on plain plants, another treatment where their diet was supplemented with one additive, “AD”, and a third treatment where their diet was supplemented with two additives, “AD” and “SA”. Another factor was aphid Genus, where individuals from the genera Brevicoryne and Myzus were tested. Three replicates of each treatment combination were performed:\n\n\naphid genus [2 levels] \\(\\times\\) food treatment [3 levels].\n\n\nThe metabolite of interest was measured with a spectrometer using three individual aphids from each replicate. The spectrometer peak area (an arbitrary scale) represents the total amount of the metabolite, which was converted to a real scale of metabolite total concentration. Finally, this total concentration was divided by 3 to estimate the concentration of the metabolite in each individual aphid.\n\n\n4.2 Untidy data\nHave a look at the file 5-untidy.xlsx in Excel.\n\nThe aphid dataset is fairly small and it is readable by humans, but in its current form it is not usable for analysis in R or other statistical software and there are a few ambiguous aspects which we will explore and try to improve.\n\nUntidy\n\n\nUntidy data\n\n\n\nThe file contains embedded figures and summary tables\nThere is empty white space in the file (Row 1 and Column A)\nThe variable names violate several naming conventions (spaces, special characters)\nMissing data is coded incorrectly (Row 13 was a failed data reading, but records zeros for the actual measurements)\nConversion information accessory to the data is present (Row 3)\nThere is no Data Dictionary (i.e. explanation of the variables)\nThe Aphid and Diet treatments are “confounded” in their coding\nWhat the heck is the “RT” column (most of the values are identical)\n\n\nNow, have a look at the Tidy Data version of the data file.\n\n\nTidy version of the data\n\n\n\nThe embedded figures have been removed\nThe white space rows and columns have been removed\nThe variable names have been edited but still are equally informative\nMissing data is coded correctly with “NA”\nThe conversion info has been removed and placed in the Data Dictionary\nA complete Data Dictionary on a new tab (“dictionary”) was added, explaining each variable\nThe aphid and food treatment variables were made separate\n\n\nTidy Data version Data Dictionary tab\n\n\nTidy data dictionary\n\n\nNotice in the Data Dictionary how there is a row for each variable with the name of the variable and an explanation for each variable.\n\n4.3 CSV files\nOnce your data is tidy, it is very easy to read in Excel data files, or they can be exported into a text file format like CSV (comma separated values) to read straight into R or other programs.\nHave a look at the Tidy Data dataset in .csv file format.\nOpen it with a plain text editor (e.g. Notepad in Windows, or similar). You will notice that each column entry is separated from others with a comma ,, hence the name Comma Separated Values!\n\nTidy csv\n\n\nTidy csv",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "05 Data frames"
    ]
  },
  {
    "objectID": "05-data-frames.html#getting-data-into-r",
    "href": "05-data-frames.html#getting-data-into-r",
    "title": "05 Data frames",
    "section": "5 Getting data into R",
    "text": "5 Getting data into R\nWe still need to actually “read data into R” from external files. There are a very large number of ways to do this and most people eventually find their own workflow. We think it is best for most people to use Excel or CSV files in Tidy Data format.\nThe basics of reading external files from a script is to to use the read.xlsx() function in the openxlsx package (you will probably need to install this with the install.packages() function), or else to use read.csv() that comes standard in base R. We will briefly try both.\n\n5.1 Working directory\n\nBest practice when working with files is to formally set your “working directory”. Basically, this tells R where your input (i.e. data) and output (like scripts or figures) files should be.\n\n\nThere are several viable ways to set your working directory in R, e.g. via the Session menu:\n\n\nWorking directory\n\n\nHowever, the best way to do this this is to set your working directory using code with the setwd() function. Here we should a workflow for Windows, which is similar on other computer systems. We consider the step of setting a working directory essential for best practice.\nIf you are unfamiliar with how to obtain the path to your working directory, open windows explorer, navigate to the folder you wish to save your script, data files and other inputs and outputs. You can think of this folder as one that contains all related files for e.g. a data analysis project, or perhaps this bootcamp module!\n\n\n\nYour directory window might look similar to this\n\n\nNotice the folder “view” is set to “Details”, and also notice that the folder options are set to “Show file extensions”. We recommend setting your own settings like this (if using Windows Explorer).\nThe pointer is indicated in the circle marked A in the picture above.\nLeft click the area to the right of the folder text once (where the pointer is in the picture above) and you should see something similar to the figure below, where the folder path is displayed and the text is automatically selected.\n\n\n\nYour selected file path might look similar to this\n\n\nAssuming you have opened the File Explorer in your working directory or navigated there, the selected PATH is the working directory path which you can copy (Ctrl + c in Windows). In your script, you can now use getwd() to get and print your working directory path, and setwd(), which takes a single character string of the path for your working directory for the argument dir , to set it.\n\n\nR file paths use the forward slash symbol “/” to separate file names. A very important step for Windows users when setting the working directory in R is to change the Windows default “” for forward slashes…\n\n\n5.2 Read in your first file\nYou need this for the following code if you did not already download it above: .xlsx data file\n# Try this\n\ngetwd() # Prints working directory in Console\n\nsetwd(\"D:/Dropbox/git-rstats-bootcamp/website/data\")\n\n# NB the quotes\n# NB the use of \"/\"\n# NB this is MY directory - change the PATH to YOUR directory :)\n\ngetwd() # Check that change worked\n\n## Read in Excel data file\n\n\ninstall.packages(openxlsx, dep = T) # Run if needed\n\nlibrary(openxlsx) # Load package needed to read Excel files\n\n# Make sure the data file \"5-tidy.xlsx\" is in your working directory\nmy_data &lt;- read.xlsx(\"5-tidy.xlsx\")\n\nAll being well, you should see the following data object in your Global Environement. Note the small blue button (A, circled below) you can press to espand the view of the variables in your data frame.\n\n\nData in your Global Environment\n\n\nNote that the same procedure works with Comma Separated Values data files, and other kinds of files that you want to read into R, except that the R function used will be specific to the file type. E.g., read.csv() for CSV files, read.delim for TAB delimited files, or read.table() as a generic function to tailor to many types of plain text data files (there are many others, but this is enough for now).",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "05 Data frames"
    ]
  },
  {
    "objectID": "05-data-frames.html#manipulating-variables-in-the-data-frame",
    "href": "05-data-frames.html#manipulating-variables-in-the-data-frame",
    "title": "05 Data frames",
    "section": "6 Manipulating variables in the Data Frame",
    "text": "6 Manipulating variables in the Data Frame\n\nNow that there is a data frame in your working environment, we can start working with the variables. This is a good time to think about the “R Space” metaphor. You are floating in R Space and you can see a data frame called my_data. You cannot see inside the container, so we will look at methods of accessing the data inside by name…\n\n\n6.1 Data manipulation in R\n\n\nThe names() function\nThe use of the $ operator for data frames\nThe use of the str() function for data frames\nThe use of the index operator [ , ]\nThe use of the attach() function\n\n\nCarefully use the follow code and try some data manipulation on your own.\n6.2 class()\n\n# Try this\n\nclass(my_data) # data.frame, a generic class for holding data\n6.3 names()\n\nThe names() function returns the name of attributes in R objects. When used on a data frame it returns the names of the variables.\n# Try this\nnames(my_data)\n6.4 $ operator for data frames\nThe $ operator allows us to access variable names inside R objects. Use it like this:\ndata_object$variable_name\n# Try this\n\nconc.ind # Error because the variable conc.ind is INSIDE my_data\n\nmy_data$conc.ind\n6.5 str()\n\nThe str() function returns the STRUCTURE of a data frame. This includes variable names, classes, and the first few values\n# Try this\nstr(my_data) \nThe output similar to the graphical Global Environment view in RStudio. Note the conc.ind variable is classed numeric\nNote the treatment variable is classed as character (not a factor)\n\n6.6 [ , ] the index operator\nThe index operator allows us to access specified rows and columns in data frames (this works exactly the same in matrices and other indexed objects).\n# Try this\nmy_data$conc.tot # The conc.tot variable with $\nmy_data$conc.tot[1:6] # each variable is a vector - 1st to 6th values\n\nhelp(dim)\ndim(my_data) # my_data has 18 rows, 6 columns\n\nmy_data[ , ] # Leaving blanks means return all rows and columns\n\nnames(my_data) # Note conc.tot is the 6th variable\n\nnames(my_data)[6] # Returns the name of the 6th variable\n\nmy_data[ , 6] # Returns all rows of the 6th variable in my_data\n\n# We can explicitly specify all rows (there are 18 remember)\nmy_data[1:18 , 6] # ALSO returns all rows of the 6th variable in my_data\n\n# We can specify the variable names with a character\nmy_data[ , \"conc.tot\"]\nmy_data[ , \"conc.ind\"]\n\n# Specify more than 1 by name with c() in the column slot of [ , ]\nmy_data[ , c(\"conc.tot\", \"conc.ind\")] \n6.7 attach()\n\nThe attach() function makes variable names available for a data frame in R space\n# Try this\nconc.ind # Error; the Passive-Aggressive Butler doesn't understand...\n\nattach(my_data)\nconc.ind # Now that my_data is \"attached\", the Butler can find variables inside\n\nhelp(detach) # Undo attach()\ndetach(my_data)\nconc.ind # Is Sir feeling well, Sir?",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "05 Data frames"
    ]
  },
  {
    "objectID": "05-data-frames.html#practice-exercises",
    "href": "05-data-frames.html#practice-exercises",
    "title": "05 Data frames",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\nButterfly data in xlsx format The data are from a small experiment measuring antenna length in butterflies manipulating diet in both sexes.\n7.1 Butterfly Data\nDownload the data file above and place it in a working directory. Set your working directory. Read in the data file and place it in a data frame object named data1. After examining the data, use mean() to calculate the mean of the variable length and report the results in a comment to two decimal points accuracy. Show your R code.\n\n\n\n\n\n\n\nReading and Analyzing Butterfly Data\n\n\n\n\n\n\n# Set working directory (adjust path to your actual directory)\n# setwd(\"your/path/here\")\n\n# Load the openxlsx package\nlibrary(openxlsx)\n\n# Read in the data file\ndata1 &lt;- read.xlsx(\"data/5-butterfly.xlsx\")\n\n# Examine the data structure\nstr(data1)\n\n# Calculate the mean of the length variable\nmean_length &lt;- mean(data1$length)\n\n# Display the mean rounded to 2 decimal places\nround(mean_length, 2)\n\nAfter examining the butterfly data, the mean antenna length is 6.92 mm (rounded to 2 decimal places).\nThe code first loads the openxlsx package, reads the Excel file into a data frame named data1, examines the structure to verify the data was loaded correctly, and then calculates the mean of the length variable.\n\n\n\n\n7.2 Factor Conversion\nShow the code to convert the diet variable to an ordinal factor with the order “control” &gt; “enhanced”, and the sex variable to a plain categorical factor.\n\n\n\n\n\n\n\nConverting Variables to Factors\n\n\n\n\n\n\n# Load the openxlsx package and read the data\nlibrary(openxlsx)\ndata1 &lt;- read.xlsx(\"data/5-butterfly.xlsx\")\n\n# Convert diet to an ordinal factor with specified order\ndata1$diet &lt;- factor(data1$diet, levels = c(\"control\", \"enhanced\"), ordered = TRUE)\n\n# Convert sex to a categorical (non-ordered) factor\ndata1$sex &lt;- factor(data1$sex)\n\n# Verify the conversions\nstr(data1)\n\nThe code converts: - The diet variable to an ordered factor with levels “control” followed by “enhanced” - The sex variable to a regular (non-ordered) factor\nUsing ordered = TRUE for the diet variable creates an ordinal factor where the levels have a specific order, which can be important for certain types of analysis. The str() function confirms the conversions were successful.\n\n\n\n\n7.3 Data Frame Subsetting\nShow code for two different variations of using only the [ , ] operator with your data frame to show the following output: diet length 8 control 6 9 control 7 10 control 6 11 enhanced 8 12 enhanced 7 13 enhanced 9\n\n\n\n\n\n\n\nData Frame Subsetting\n\n\n\n\n\n\n# Load the openxlsx package and read the data\nlibrary(openxlsx)\ndata1 &lt;- read.xlsx(\"data/5-butterfly.xlsx\")\n\n# Method 1: Using row numbers and column names\ndata1[8:13, c(\"diet\", \"length\")]\n\n# Method 2: Using row numbers and column indices\n# First, find which columns are diet and length\nnames(data1)  # Check column positions\ndata1[8:13, c(1, 3)]  # Assuming diet is column 1 and length is column 3\n\nThese two methods demonstrate different ways to subset a data frame using the [ , ] operator:\n\nThe first method uses row numbers (8 through 13) and column names (“diet” and “length”)\nThe second method uses row numbers and column indices (positions)\n\nBoth produce the same output showing diet and length values for rows 8-13, which includes three control and three enhanced diet observations.\n\n\n\n\n7.4 CSV Without Headers\nShow code to read in a comma separated values data file that does not have a header (first row containing variable names).\n\n\n\n\n\n\n\nReading CSV Without Headers\n\n\n\n\n\n\n# Method to read a CSV file without headers\ndata_no_header &lt;- read.csv(\"data/5-butterfly.csv\", header = FALSE)\n\n# Examine the structure\nstr(data_no_header)\n\n# When reading a CSV without headers, R assigns default names V1, V2, etc.\n# We can assign our own column names if needed:\nnames(data_no_header) &lt;- c(\"diet\", \"sex\", \"length\")\n\n# Verify the new column names\nhead(data_no_header)\n\nWhen reading a CSV file without headers (column names in the first row):\n\nUse the read.csv() function with header = FALSE parameter\nR will automatically assign generic column names (V1, V2, V3, etc.)\nYou can then assign meaningful column names using the names() function\n\nThis approach is useful when working with data files that contain only values without descriptive headers.\n\n\n\n\n7.5 Attach Function\nDescribe in your own words what the attach() function does.\n\n\n\n\n\n\n\nAttach Function Explanation\n\n\n\n\n\n\n# Example of attach() functionality\nlibrary(openxlsx)\ndata1 &lt;- read.xlsx(\"data/5-butterfly.xlsx\")\n\n# Without attach, we need to use the $ operator\nmean(data1$length)\n\n# With attach, we can access variables directly\nattach(data1)\nmean(length)\n\n# Always detach when finished to avoid conflicts\ndetach(data1)\n\nThe attach() function makes the variables (columns) in a data frame directly accessible in the R environment without having to specify the data frame name and $ operator each time.\nWhen you attach a data frame, R adds its variables to the search path, allowing you to refer to them by name alone. This can make code more concise and readable, especially when working extensively with a single data frame.\nHowever, using attach() comes with risks: - It can create naming conflicts if variables have the same names as other objects - It can lead to confusion about which data is being used - It’s easy to forget which data frame is attached - It’s considered poor practice in modern R programming\nBest practice is to use the explicit data frame$variable notation instead, or to use functions like with() or pipe operators for cleaner code.\n\n\n\n\n7.6 Data Frame Question\nWrite a plausible practice question involving any aspect of manipulation of a data frame.\n\n\n\n\n\n\n\nData Frame Manipulation Question\n\n\n\n\n\n\n# A plausible practice question could be:\n\n# \"Using the butterfly dataset, create a new data frame that contains only female \n# butterflies. Then add a new column called 'length_category' that categorizes \n# antenna length as 'short' if less than 7 mm, and 'long' if 7 mm or greater. \n# Finally, calculate the mean antenna length for each diet type within this \n# female-only dataset.\"\n\n# Solution:\nlibrary(openxlsx)\ndata1 &lt;- read.xlsx(\"data/5-butterfly.xlsx\")\n\n# Create a subset with only females\nfemales &lt;- data1[data1$sex == \"female\", ]\n\n# Add a new column for length category\nfemales$length_category &lt;- ifelse(females$length &lt; 7, \"short\", \"long\")\n\n# Calculate mean length by diet type\ntapply(females$length, females$diet, mean)\n\n# Alternative using aggregate function\naggregate(length ~ diet, data = females, FUN = mean)\n\nThis practice question tests understanding of: - Subsetting data frames using logical conditions - Adding new calculated columns - Using functions like tapply() or aggregate() to calculate statistics by group - Working with categorical data\nThe solution demonstrates multiple data frame manipulation techniques commonly used in data analysis workflows.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "05 Data frames"
    ]
  },
  {
    "objectID": "13-reproducibility.html",
    "href": "13-reproducibility.html",
    "title": "13 Reproducibility",
    "section": "",
    "text": "Reproducible research is beautiful",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "13 Reproducibility"
    ]
  },
  {
    "objectID": "13-reproducibility.html#why-reproducibility-matters",
    "href": "13-reproducibility.html#why-reproducibility-matters",
    "title": "13 Reproducibility",
    "section": "1 Why Reproducibility Matters",
    "text": "1 Why Reproducibility Matters\nReproducibility is a cornerstone of scientific research and data analysis. It ensures that your findings can be verified, your methods can be understood, and your work can be built upon by others (including your future self!).\n\n\n\n\n\n\nDefinition\n\n\n\nReproducibility means that your analysis can be recreated by others using the same data and methods, producing the same results.\n\n\nIn practice, reproducibility provides several key benefits:\n\nVerification: Others can confirm your findings\nCollaboration: Team members can understand and contribute to your work\nEfficiency: You can revisit and build upon your own work more easily\nImpact: Your research can have greater influence when others can use it\nTrust: Reproducible research builds credibility in your findings",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "13 Reproducibility"
    ]
  },
  {
    "objectID": "13-reproducibility.html#components-of-reproducible-workflows",
    "href": "13-reproducibility.html#components-of-reproducible-workflows",
    "title": "13 Reproducibility",
    "section": "2 Components of Reproducible Workflows",
    "text": "2 Components of Reproducible Workflows\nA reproducible data science workflow typically includes the following elements:\n\n2.1 Documented Code\n# Example of well-documented code\n# Purpose: Calculate mean values by group\n# Input: data frame with numeric 'value' column and categorical 'group' column\n# Output: data frame of group means\n\ncalculate_group_means &lt;- function(data, value_col, group_col) {\n  # Check inputs\n  if (!is.data.frame(data)) {\n    stop(\"Input must be a data frame\")\n  }\n  \n  # Calculate means by group\n  result &lt;- aggregate(data[[value_col]], by = list(Group = data[[group_col]]), \n                     FUN = mean, na.rm = TRUE)\n  \n  # Rename columns for clarity\n  names(result)[2] &lt;- \"Mean\"\n  \n  return(result)\n}\n\n\n2.2 Version Control\nVersion control systems like Git help track changes to your code and files over time. We’ll cover this in detail in a later lesson.\n\n\n2.3 Environment Management\nDocumenting your software environment ensures others can recreate the same conditions:\n# Example of capturing package versions\nsessionInfo()\n\n# Or using the renv package for project-specific environments\n# install.packages(\"renv\")\n# renv::init()\n# renv::snapshot()\n\n\n2.4 Data Management\nProper data management includes:\n\nRaw data preservation (never modify the original data)\nData cleaning scripts (document all transformations)\nClear data documentation (metadata)\n\n\n\n2.5 Clear Documentation\nDocumentation should include:\n\nProject overview and purpose\nData sources and descriptions\nAnalysis methods and justification\nInstructions for reproducing results",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "13 Reproducibility"
    ]
  },
  {
    "objectID": "13-reproducibility.html#file-organization-strategies",
    "href": "13-reproducibility.html#file-organization-strategies",
    "title": "13 Reproducibility",
    "section": "3 File Organization Strategies",
    "text": "3 File Organization Strategies\nAn organized file structure makes your project more navigable and reproducible:\nproject/\n├── README.md           # Project overview and instructions\n├── data/\n│   ├── raw/            # Original, immutable data\n│   └── processed/      # Cleaned, transformed data\n├── code/\n│   ├── 01_clean.R      # Data cleaning script\n│   ├── 02_analyze.R    # Analysis script\n│   └── 03_visualize.R  # Visualization script\n├── results/\n│   ├── figures/        # Generated plots\n│   └── tables/         # Generated tables\n├── docs/\n│   └── report.Rmd      # R Markdown report\n└── renv/               # Package environment information\n\n\n\n\n\n\nBest Practice\n\n\n\nName your files in a way that indicates their order and purpose, such as 01_data_cleaning.R, 02_analysis.R, etc.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "13 Reproducibility"
    ]
  },
  {
    "objectID": "13-reproducibility.html#documentation-best-practices",
    "href": "13-reproducibility.html#documentation-best-practices",
    "title": "13 Reproducibility",
    "section": "4 Documentation Best Practices",
    "text": "4 Documentation Best Practices\nEffective documentation should:\n\nBe comprehensive - Include all necessary information\nBe clear - Use plain language and avoid jargon\nBe current - Update as your project evolves\nBe accessible - Store documentation with your project\n\nA good README file typically includes:\n\nProject title and description\nInstallation and setup instructions\nUsage examples\nData dictionary\nAnalysis workflow overview\nDependencies and requirements",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "13 Reproducibility"
    ]
  },
  {
    "objectID": "13-reproducibility.html#practice-exercises",
    "href": "13-reproducibility.html#practice-exercises",
    "title": "13 Reproducibility",
    "section": "5 Practice Exercises",
    "text": "5 Practice Exercises\n\n\n5.1 Project Evaluation\nEvaluate a recent project of yours for reproducibility. Identify three specific improvements you could make to enhance its reproducibility.\n\n\n\n\n\n\n\nSolution: Reproducibility Evaluation\n\n\n\n\n\nThree specific improvements to enhance reproducibility in a data analysis project:\n\nVersion Control Implementation: Set up a Git repository for the project to track changes to code and documentation. This would allow for better tracking of modifications, collaboration with others, and the ability to revert to previous versions if needed.\n\n# Example Git setup commands\n# Initialize a Git repository\ngit init\n\n# Add all files to the repository\ngit add .\n\n# Make an initial commit\ngit commit -m \"Initial commit of project files\"\n\n# Create a remote repository on GitHub and link it\ngit remote add origin https://github.com/username/project-name.git\ngit push -u origin main\n\nEnvironment Documentation: Create a reproducible environment using the renv package to track and manage package dependencies. This ensures that others can recreate the exact same package versions used in the analysis.\n\n# Install and initialize renv\ninstall.packages(\"renv\")\nrenv::init()\n\n# After installing all required packages, create a snapshot\nrenv::snapshot()\n\n# Include instructions in README.md for others to restore the environment\n# renv::restore()\n\nData Pipeline Documentation: Create a clear data processing pipeline with numbered script files and documentation of each transformation step. This would make it easier to understand how raw data was processed into the final analysis dataset.\n\n# Example file structure\n# 01_data_import.R - Import raw data\n# 02_data_cleaning.R - Clean and preprocess data\n# 03_analysis.R - Perform main analysis\n# 04_visualization.R - Create visualizations\n\n# Example documentation in scripts\n#' Data Cleaning Script\n#' \n#' This script performs the following operations:\n#' 1. Removes duplicate records\n#' 2. Handles missing values\n#' 3. Transforms variables to appropriate types\n#' 4. Creates derived variables needed for analysis\n#' \n#' Input: raw_data.csv\n#' Output: clean_data.csv\n\n\n\n\n\n\n5.2 File Organization\nCreate a file organization template for your next data analysis project, following the best practices outlined in this lesson.\n\n\n\n\n\n\n\nSolution: Project Organization Template\n\n\n\n\n\nHere’s a file organization template for a data analysis project following best practices:\nproject_name/\n├── README.md                 # Project overview, setup instructions, and usage\n├── CONTRIBUTING.md           # Guidelines for contributors (if collaborative)\n├── LICENSE                   # Project license information\n├── .gitignore                # Specifies files to exclude from version control\n├── renv/                     # R environment information (created by renv)\n├── renv.lock                 # Package dependency snapshot\n├── data/\n│   ├── raw/                  # Original, immutable data\n│   │   ├── dataset1.csv      # Original data files\n│   │   └── dataset2.xlsx\n│   ├── processed/            # Cleaned, transformed data\n│   │   ├── clean_dataset1.csv\n│   │   └── analysis_ready.rds\n│   └── README.md             # Data dictionary and source information\n├── code/\n│   ├── 00_setup.R            # Project setup, load packages, set global options\n│   ├── 01_import_data.R      # Data import scripts\n│   ├── 02_clean_data.R       # Data cleaning and preprocessing\n│   ├── 03_explore_data.R     # Exploratory data analysis\n│   ├── 04_analyze_data.R     # Main analysis scripts\n│   └── 05_visualize_data.R   # Visualization scripts\n├── results/\n│   ├── figures/              # Generated plots and visualizations\n│   │   ├── exploratory/      # Exploratory visualizations\n│   │   └── final/            # Publication-ready figures\n│   └── tables/               # Generated tables and summaries\n├── docs/\n│   ├── data_dictionary.md    # Detailed variable descriptions\n│   ├── analysis_plan.md      # Pre-specified analysis plan\n│   └── methods.md            # Detailed methodological notes\n└── reports/\n    ├── exploratory_report.Rmd # R Markdown for exploratory analysis\n    └── final_report.Rmd       # Final analysis report\nKey features of this template: - Clear separation of raw and processed data - Numbered scripts to indicate workflow order - Comprehensive documentation - Version control readiness - Environment management with renv - Separate locations for code, data, results, and documentation\n\n\n\n\n\n\n5.3 Function Documentation\nWrite documentation for a simple R function that you commonly use, following the guidelines for effective code documentation.\n\n\n\n\n\n\n\nSolution: Function Documentation\n\n\n\n\n\nHere’s an example of well-documented R function for summarizing data:\n#' Summarize Numeric Variables by Group\n#' \n#' This function calculates summary statistics for numeric variables\n#' grouped by one or more categorical variables.\n#' \n#' @param data A data frame containing the variables to summarize\n#' @param num_vars Character vector of numeric variable names to summarize\n#' @param group_vars Character vector of grouping variable names\n#' @param stats Character vector of statistics to calculate.\n#'        Options include: \"mean\", \"median\", \"sd\", \"min\", \"max\", \"n\", \"se\"\n#' @param na.rm Logical indicating whether to remove NA values (default: TRUE)\n#' \n#' @return A data frame with summary statistics for each numeric variable by group\n#' \n#' @examples\n#' # Create example data\n#' example_data &lt;- data.frame(\n#'   group1 = rep(c(\"A\", \"B\"), each = 10),\n#'   group2 = rep(c(\"X\", \"Y\"), times = 10),\n#'   value1 = rnorm(20, mean = 10, sd = 2),\n#'   value2 = runif(20, min = 0, max = 100)\n#' )\n#' \n#' # Summarize one numeric variable by one group\n#' summarize_by_group(\n#'   data = example_data,\n#'   num_vars = \"value1\",\n#'   group_vars = \"group1\",\n#'   stats = c(\"mean\", \"sd\", \"n\")\n#' )\n#' \n#' # Summarize multiple numeric variables by multiple groups\n#' summarize_by_group(\n#'   data = example_data,\n#'   num_vars = c(\"value1\", \"value2\"),\n#'   group_vars = c(\"group1\", \"group2\"),\n#'   stats = c(\"median\", \"min\", \"max\")\n#' )\n#' \n#' @export\nsummarize_by_group &lt;- function(data, num_vars, group_vars, \n                               stats = c(\"mean\", \"sd\", \"median\", \"min\", \"max\", \"n\"),\n                               na.rm = TRUE) {\n  # Input validation\n  if (!is.data.frame(data)) {\n    stop(\"'data' must be a data frame\")\n  }\n  if (!all(num_vars %in% names(data))) {\n    stop(\"Not all numeric variables found in data\")\n  }\n  if (!all(group_vars %in% names(data))) {\n    stop(\"Not all grouping variables found in data\")\n  }\n  \n  # Create grouping formula for aggregate\n  group_formula &lt;- as.formula(paste(\"~\", paste(group_vars, collapse = \"+\")))\n  \n  # Initialize results list\n  results_list &lt;- list()\n  \n  # Calculate standard error function\n  se &lt;- function(x, na.rm = TRUE) {\n    if (na.rm) x &lt;- x[!is.na(x)]\n    sqrt(var(x) / length(x))\n  }\n  \n  # Process each numeric variable\n  for (var in num_vars) {\n    # Initialize stats list for this variable\n    var_stats &lt;- list()\n    \n    # Calculate requested statistics\n    if (\"mean\" %in% stats) {\n      var_stats$mean &lt;- aggregate(data[[var]], by = eval(group_formula, data), \n                                 FUN = mean, na.rm = na.rm)\n    }\n    if (\"median\" %in% stats) {\n      var_stats$median &lt;- aggregate(data[[var]], by = eval(group_formula, data), \n                                   FUN = median, na.rm = na.rm)\n    }\n    if (\"sd\" %in% stats) {\n      var_stats$sd &lt;- aggregate(data[[var]], by = eval(group_formula, data), \n                               FUN = sd, na.rm = na.rm)\n    }\n    if (\"min\" %in% stats) {\n      var_stats$min &lt;- aggregate(data[[var]], by = eval(group_formula, data), \n                                FUN = min, na.rm = na.rm)\n    }\n    if (\"max\" %in% stats) {\n      var_stats$max &lt;- aggregate(data[[var]], by = eval(group_formula, data), \n                                FUN = max, na.rm = na.rm)\n    }\n    if (\"n\" %in% stats) {\n      var_stats$n &lt;- aggregate(data[[var]], by = eval(group_formula, data), \n                              FUN = function(x) sum(!is.na(x)))\n    }\n    if (\"se\" %in% stats) {\n      var_stats$se &lt;- aggregate(data[[var]], by = eval(group_formula, data), \n                               FUN = se, na.rm = na.rm)\n    }\n    \n    # Combine all stats for this variable\n    var_result &lt;- var_stats[[1]]\n    names(var_result)[ncol(var_result)] &lt;- paste0(var, \"_\", names(var_stats)[1])\n    \n    for (i in 2:length(var_stats)) {\n      stat_name &lt;- names(var_stats)[i]\n      var_result[[paste0(var, \"_\", stat_name)]] &lt;- var_stats[[i]][, ncol(var_stats[[i]])]\n    }\n    \n    results_list[[var]] &lt;- var_result\n  }\n  \n  # Merge results for all variables\n  final_result &lt;- results_list[[1]]\n  if (length(results_list) &gt; 1) {\n    for (i in 2:length(results_list)) {\n      final_result &lt;- merge(final_result, results_list[[i]], by = group_vars)\n    }\n  }\n  \n  return(final_result)\n}\nThis documentation follows best practices by including: 1. A clear title and description 2. Detailed parameter descriptions 3. Information about the return value 4. Multiple examples showing different use cases 5. Export tag for package development 6. Input validation within the function 7. Comprehensive comments explaining the code logic",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "13 Reproducibility"
    ]
  },
  {
    "objectID": "13-reproducibility.html#additional-resources",
    "href": "13-reproducibility.html#additional-resources",
    "title": "13 Reproducibility",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nThe Turing Way: Guide to Reproducible Research\nTen Simple Rules for Reproducible Computational Research\nrOpenSci Reproducibility Guide",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "13 Reproducibility"
    ]
  },
  {
    "objectID": "14-rmarkdown.html",
    "href": "14-rmarkdown.html",
    "title": "14 R Markdown",
    "section": "",
    "text": "Use markdown to make great reports",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#what-is-r-markdown",
    "href": "14-rmarkdown.html#what-is-r-markdown",
    "title": "14 R Markdown",
    "section": "1 What is R Markdown?",
    "text": "1 What is R Markdown?\nR Markdown is a file format that allows you to combine R code, its output, and narrative text in a single document. It’s a powerful tool for creating reproducible reports, presentations, dashboards, and even websites.\n\n\n\n\n\n\nKey Concept\n\n\n\nR Markdown documents are plain text files that contain three important elements: 1. YAML header - Document metadata and formatting options 2. Markdown text - For narrative content 3. Code chunks - R code that can be executed",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#getting-started-with-r-markdown",
    "href": "14-rmarkdown.html#getting-started-with-r-markdown",
    "title": "14 R Markdown",
    "section": "2 Getting Started with R Markdown",
    "text": "2 Getting Started with R Markdown\nTo create an R Markdown document in RStudio:\n\nClick File → New File → R Markdown\n\nChoose a document type (HTML, PDF, or Word)\nGive your document a title and author name\nClick OK\n\n\nThis will generate a template R Markdown file that you can modify.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#yaml-header",
    "href": "14-rmarkdown.html#yaml-header",
    "title": "14 R Markdown",
    "section": "3 YAML Header",
    "text": "3 YAML Header\nThe YAML header appears at the top of the document between triple dashes (---):\n---\ntitle: \"My Analysis Report\"\nauthor: \"Your Name\"\ndate: \"2023-06-15\"\noutput: html_document\n---\nYou can customize various aspects of your document by adding options to the YAML header:\n---\ntitle: \"My Analysis Report\"\nauthor: \"Your Name\"\ndate: \"2025-07-08\"\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n    theme: flatly\n    highlight: tango\n    code_folding: show\n---",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#markdown-text",
    "href": "14-rmarkdown.html#markdown-text",
    "title": "14 R Markdown",
    "section": "4 Markdown Text",
    "text": "4 Markdown Text\nMarkdown is a lightweight markup language that allows you to format text using simple syntax:\n4.1 Basic Formatting\n# Heading 1\n## Heading 2\n### Heading 3\n\n**Bold text**\n*Italic text*\n~~Strikethrough~~\n\n[Link text](https://example.com)\n![Image alt text](path/to/image.png)\n4.2 Lists\nUnordered list:\n- Item 1\n- Item 2\n  - Subitem 2.1\n  - Subitem 2.2\n\nOrdered list:\n1. First item\n2. Second item\n   a. Subitem a\n   b. Subitem b\n4.3 Tables\n| Column 1 | Column 2 | Column 3 |\n|----------|----------|----------|\n| Row 1    | Data     | Data     |\n| Row 2    | Data     | Data     |",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#code-chunks",
    "href": "14-rmarkdown.html#code-chunks",
    "title": "14 R Markdown",
    "section": "5 Code Chunks",
    "text": "5 Code Chunks\nCode chunks in R Markdown allow you to execute R code and display its results:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your R code here\n```\n:::\nFor example:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data\ndata(mtcars)\n\n# Calculate summary statistics\nsummary(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n```\n\n\n:::\n:::\n5.1 Chunk Options\nYou can control how code chunks behave using options:\n\n\nOption\nDescription\n\n\n\neval=TRUE/FALSE\nWhether to evaluate the code\n\n\necho=TRUE/FALSE\nWhether to show the code\n\n\ninclude=TRUE/FALSE\nWhether to include the chunk in output\n\n\nmessage=TRUE/FALSE\nWhether to display messages\n\n\nwarning=TRUE/FALSE\nWhether to display warnings\n\n\nfig.width=7\nFigure width in inches\n\n\nfig.height=5\nFigure height in inches\n\n\n\nExample:\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-rmarkdown_files/figure-html/plot-1.png){width=768}\n:::\n:::",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#inline-code",
    "href": "14-rmarkdown.html#inline-code",
    "title": "14 R Markdown",
    "section": "6 Inline Code",
    "text": "6 Inline Code\nYou can also include R code directly within text using backticks and r:\nThe average miles per gallon is 20.090625.\nThis will calculate the mean and insert the result directly into your text.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#generating-reports",
    "href": "14-rmarkdown.html#generating-reports",
    "title": "14 R Markdown",
    "section": "7 Generating Reports",
    "text": "7 Generating Reports\nTo generate your report:\n\nClick the Knit button in RStudio\nChoose your desired output format\nView the generated document\n\nR Markdown will: - Run all the code chunks - Generate all outputs (tables, plots, etc.) - Format the text according to Markdown syntax - Combine everything into a single document",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#example-data-analysis-report",
    "href": "14-rmarkdown.html#example-data-analysis-report",
    "title": "14 R Markdown",
    "section": "8 Example: Data Analysis Report",
    "text": "8 Example: Data Analysis Report\nHere’s a simple example of an R Markdown document for data analysis:\n---\ntitle: \"Car Performance Analysis\"\nauthor: \"Data Scientist\"\ndate: \"2025-07-08\"\noutput: html_document\n---\n\n### Introduction\n\nThis report analyzes the relationship between car characteristics and fuel efficiency.\n\n### Data Overview\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the mtcars dataset\ndata(mtcars)\n\n# Display the first few rows\nhead(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Summary statistics\nsummary(mtcars[, c(\"mpg\", \"wt\", \"hp\")])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      mpg              wt              hp       \n Min.   :10.40   Min.   :1.513   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:2.581   1st Qu.: 96.5  \n Median :19.20   Median :3.325   Median :123.0  \n Mean   :20.09   Mean   :3.217   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:3.610   3rd Qu.:180.0  \n Max.   :33.90   Max.   :5.424   Max.   :335.0  \n```\n\n\n:::\n:::\n\n\n### Visualization\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a scatterplot\nplot(mtcars$wt, mtcars$mpg, \n     main=\"Car Weight vs. Mileage\",\n     xlab=\"Weight (1000 lbs)\",\n     ylab=\"Miles Per Gallon\",\n     pch=19, col=\"blue\")\n\n# Add a regression line\nabline(lm(mpg ~ wt, data = mtcars), col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](14-rmarkdown_files/figure-html/visualization-1.png){width=960}\n:::\n:::\n\n\n### Statistical Analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a linear model\nmodel &lt;- lm(mpg ~ wt + hp, data = mtcars)\n\n# Display model summary\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ wt + hp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n```\n\n\n:::\n:::\n\n\n### Conclusion\n\nBased on our analysis, there is a significant negative relationship between car weight and fuel efficiency. For every 1,000 lb increase in weight, the miles per gallon decreases by approximately 3.88 units.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#practice-exercises",
    "href": "14-rmarkdown.html#practice-exercises",
    "title": "14 R Markdown",
    "section": "9 Practice Exercises",
    "text": "9 Practice Exercises\n9.1 Basic R Markdown\nCreate a new R Markdown document that includes: - A title and your name - A brief introduction - A code chunk that loads and summarizes a dataset of your choice - A visualization of the data - A brief conclusion\n\n\n\n\n\n\n\nSolution: Basic R Markdown Document\n\n\n\n\n\nHere’s an example of a basic R Markdown document:\n---\ntitle: \"Analysis of Iris Dataset\"\nauthor: \"Your Name\"\ndate: \"2025-07-08\"\noutput: html_document\n---\n\n## Introduction\n\nThis document provides a brief analysis of the iris dataset, which contains measurements of sepal length, sepal width, petal length, and petal width for three species of iris flowers: setosa, versicolor, and virginica.\n\n## Data Summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the iris dataset\ndata(iris)\n\n# Display the structure of the dataset\nstr(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Summary statistics\nsummary(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n```\n\n\n:::\n:::\n\n\n## Data Visualization\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a scatterplot of sepal dimensions by species\nplot(iris$Sepal.Length, iris$Sepal.Width,\n     main = \"Sepal Dimensions by Iris Species\",\n     xlab = \"Sepal Length (cm)\",\n     ylab = \"Sepal Width (cm)\",\n     pch = 19,\n     col = as.numeric(iris$Species))\n\n# Add a legend\nlegend(\"topright\",\n       legend = levels(iris$Species),\n       col = 1:3,\n       pch = 19,\n       title = \"Species\")\n```\n\n::: {.cell-output-display}\n![](14-rmarkdown_files/figure-html/visualization-md-1.png){width=768}\n:::\n:::\n\n\n## Conclusion\n\nThe visualization reveals clear clustering of iris species based on sepal dimensions. Setosa irises (shown in black) have shorter sepals that are wider, while versicolor (red) and virginica (green) have longer, narrower sepals. This simple analysis demonstrates how morphological measurements can be used to distinguish between iris species.\nThis R Markdown document includes: 1. A YAML header with title, author, and date 2. A brief introduction to the dataset 3. A code chunk that loads and summarizes the iris dataset 4. A visualization showing the relationship between sepal dimensions by species 5. A brief conclusion interpreting the visualization\n\n\n\n\n9.2 Output Formats\nExperiment with different output formats (HTML, PDF, Word) and observe the differences.\n\n\n\n\n\n\n\nSolution: Output Format Comparison\n\n\n\n\n\nTo experiment with different output formats, you would modify the YAML header of your R Markdown document as follows:\nFor HTML output:\n---\ntitle: \"My Analysis\"\nauthor: \"Your Name\"\ndate: \"2025-07-08\"\noutput: html_document\n---\nFor PDF output:\n---\ntitle: \"My Analysis\"\nauthor: \"Your Name\"\ndate: \"2025-07-08\"\noutput: pdf_document\n---\nFor Word output:\n---\ntitle: \"My Analysis\"\nauthor: \"Your Name\"\ndate: \"2025-07-08\"\noutput: word_document\n---\nFor multiple output formats:\n---\ntitle: \"My Analysis\"\nauthor: \"Your Name\"\ndate: \"2025-07-08\"\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n  pdf_document:\n    toc: true\n  word_document:\n    toc: true\n---\nKey differences between formats:\n\n\nHTML:\n\nMost interactive and customizable\nSupports interactive elements (e.g., plotly plots, shiny apps)\nEasy to share online\nSupports custom CSS styling\nIncludes features like floating table of contents and code folding\n\n\n\nPDF:\n\nMore formal appearance, suitable for printing\nRequires LaTeX installation (TinyTeX recommended)\nBetter for precise layout control\nGood for academic papers and reports\nMay have issues with very large tables or complex plots\n\n\n\nWord:\n\nFamiliar format for non-technical collaborators\nEasy for others to edit and add comments\nGood for documents that need further editing\nLimited in terms of formatting control compared to HTML/PDF\nMay have inconsistent rendering of complex elements\n\n\n\nTo fully experience these differences, you would need to knit the same document to each format and compare the results.\n\n\n\n\n9.3 Advanced Features\nCreate an R Markdown document with a table of contents, code folding, and a custom theme.\n\n\n\n\n\n\n\nSolution: Advanced R Markdown Features\n\n\n\n\n\nHere’s an example of an R Markdown document with a table of contents, code folding, and a custom theme:\n---\ntitle: \"Advanced R Markdown Features\"\nauthor: \"Your Name\"\ndate: \"2025-07-08\"\noutput:\n  html_document:\n    toc: true\n    toc_float: \n      collapsed: false\n      smooth_scroll: true\n    toc_depth: 3\n    number_sections: true\n    theme: flatly\n    highlight: tango\n    code_folding: show\n    df_print: paged\n---\n\n\n\n# Introduction\n\nThis document demonstrates advanced R Markdown features including a floating table of contents, code folding, and a custom theme.\n\n# Data Analysis\n\n## Loading Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(datasets)\n```\n:::\n\n\n## Data Exploration\n\nLet's explore the built-in mtcars dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(mtcars)\nstr(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#&gt; 'data.frame':    32 obs. of  11 variables:\n#&gt;  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n#&gt;  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n#&gt;  $ disp: num  160 160 108 258 360 ...\n#&gt;  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n#&gt;  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n#&gt;  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n#&gt;  $ qsec: num  16.5 17 18.6 19.4 17 ...\n#&gt;  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n#&gt;  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n#&gt;  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n#&gt;  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#&gt;       mpg             cyl             disp             hp       \n#&gt;  Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n#&gt;  1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n#&gt;  Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n#&gt;  Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n#&gt;  3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n#&gt;  Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n#&gt;       drat             wt             qsec             vs        \n#&gt;  Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n#&gt;  1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n#&gt;  Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n#&gt;  Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n#&gt;  3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n#&gt;  Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n#&gt;        am              gear            carb      \n#&gt;  Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n#&gt;  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n#&gt;  Median :0.0000   Median :4.000   Median :2.000  \n#&gt;  Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n#&gt;  3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n#&gt;  Max.   :1.0000   Max.   :5.000   Max.   :8.000\n```\n\n\n:::\n:::\n\n\n## Data Visualization\n\n### Basic Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mtcars$wt, mtcars$mpg,\n     main = \"Car Weight vs. Mileage\",\n     xlab = \"Weight (1000 lbs)\",\n     ylab = \"Miles Per Gallon\",\n     pch = 19, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](14-rmarkdown_files/figure-html/basic-plot-1.png){width=100%}\n:::\n:::\n\n\n### Grouped Analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add a categorical variable\nmtcars$cyl_factor &lt;- as.factor(mtcars$cyl)\n\n# Boxplot by cylinder groups\nboxplot(mpg ~ cyl_factor, data = mtcars,\n        main = \"MPG by Number of Cylinders\",\n        xlab = \"Cylinders\",\n        ylab = \"Miles Per Gallon\",\n        col = c(\"lightblue\", \"lightgreen\", \"lightpink\"))\n```\n\n::: {.cell-output-display}\n![](14-rmarkdown_files/figure-html/grouped-analysis-1.png){width=100%}\n:::\n:::\n\n\n# Statistical Analysis\n\n## Linear Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a linear model\nmodel &lt;- lm(mpg ~ wt + hp, data = mtcars)\n\n# Display model summary\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = mpg ~ wt + hp, data = mtcars)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -3.941 -1.600 -0.182  1.050  5.854 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\n#&gt; wt          -3.87783    0.63273  -6.129 1.12e-06 ***\n#&gt; hp          -0.03177    0.00903  -3.519  0.00145 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.593 on 29 degrees of freedom\n#&gt; Multiple R-squared:  0.8268, Adjusted R-squared:  0.8148 \n#&gt; F-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n```\n\n\n:::\n:::\n\n\n## Diagnostic Plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(model)\n```\n\n::: {.cell-output-display}\n![](14-rmarkdown_files/figure-html/diagnostics-1.png){width=100%}\n:::\n:::\n\n\n# Conclusion\n\nThis document has demonstrated several advanced R Markdown features:\n\n1. A floating table of contents with section numbering\n2. Code folding (try clicking the \"Code\" buttons)\n3. The Flatly theme with Tango syntax highlighting\n4. Customized chunk options\n5. Multi-level headings that appear in the TOC\nKey features implemented:\n\n\nTable of Contents:\n\n\ntoc: true enables the table of contents\n\ntoc_float: collapsed: false, smooth_scroll: true creates a floating TOC\n\ntoc_depth: 3 includes headings up to level 3\n\nnumber_sections: true adds numbering to sections\n\n\n\nCode Folding:\n\n\ncode_folding: show makes code chunks expandable/collapsible\nDefault is to show code, but readers can hide it\n\n\n\nCustom Theme:\n\n\ntheme: flatly sets the document theme (other options include “default”, “cerulean”, “journal”, “lumen”, etc.)\n\nhighlight: tango sets the code highlighting style\n\n\n\nAdditional Features:\n\n\ndf_print: paged creates interactive tables for data frames\nThe setup chunk configures global options for all code chunks",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#additional-resources",
    "href": "14-rmarkdown.html#additional-resources",
    "title": "14 R Markdown",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nR Markdown: The Definitive Guide\nR Markdown Cheat Sheet\nR Markdown Cookbook",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "12-1-way-anova.html",
    "href": "12-1-way-anova.html",
    "title": "12 ANOVA",
    "section": "",
    "text": "Arrange the arithmetic to compare more than two groups",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "12 ANOVA"
    ]
  },
  {
    "objectID": "12-1-way-anova.html#analysis-of-variance-anova",
    "href": "12-1-way-anova.html#analysis-of-variance-anova",
    "title": "12 ANOVA",
    "section": "1 ANalysis Of VAriance (ANOVA)",
    "text": "1 ANalysis Of VAriance (ANOVA)\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this lesson, you will be able to:\n\nArticulate the question of 1-way ANOVA\nEvaluate data and assumptions for 1-way ANOVA\nGraph 1-way ANOVA data\nPerform tests and alternatives for 1-way ANOVA\n\n\n\n\nThe analysis of variance is not a mathematical theorem, but rather a convenient method of arranging the arithmetic. R. A. Fisher (via Wishart 1934. Sppl. J. Roy. Soc. 1(1):26-61.)\n\nPerhaps more so than any other tool, the Analysis of Variance (ANOVA) played a role in literally revolutionizing the idea of objectivity in using data to produce evidence to support claims for certain experimental designs. Invented by the famous statistician and biologist R. A. Fisher while he worked at Rothamsted Research, the intention was for ANOVA to be a useful tool to analyze agriculture experiments. Today, despite many innovations and competing approaches, it remains at the foundation of the basic practice of statistics.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "12 ANOVA"
    ]
  },
  {
    "objectID": "12-1-way-anova.html#the-question-of-1-way-anova",
    "href": "12-1-way-anova.html#the-question-of-1-way-anova",
    "title": "12 ANOVA",
    "section": "2 The question of 1-way ANOVA",
    "text": "2 The question of 1-way ANOVA\nThere are several reasons to use a “1-way ANOVA” experimental design. The scenario usually involves:\n\none numeric continuous dependent variable of interest\na factor that contains 2 or more levels, often with a control\nWhen there are just two levels, the 1-ANOVA is conceptually equivalent to the t-test\n\nAn example might be something like a classic field trial, where crop pest damage is measured (the numeric continuous dependent variable) and the factor compares pest treatment with 3 levels: a control level (no pesticide), an organic pesticide, and a chemical pesticide. The basic question is whether there is an overall difference in the numeric dependent variable amongst the factor levels, however several kinds of questions are also possible to answer:\n\noverall difference test of means between the factors\nComparison of difference of each factor level with the control or other reference factor level\npost hoc tests of difference between specific factor levels, e.g. pairwise tests for a factor with levels A, B, and C might test all possible comparisons A:B, A:C, and B:C.\nExamination of the “sources of variation” observed in the dependent variable, e.g. what proportion of total variation can be accounted for by the factor\n\n\nThe test statistic for ANOVA is the F ratio, which is proportion of variance in the dependent variable between the groups, relative to that within the categories. We will (very briefly) look at this calculation with the aim of gaining a practical understanding of what is going.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "12 ANOVA"
    ]
  },
  {
    "objectID": "12-1-way-anova.html#data-and-assumptions",
    "href": "12-1-way-anova.html#data-and-assumptions",
    "title": "12 ANOVA",
    "section": "3 Data and assumptions",
    "text": "3 Data and assumptions\nThe data we will look at is an experiment in animal genetics, looking at the weight of male chickens (8-week old weight in grams), where weight is the continuous dependent variable. The factor is the sire identity, where the measure young male chicks were sired by one of 5 sires, thus sire is a factor with 5 levels A, B, C, D, and E.\n\n3.1 Wide format data\nHere, the numeric data are stored in five vectors, each corresponding to one factor level. One row does not correspond to a single “case” because each column contains measures from different individual offspring. This is an unusual way to store data like this (these days), but we will look at this “wide format” first.\n\n# Try this\n# Data in \"wide format\" \nA &lt;- c(687, 691, 793, 675, 700, 753, 704, 717)\nB &lt;- c(618, 680, 592, 683, 631, 691, 694, 732)\nC &lt;- c(618, 687, 763, 747, 687, 737, 731, 603)\nD &lt;- c(600, 657, 669, 606, 718, 693, 669, 648)\nE &lt;- c(717, 658, 674, 611, 678, 788, 650, 690)\n\nhead(chicken.wide &lt;- data.frame(A, B, C, D, E))\n\n    A   B   C   D   E\n1 687 618 618 600 717\n2 691 680 687 657 658\n3 793 592 763 669 674\n4 675 683 747 606 611\n5 700 631 687 718 678\n6 753 691 737 693 788\n\n\n\n3.2 Long format data (preferred)\nHere, the numeric data is stored in a single vector, with a factor vector for the sire data. Each row corresponds to a single, independent “case”. This format is preferred and adheres to the “Tidy Data” standard, although it is not hard to move between wide and long formats.\n\n# Try this\n# Data in \"wide format\"  ####\nA &lt;- c(687, 691, 793, 675, 700, 753, 704, 717)\nB &lt;- c(618, 680, 592, 683, 631, 691, 694, 732)\nC &lt;- c(618, 687, 763, 747, 687, 737, 731, 603)\nD &lt;- c(600, 657, 669, 606, 718, 693, 669, 648)\nE &lt;- c(717, 658, 674, 611, 678, 788, 650, 690)\n\nhead(chicken.wide &lt;- data.frame(A, B, C, D, E))\n\n    A   B   C   D   E\n1 687 618 618 600 717\n2 691 680 687 657 658\n3 793 592 763 669 674\n4 675 683 747 606 611\n5 700 631 687 718 678\n6 753 691 737 693 788\n\n# Data in \"long format\"  ####\n# The hard way\nweight &lt;- c(A,B,C,D,E)\n\nsire &lt;- c(rep(\"A\", 8),\n          rep(\"B\", 8),\n          rep(\"C\", 8),\n          rep(\"D\", 8),\n          rep(\"E\", 8) )\n\nhead(data.frame(weight, sire))\n\n  weight sire\n1    687    A\n2    691    A\n3    793    A\n4    675    A\n5    700    A\n6    753    A\n\ntail(data.frame(weight, sire))\n\n   weight sire\n35    674    E\n36    611    E\n37    678    E\n38    788    E\n39    650    E\n40    690    E\n\n# The \"programm-ey\" way\nweight1 &lt;- c(A,B,C,D,E)\n\nsire1 &lt;- vector(mode = \"character\", length = 40)\nfor(i in 1:5) { sire1[(8*i-8)+c(1:8)] &lt;- rep(LETTERS[i], 8) }\n\nhead(data.frame(weight1, sire1))\n\n  weight1 sire1\n1     687     A\n2     691     A\n3     793     A\n4     675     A\n5     700     A\n6     753     A\n\ntail(data.frame(weight1, sire1))\n\n   weight1 sire1\n35     674     E\n36     611     E\n37     678     E\n38     788     E\n39     650     E\n40     690     E\n\n# With function from {tidyr}\nhead(chicken.wide) # From above\n\n    A   B   C   D   E\n1 687 618 618 600 717\n2 691 680 687 657 658\n3 793 592 763 669 674\n4 675 683 747 606 611\n5 700 631 687 718 678\n6 753 691 737 693 788\n\nlibrary(reshape2) # For melt()\n#?melt\nnew.long &lt;- melt(chicken.wide)\n\nNo id variables; using all as measure variables\n\nhead(new.long) # Not bad but note the variable names... \n\n  variable value\n1        A   687\n2        A   691\n3        A   793\n4        A   675\n5        A   700\n6        A   753\n\nnames(new.long)\n\n[1] \"variable\" \"value\"   \n\n# Flash challenge: change the variable names in new.long\n\nnames(new.long) &lt;- c('Sire', 'Weight')\nnames(new.long)\n\n[1] \"Sire\"   \"Weight\"\n\n# NB, you should probably just use long format for your data in the first place!\n\nR output\n&gt; head(new.long) # Not bad but note the variable names\n  variable value\n1        A   687\n2        A   691\n3        A   793\n4        A   675\n5        A   700\n6        A   753\n\n##4 Assumptions of ANOVA\nThe assumptions of ANOVA are similar to those of regression (indeed, both are a specific kind of Linear Model and share the assumptions of the Gaussian Linear Model). The most important to consider now are:\n\nGaussian residuals (we test graphically and with NHST for Gaussian residual distribution)\nHomoscedasticity (we test graphically with a residuals versus fitted values plot)\nEquality of variance (plot of residual versus factor and NHST for == variance)\nIndependent observations (we assume this for now with the chicken data, but will not test is formally)\n\n\n\n## **Assumptions** ####\n\n## - Gaussian residuals ####\n# Make the model object with aov()\n\n# ?aov\nm1 &lt;- aov(formula = Weight ~ Sire, \n         data = new.long)\n\n# Graph to examine Gaussian assumption of residuals\n# NB we use rstandard()\npar(mfrow = c(1,2))\nhist(rstandard(m1),\n     main = \"Gaussian?\")\n\n# Look at residuals with qqPlot()\nlibrary(car) # For qqPlot()\n\nLoading required package: carData\n\nqqPlot(x = m1,\n       main = \"Gaussian?\")\n\n\n\n\n\n\n\n[1] 24 38\n\npar(mfrow=c(1,1))\n\n\n4.1 Formal test of Gaussian residuals\nAt a glance, there are no serious issues with the assumption of Gaussian residual distribution. We can use NHST to help us decide; we will try the shapiro.test().\n\n\nshapiro.test(rstandard(m1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  rstandard(m1)\nW = 0.99182, p-value = 0.9913\n\n\n\n\nThere is no evidence of difference to Gaussian in our residuals for our ANOVA model (Shapiro-Wilk: W = 0.99, n = 40, P = 0.99).\n\n\n4.2 Homoscedasticity check\nWe will look at the residuals relative to the fitted values.\n\n\n# Plot for homoscedasticity check\nplot(formula = rstandard(m1) ~ fitted(m1),\n     ylab = \"m1: residuals\",\n     xlab = \"m1: fitted values\",\n     main = \"Spread similar across x?\")\nabline(h = 0,\n       lty = 2, lwd = 2, col = \"red\")\n\n# Make the mean residual y points (just to check)\ny1 &lt;- aggregate(rstandard(m1), by = list(new.long$Sire), FUN = mean)[,2]\n# Make the x unique fitted values (just to check)\nx1 &lt;- unique(round(fitted(m1), 6))\n\npoints(x = x1, y = y1, \n       pch = 16, cex = 1.2, col = \"blue\")\n\n\n\n\n\n\n\n\n4.3 Bartlett test\nFinally, we can use NHST just to have a final check of whether the variance in weight is equal between factor levels. There are several ways to do this; we will use the Bartlett test using bartlett.test(), which compares the variance for more than 2 groups.\n\n\n# NHST to examine  assumption of homoscedasticity\n# (homoscedasticiyy good, heteroscedasticity bad)\n\nbartlett.test(formula = weight~sire, data = new.long)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  weight by sire\nBartlett's K-squared = 1.6868, df = 4, p-value = 0.7931\n\n\n\n\nWe find no evidence that variance in offspring weight differs between sires (Bartlett test: K-sqared = 1.69, df = 4, P = 0.79).",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "12 ANOVA"
    ]
  },
  {
    "objectID": "12-1-way-anova.html#graphing-anova",
    "href": "12-1-way-anova.html#graphing-anova",
    "title": "12 ANOVA",
    "section": "5 Graphing ANOVA",
    "text": "5 Graphing ANOVA\nThe classic way to visualize the 1-way ANOVA is with boxplot, with some way to show the central tendency of the data separately for each factor level. For continuous variables, boxplots show this perfectly. For count variables, barplots are sometimes used with the height set to the mean, along with some form of error bar. Here we will use a boxplot.\n\n\n## basic boxplot ####\n\n# It always pays to make a nice plot\n\n# Do you think sire affects offspring weight?\nboxplot(Weight ~ Sire, data = new.long, \n        main = \"Is this plot good enough?\") \n\n\n\n\n\n\n\n\nSo, it looks like sire identity could have an effect on mean male offspring weight. But, is this graph good enough? Can we make it better? Let’s critique it:\n\n(important) The y axis does not indicate the unit of measurement\n(important) Neither axis title is capitalized\n(optional) Adding on the points might add interesting detail\n(optional) Reference line for control (we do not really have a control here) or of the “grand mean” might be useful\n\n\n5.1 Make a better graph\n\n## **Make a better graph** ####\n\nboxplot(Weight ~ Sire, data = new.long,\n        ylab = \"Weight (g)\",\n        xlab = \"Sire\",\n        main = \"Effect of Sire on 8-wk weight\",\n        cex = 0) # Get rid of the outlier dot (we will draw it back)\n\n# Make horizontal line for grand mean\nabline(h = mean(new.long$Weight), \n       lty = 2, lwd = 2, col = \"red\") # Mere vanity\n\n# Draw on raw data\nset.seed(42)\npoints(x = jitter(rep(1:5, each = 8), amount = .1),\n       y = new.long$Weight,\n       pch = 16, cex = .8, col = \"blue\") # Mere vanity",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "12 ANOVA"
    ]
  },
  {
    "objectID": "12-1-way-anova.html#anova-f-test-statistic-and-alternatives",
    "href": "12-1-way-anova.html#anova-f-test-statistic-and-alternatives",
    "title": "12 ANOVA",
    "section": "6 ANOVA F test statistic and alternatives",
    "text": "6 ANOVA F test statistic and alternatives\nThe basic application of ANOVA in R is the aov() function. There are actually a lot of alternative ways to perform the exact same test in R.\nTo micro-digress, ANOVA is a subset of the Gaussian linear model; the Gaussian linear model is a subset of the General Linear Model; and the General Linear Model is a subset of the GeneralIZED Linear Model. For now we will forget all of that and go with aov().\n\n6.1 perform the ANOVA**\nWe have a few things to do here:\n\nPerform 1-way ANOVA and look at the basic output for the overall effect of sire\nLook at how to examine contrasts (differences between the control or a reference factor level, and each of the others) and post hoc testing (e.g. all pairwise comparisons between factor levels)\nExamine what happens in the ANOVA in a little more detail\n\n\n6.2 ANOVA basic output\n\n## Perform 1-way ANOVA ####\n# Try this\n\n# NB if the factor is a character, it \"should\" be coerced to a factor\n# by R, \"the passive aggressive butler\"\n# If in doubt, explicitly make the vector class == factor()\nm1 &lt;- aov(formula = Weight ~ factor(Sire), \n          data = new.long)\n\nsummary(m1)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)\nfactor(Sire)  4  17426    4356   1.872  0.137\nResiduals    35  81442    2327               \n\n\n\nThe output is formatted in a classic “ANOVA Table” style. There are 2 rows - one for the “main effect” of the sire factor, one for the residual error. The test statistic is the F value (1.87), the P-value column is named “Pr(&gt;F)” (0.14), and there are 4 degrees of freedom for this test for the factor (the factor degrees of freedom is the number of factor levels minus 1 = 5 factor levels - 1 = 4; the residual degrees of freedom is the total number of observations minus the number of factor levels = 40 - 5 = 35).\n\nHere we can see that the overall effect of sire does not significantly explain variation in weight (1-way ANOVA: F = 1.87, df = 4,35, P = 0.14).\n\n\n6.3 Contrasts and post hoc test\nAn alternative to the ANOVA table format, and possibly different to the question of an overall mean effect of the factor, is the approach for a regular linear model looking at differences for each factor level relative to a reference factor level like a control. For our experiment, let’s say that sire C is our reference level sire, against which we would like to statistically compare offspring weight for other sires.\n\n\n# Use lm() and summary() to generate contrasts\n# Use relevel() to set sire C to the reference factor level\n\n# make Sire C the reference level\nnew.long$Sire &lt;- relevel(new.long$Sire, ref=\"C\")\n\n# calculate linear model\nm2 &lt;- lm(formula = Weight ~ Sire, \n         data = new.long)\n\nsummary(m2)\n\n\nCall:\nlm(formula = Weight ~ Sire, data = new.long)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-93.625 -29.312  -2.875  33.906 104.750 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   696.63      17.05  40.846   &lt;2e-16 ***\nSireA          18.38      24.12   0.762    0.451    \nSireB         -31.50      24.12  -1.306    0.200    \nSireD         -39.13      24.12  -1.622    0.114    \nSireE         -13.38      24.12  -0.555    0.583    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 48.24 on 35 degrees of freedom\nMultiple R-squared:  0.1763,    Adjusted R-squared:  0.08211 \nF-statistic: 1.872 on 4 and 35 DF,  p-value: 0.1373\n\nplot(Weight ~ Sire, \n     data = new.long)\n\n\n\n\n\n\n\nR output\n&gt; m2 &lt;- lm(formula = Weight ~ Sire, \n+          data = new.long)\n&gt; summary(m2)\n\nCall:\nlm(formula = Weight ~ Sire, data = new.long)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-93.625 -29.312  -2.875  33.906 104.750 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   696.63      17.05  40.846   &lt;2e-16 ***\nSireA          18.38      24.12   0.762    0.451    \nSireB         -31.50      24.12  -1.306    0.200    \nSireD         -39.13      24.12  -1.622    0.114    \nSireE         -13.38      24.12  -0.555    0.583    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 48.24 on 35 degrees of freedom\nMultiple R-squared:  0.1763,    Adjusted R-squared:  0.08211 \nF-statistic: 1.872 on 4 and 35 DF,  p-value: 0.1373\n\nNotice how the output format has changed. This is because the summary function (and lots of functions in fact) “behave differently” in response to the specific class() of object we pass to it (here an lm object; before an aov object). One big difference we see is the table of contrasts. Now, there is are 5 rows: one for the intercept coefficient (testing whether the “grand mean” of Weight is different to zero - NB this is not at all interesting for us), and for rows comparing each factor level to the reference mean for sire “C”.\nHere, the Estimate column is an estimate of the AMOUNT of difference in weight relative to the reference mean weight for that sire. E.g., sire B offspring weight is estimated at -31.5 grams (the negative indicate less than) compared to offspring weight for sire C. However, this observed sample difference is not statistically significant (P = 0.20).\nNotice the overall F value and p-value for the 1-way are also present at the bottom of the output, which is exactly the same as that produced by the aov() function.\n\nWe can make a new boxplot based on our new sire variable and notice how it automatically moves sire C to the leftmost “reference position”\nplot(Weight ~ Sire, \n     data = new.long,\n     main = \"Sire C as reference\")\n6.4 Post hoc tests\n\nThe collection and analysis of data should be driven by the question. We should always be careful to make the distinction between data analysis that SHOULD be done versus that which merely CAN be done. The former is driven by prediction and motivated by evidence, expectation and the design of data collection. The latter is often a waste of time, or worse, a “fishing expedition” in crass pursuit of a P-value, any P-value, which is &lt; 0.05 .\n\n\n\n\n\nThe meaning of significance\n\n\n\nPost hoc tests are often interesting in research, and the 1-way ANOVA is a good example, where an overall question of “is there a difference?” can be enhanced by asking whether there are particular or specific differences, say between pairs of means. The phrase post hoc implies that the sometimes these questions can be an afterthought. Thus, consideration should be given as to whether these specific questions NEED to be asked.\n\n6.5 Type I errors\nThe alpha (\\(\\alpha\\)) value, the value to which we compare our P-value, can be interpreted as the (maximum) probability we are willing to accept of being wrong if we conclude there is significance in our data. Traditionally, this alpha value is accepted to be 0.05, or a 5% chance of making a false positive error. When multiple tests are made on the same data, it increases the chance of discovering a false positive, by chance alone, to above 5%. Thus, there are methods that are used to avoid doing this, by adjusting the P-value to keep the overall likelihood of false positive error at 5%.\nThe Bonferroni adjustment see Bland and Altman 1995 is a baseline, conservative adjustment to the alpha value to avoid false positive errors. This might be typically applied to a 1-way ANOVA situation where specific, pairwise comparisons between means are required that are not covered by the overall test, or by the contrasts. E.g. in our chicken data, we know there is not a difference in offspring weight between sire C and all other sire offsring weights: C:A, C:B, C:D, C:E, but what if we really wanted to test the others? We could use the Bonferroni adjustment as implemented in the pairwise.t.test() function.\nThe Bonferroni adjustment simply divides the alpha expected value by the number of post hoc pairwise comparisons. NB the Bonferroni adjustment is conservative, and there are alternatives. The point here is just to illustrate how these tests function and, like with many things, more study will be required to round out foundational knowledge for post hoc testing procedures. The Bonferroni test is nice to know about and understand because it is easy to use, and manually calculate, and can be applied to any situation.\n\n\n# Try this:\n\n## Bonferroni ####\n\n# ?pairwise.t.test\n\n# there are a few p.adjust.methods\n# c(\"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\",\n#   \"fdr\", \"none\")\n# we will use \"bonferroni\"\npairwise.t.test(x = new.long$Weight, \n                g = new.long$Sire,\n                p.adjust.method = \"bonferroni\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  new.long$Weight and new.long$Sire \n\n  C    A    B    D   \nA 1.00 -    -    -   \nB 1.00 0.46 -    -   \nD 1.00 0.23 1.00 -   \nE 1.00 1.00 1.00 1.00\n\nP value adjustment method: bonferroni \n\n\n\nNB the output here is a matrix of (Bonferroni adjusted!) p-values for each possible pairwise comparison (none are significant, i.e., less than 0.05). It is also possible to simply calculate uncorrected p-values and compare them to Bonferroni adjust alpha values, as described above!\n\n6.6 Tukey HSD (Honestly Significant Differences)\nThe Tukey HSD test is ideal for 1-way ANOVA, and is less conservative than the Bonferroni adjustment. We use the Tukey.HSD() function to apply it here.\n\n## Tukey Honestly Significant Differences ####\n# ?TukeyHSD\n\nTukeyHSD(m1) # NB m1 - this function requires an \"aov\" object\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Weight ~ factor(Sire), data = new.long)\n\n$`factor(Sire)`\n       diff        lwr       upr     p adj\nB-A -49.875 -119.21883  19.46883 0.2565333\nC-A -18.375  -87.71883  50.96883 0.9397600\nD-A -57.500 -126.84383  11.84383 0.1436866\nE-A -31.750 -101.09383  37.59383 0.6830523\nC-B  31.500  -37.84383 100.84383 0.6893101\nD-B  -7.625  -76.96883  61.71883 0.9977281\nE-B  18.125  -51.21883  87.46883 0.9425341\nD-C -39.125 -108.46883  30.21883 0.4937665\nE-C -13.375  -82.71883  55.96883 0.9806452\nE-D  25.750  -43.59383  95.09383 0.8216369\n\nplot(TukeyHSD(m1))\n\n\n\n\n\n\n\n\nNotice the table of pairwise comparisons. The format is slightly different than that for the simple pairwise.t.test() function output and there is a bit more information. Also note that the p-values tend to be smaller for the exact same comparisons, but there are sill no significant comparisons.\n\n6.7 Alternatives to 1-way ANOVA\nIn case the assumptions of 1-way ANOVA cannot be met by the data, there are a few options:\n\nAttempt to transform the data (e.g. with log(), sqrt() or other transformation) to “coerce” the data to conform to the assumptions of 1-way ANOVA\nUse an alternative test for which assumptions are not violated\n\n\nThe simplest alternative test to use for a 1-way ANOVA design would be a “non-parameteric” test that simply does not make the assumptions of Gaussian residuals or of homscedasticity. NB there are other many other methods as well (e.g. the Generalized linear model, randomization, Bayesian modelling), which we will not cover here.\nNon-parametric tests have the advantage of being very easy to use, and being very easy to interpret as they tend to be analogous to tests that require assumptions of the data (so-called “parametric tests” like the t-test, regression and 1-way ANOVA).\nA downside to these non-parametric tests is that they tend to have less statistical power. That is, they are less likely than their parametric cousins to detect a significant difference even if one does exist!\n\n6.8 Kruskal-Wallis non-parametric alternative to the 1-way ANOVA\nHere we will look at a non-parametric test that is perfect to use instead of, the Kruskal-Wallis test.\n\n# Try this:\n# ?kruskal.test\nkruskal.test(formula = Weight ~ Sire,\n             data = new.long)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Weight by Sire\nKruskal-Wallis chi-squared = 7.648, df = 4, p-value = 0.1054\n\n\n\nThe result is qualitatively the same as that for our 1-way ANOVA test, which is not surprising. Reporting the results of this test is similar as well.\n\n\nWe found no evidence of a difference in offspring weight for different sires (Kruskal-Wallis: chi-squared = 7.65, df = 4, P = 0.11).",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "12 ANOVA"
    ]
  },
  {
    "objectID": "12-1-way-anova.html#anova-calculation-details",
    "href": "12-1-way-anova.html#anova-calculation-details",
    "title": "12 ANOVA",
    "section": "7 ANOVA calculation details",
    "text": "7 ANOVA calculation details\n# Our data\nchicken.wide\n\n7.1 ANOVA Equations\n{width = “600px”}\n\n7.2 ANOVA Variables\n{width = “400px”}\n\n7.3 ANOVA Sources of variation table\n\n\n\nI like the cocksure argument that you never need to actually “do an ANOVA by hand” because we have computers for that sort of thing these days. Which, we do. But, I think I would make a distinction between someone who has never, or fears they cannot, calculate an ANOVA, versus someone who might occasionally do so just to understand better how the world works. I know which one I would rather be.\n\n\n- Unknown philosopher\n\n\n7.4 Do an ANOVA “by hand” programmatically\n\n## ANOVA details ####\n\n# Try this:\n# For the code below, try to follow what is going on in the code\n# It is okay if not every detail is clear yet\n# Do we get the same answer as aov()?\n\nn.groups &lt;- ncol(chicken.wide)\nn.per.group &lt;- vector(mode = \"integer\", length = ncol(chicken.wide))\n\nfor(i in 1:ncol(chicken.wide)) {n.per.group[i] &lt;- length(chicken.wide[,i])}\n\nn.individuals &lt;- ncol(chicken.wide)*nrow(chicken.wide)\ndf.between &lt;- n.groups - 1\ndf.within &lt;- n.individuals - n.groups\nmean.total &lt;- mean(as.matrix(chicken.wide))\nmean.per.group &lt;- colMeans(chicken.wide)\n\nss.between &lt;- sum(n.per.group*(mean.per.group - mean.total)^2)\n\nss.within &lt;- sum(sum((chicken.wide[,1] - mean.per.group[1])^2),\n                 sum((chicken.wide[,2] - mean.per.group[2])^2),\n                 sum((chicken.wide[,3] - mean.per.group[3])^2),\n                 sum((chicken.wide[,4] - mean.per.group[4])^2),\n                 sum((chicken.wide[,5] - mean.per.group[5])^2)\n                 )\nms.between &lt;- ss.between/df.between\nms.within &lt;- ss.within/df.within\n\n# Manual F\n(myF &lt;- ms.between/ms.within)\n\n[1] 1.872189\n\n# Anova table\nanova(m1)$\"F value\"[1] # Store-bought F\n\n[1] 1.872189",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "12 ANOVA"
    ]
  },
  {
    "objectID": "12-1-way-anova.html#practice-exercises",
    "href": "12-1-way-anova.html#practice-exercises",
    "title": "12 ANOVA",
    "section": "8 Practice exercises",
    "text": "8 Practice exercises\nFor the following exercises, use the dataset in the file 1.5-OrchardSprays.csv. The dataset is in tidy format. Take advantage of this by looking at the terse information present in the data dictionary tab. The data are from an experiment testing the effectiveness of different pesticide treatments on red lily beetle damage to lily leaves. There will be some data handling as part of the exercises below, a practical and important part of every real data analysis.\n\n8.1 Loading and Exploring the Pesticide Dataset\nRead in the data from the file 1.5-OrchardSprays.csv. Use the read.csv() function. Print the first few rows of the data, and check the structure of the data. Show your code.\n\n\n\n\n\n\n\nSolution: Loading and Exploring Data\n\n\n\n\n\n\n# Read in the data from the file 1.5-OrchardSprays.csv\npest &lt;- read.csv(\"1.5-OrchardSprays.csv\")\n\n# Print the first few rows of the data\nhead(pest)\n\n# Check the structure of the data\nstr(pest)\n\n\n\n\n\n8.2 Testing Normality of Residuals\nTest the assumption of Gaussian residuals for 1-way ANOVA using any graphs or NHST approach that you deem appropriate. Show your code and briefly describe your EDA findings and conclusion as to whether these data adhere to the Gaussian assumption.\n\n\n\n\n\n\n\nSolution: Testing Normality of Residuals\n\n\n\n\n\n\n# Create the ANOVA model\npest_model &lt;- aov(damage ~ treatment, data = pest)\n\n# Extract residuals\npest_residuals &lt;- residuals(pest_model)\n\n# Graphical assessment\npar(mfrow = c(1, 2))\n\n# Histogram of residuals\nhist(pest_residuals, \n     main = \"Histogram of Residuals\",\n     xlab = \"Residuals\",\n     breaks = 10,\n     col = \"lightblue\")\n\n# Q-Q plot\nqqnorm(pest_residuals, \n       main = \"Normal Q-Q Plot of Residuals\")\nqqline(pest_residuals, col = \"red\", lwd = 2)\n\npar(mfrow = c(1, 1))\n\n# Formal test for normality\nshapiro_test &lt;- shapiro.test(pest_residuals)\nshapiro_test\n\n\n\n\n\n8.3 Testing Homoscedasticity\nTest the assumption of homoscedasticity of residuals for 1-way ANOVA using any graphs or NHST approach that you deem appropriate. Show your code and briefly describe your EDA findings and conclusion as to whether these data adhere to the homoscedasticity assumption.\n\n\n\n\n\n\n\nSolution: Testing Homoscedasticity\n\n\n\n\n\n\n# Create the ANOVA model\npest_model &lt;- aov(damage ~ treatment, data = pest)\n\n# Graphical assessment of homoscedasticity\n# Plot residuals vs. fitted values\nplot(fitted(pest_model), residuals(pest_model),\n     main = \"Residuals vs. Fitted Values\",\n     xlab = \"Fitted Values\",\n     ylab = \"Residuals\",\n     pch = 19)\nabline(h = 0, lty = 2, col = \"red\")\n\n# Plot residuals by treatment group\nboxplot(residuals(pest_model) ~ pest$treatment,\n        main = \"Residuals by Treatment\",\n        xlab = \"Treatment\",\n        ylab = \"Residuals\")\nabline(h = 0, lty = 2, col = \"red\")\n\n# Formal test for homogeneity of variances\nbartlett_test &lt;- bartlett.test(damage ~ treatment, data = pest)\nbartlett_test\n\n\n\n\n\n8.4 Performing One-way ANOVA\nPerform either a 1-way ANOVA or an appropriate alternative based on your findings in the previous answers. Show your code, state your results in the technical style and briefly interpret your findings.\n\n\n\n\n\n\n\nSolution: Performing One-way ANOVA\n\n\n\n\n\n\n# Perform one-way ANOVA\npest_anova &lt;- aov(damage ~ treatment, data = pest)\nsummary(pest_anova)\n\n# Calculate group means for interpretation\ntreatment_means &lt;- tapply(pest$damage, pest$treatment, mean)\ntreatment_means\n\n# Calculate proportion of variance explained (effect size)\npest_anova_summary &lt;- summary(pest_anova)\nSS_treatment &lt;- pest_anova_summary[[1]][1, \"Sum Sq\"]\nSS_total &lt;- sum(pest_anova_summary[[1]][, \"Sum Sq\"])\neta_squared &lt;- SS_treatment / SS_total\neta_squared\n\n\n\n\n\n8.5 Post-hoc Analysis of Treatment Effects\nPerform an appropriate set of post hoc tests to compare pairwise mean differences in these data. Focus on the post hoc questions of interest: Is the organic pesticide effective? Does dose matter in the non-organic treatments?\n\n\n\n\n\n\n\nSolution: Post-hoc Analysis\n\n\n\n\n\n\n# Perform Tukey HSD test for all pairwise comparisons\ntukey_result &lt;- TukeyHSD(aov(damage ~ treatment, data = pest))\ntukey_result\n\n# Visualize the Tukey results\nplot(tukey_result, las = 1)\n\n# For the specific questions of interest:\n# 1. Is organic pesticide effective? (compare organic vs control)\n# 2. Does dose matter? (compare x.half vs x.full)\n\n# Extract specific comparisons\norganic_vs_control &lt;- tukey_result$treatment[\"organic-control\", ]\nhalf_vs_full &lt;- tukey_result$treatment[\"x.half-x.full\", ]\n\n# Display these specific comparisons\norganic_vs_control\nhalf_vs_full\n\n# Create a visual summary of all treatments\nboxplot(damage ~ treatment, data = pest,\n        main = \"Treatment Effects with Significance Groups\",\n        xlab = \"Treatment Type\",\n        ylab = \"Leaf Damage (mm²)\",\n        col = c(\"lightgray\", \"lightgreen\", \"lightblue\", \"lightyellow\"))\n\n# Add letters to indicate significance groups\ntext(x = 1:4, \n     y = tapply(pest$damage, pest$treatment, max) + 5,\n     labels = c(\"A\", \"B\", \"C\", \"A\"),  # Based on Tukey results\n     font = 2)\n\n# Add a legend explaining the letters\nlegend(\"topright\", \n       legend = \"Groups with different letters are significantly different (p &lt; 0.05)\",\n       cex = 0.8, bty = \"n\")\n\n\n\n\n\n8.6 Create Your Own ANOVA Question\nWrite a plausible practice question involving any aspect of data handling, graphing or analysis for the 1-way ANOVA framework for the iris data (data(iris); help(iris)).\n\n\n\n\n\n\n\nSolution: Creating an ANOVA Question\n\n\n\n\n\n\n# A plausible practice question could be:\n# \"Using the iris dataset, perform a one-way ANOVA to determine if there are \n# significant differences in petal length among the three species. If significant \n# differences exist, conduct post-hoc tests to identify which species differ from \n# each other. Create an appropriate visualization to display these differences.\"\n\n# Solution:\n# Load the iris dataset\ndata(iris)\n\n# Examine the data structure\nstr(iris)\n\n# Create a boxplot to visualize the differences\nboxplot(Petal.Length ~ Species, data = iris,\n        main = \"Petal Length Comparison Across Iris Species\",\n        xlab = \"Species\",\n        ylab = \"Petal Length (cm)\",\n        col = c(\"lightpink\", \"lightblue\", \"lightgreen\"))\n\n# Perform one-way ANOVA\niris_anova &lt;- aov(Petal.Length ~ Species, data = iris)\nsummary(iris_anova)\n\n# Check assumptions\n# Normality of residuals\nshapiro.test(residuals(iris_anova))\n\n# Homogeneity of variances\nbartlett.test(Petal.Length ~ Species, data = iris)\n\n# Perform post-hoc test (Tukey's HSD)\ntukey_result &lt;- TukeyHSD(iris_anova)\ntukey_result\n\n# Visualize the Tukey results\nplot(tukey_result)\n\n# Add mean values to the boxplot\nspecies_means &lt;- tapply(iris$Petal.Length, iris$Species, mean)\ntext(x = 1:3, \n     y = species_means - 0.3, \n     labels = round(species_means, 2),\n     col = \"red\",\n     font = 2)\n\n# Add significance indicators\ntext(x = 1:3, \n     y = tapply(iris$Petal.Length, iris$Species, max) + 0.2,\n     labels = c(\"a\", \"b\", \"c\"),  # All species differ significantly\n     font = 2)",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "12 ANOVA"
    ]
  },
  {
    "objectID": "04-data.html",
    "href": "04-data.html",
    "title": "04 Data objects",
    "section": "",
    "text": "Data objects in ‘R’ space",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "04 Data objects"
    ]
  },
  {
    "objectID": "04-data.html#data-objects-in-r",
    "href": "04-data.html#data-objects-in-r",
    "title": "04 Data objects",
    "section": "1 Data objects in R",
    "text": "1 Data objects in R\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this lesson, you will be able to:\n\nDescribe basic data types\nUse str() to inspect data objects\nDescribe data with factors\nUse class() and convert variable types\nUse vector and matrix functions to manipulate data\n\n\n\n\n\nImagine you are floating in space in the R Global Environment and any data object you can see, you can call on by name to manipulate with functions. Let’s call it R Space\n\nThe fundamental way to analyse data in R is to be able to manipulate it with code. In order to do that, we need a system of containers to store the data in. This page is about the rules used for storing data in data objects. We will examine basic data types at the same time as how R stores them. R is actually very flexible in the way it handles data, making it as easy as possible for non-programmers to get going.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "04 Data objects"
    ]
  },
  {
    "objectID": "04-data.html#basic-data-types-str",
    "href": "04-data.html#basic-data-types-str",
    "title": "04 Data objects",
    "section": "2 Basic data types, str()\n",
    "text": "2 Basic data types, str()\n\n2.1 Storage containers types\nThe first things to examine are the way that R variables are named, and the organization system for storing the data. The organization part is particularly important, because it is used to actually access and use data.\nVariable names in R are simple and do not need to be “declared” like in some computing languages, and they can be almost anything, but there are a few rules. The R system has a built-in error message and warning message system (also known as the Passive-Aggressive Butler), which will usually give a hint when some of these rules are violated.\n\n2.2 The Global Environment\nOne of the things we notice when people begin using R, even if they are experienced in data analysis, is that they expect to “see” data and data sets, almost as they are physical things. This might be because of experience using Excel and seeing the visual representation of data in spreadsheets (indeed, a graphical representation of physical spreadsheets!).\nThe regular R system for interacting with data is a little more abstract which can be disconcerting to new users. Typical use is to create variables in code script files and, usually, the bring data into the Global Environment from external files on the local PC, or from the web. We will practice using the Global Environment is the main way to interact with data.\nYou can use the class() function to find out the variable type (using this this is a good idea, since R occasionally guesses the intended data type incorrectly).\n# Try this\n\nvariable_1 &lt;- c(4,5,7,6,5,4,5,6,7,10,3,4,5,6) # a numeric vector\n\nvariable_2 &lt;- c(TRUE, TRUE, TRUE, FALSE) # a logical vector\n\nvariable_3 &lt;- c(\"Peter Parker\", \"Bruce Wayne\", \"Groo the Wanderer\") # a character vector\n\nclass(variable_1) # \"numeric\"\nclass(variable_2) # \"logical\"\nclass(variable_3) # \"character\"\n\nLook at the upper right pane of your RStudio and you should see something like:\n\n\n\nRStudio Global Environment\n\n\n\nThe Environment tab contains the Global Environment (labelled A in the picture above). There are some other tabs, but we will ignore these for now. The Global Environment itself contains information about the variables that are held in memory. If we think of this as “R Space”, a general rule is that if you can see a variable here in the Global Environment, you can manipulate it and work with it.\nNotice that there is quite a lot of information in the Global Environment about the actual variables (B in the picture). There is a column with the variable NAME (variable_1, variable_2, etc.), a column with the variable TYPES (num, logi, etc.), a column with the variable dimensions ([1:14] is an index like a unique “street address” for each of the 14 numeric values contained in “variable_1”)\n\n2.3 Naming conventions for variables\nVariable names:\n\nCan contain letters, numbers, some symbolic characters\nMust begin with a letter\nMust not contain spaces\nSome forbidden characters like math operators, “@”, and a few others\nShould be human-readable, consistent, and not too long\nCase sensitive\n\n# Try this\n\n## Variable name rules ####\n\n# Can contain letters, numbers, some symbolic characters\nx1 &lt;- 5  # OK\n\nx2 &lt;- \"It was a dark and stormy night\" # OK\n\nmy_variable_9283467 &lt;- 1 # technically works, but hard to read\n\n# Must begin with a letter \n\nvarieties &lt;- c(\"red delicious\", \"granny smith\") # OK\n\nx432 &lt;- c(\"a\", \"b\") # OK\n\n22catch &lt;- c(TRUE, TRUE, FALSE)  # nope\n\n# Must not contain spaces\nmy_variable &lt;- 3 # OK\n\nmy.variable &lt;- 4 # OK\n\nmyVariable &lt;- 5 # OK\n\nmy variable &lt;- 6 # nope\n\n\"my variable\" &lt;- 7 # nope\n\n# Must not contain forbidden characters like \n# math operators, \"@\", and a few others\nmy@var &lt;- 1 # nope\n\nmy-var &lt;- 1 # nope\n\nmy=var &lt;- 1 # nope\n\n# etc.\n\n# Should be human-readable, consistent, and not too long\n\nDiameter_Breast_Height_cm &lt;- c(22, 24, 29, 55, 43) # legal but too long\n\nDBH_cm &lt;- c(22, 24, 29, 55, 43) # much better\n\n#Case sensitive\nheight &lt;- c(180, 164, 177) # OK\n\nHeight # Error: object 'Height' not found (notice capital H)\n\nheight # OK",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "04 Data objects"
    ]
  },
  {
    "objectID": "04-data.html#data-with-factor",
    "href": "04-data.html#data-with-factor",
    "title": "04 Data objects",
    "section": "3 Data with factor()\n",
    "text": "3 Data with factor()\n\nSometimes you will need to analyze data that is a factor, where the different values are categories. Factors in R can be a cause for confusion, but there needn’t be problems if you understand them. The information here is a starting point, and we skip some complexities, but essentially\n3.1 Two types of factors, non-ordered and ordinal\nNon-ordered factors are simply categories and the levels are simply the names of the categories. By definition, non-ordered factor do not have a specific order! an example here might be plant varieties.\nOrdered factors have a specific order, which can be important for analysis or for graphing. You usually have to specify the order explicitly to get this right. An example here might be the days of the week, where the order of day is important.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "04 Data objects"
    ]
  },
  {
    "objectID": "04-data.html#class-and-converting-variables",
    "href": "04-data.html#class-and-converting-variables",
    "title": "04 Data objects",
    "section": "4 Class() and converting variables",
    "text": "4 Class() and converting variables\nWe use the class() function to query what data category a variable is set too. R is pretty good at setting this correctly, but it is a good idea to check sometimes and occasionally you will have to manually set variable characteristics.\n\n# try this\n\n# non-ordered factor\nvariety &lt;- c(\"short\", \"short\", \"short\",\n             \"early\", \"early\", \"early\",\n             \"hybrid\", \"hybrid\", \"hybrid\")\nclass(variety)  # \"character\", but this is really a factor...\nvariety # Notice the character strings are just printed out\n\nvariety &lt;- factor(variety) # use factor() to convert the character vector to a factor\nclass(variety)  # now variety is a \"factor\"\nvariety # notice the output has changed\n\nNow, try some code manipulating ordered factors.\n# Ordered factors\nday &lt;- c(\"Monday\", \"Monday\", \n         \"Tuesday\", \"Tuesday\", \n         \"Wednesday\", \"Wednesday\", \n         \"Thursday\")\nclass(day) # character\n\n#make day a factor\nday &lt;- factor(day)\nclass(day)\nday # Notice the Levels: Monday Thursday Tuesday Wednesday\n\n# To set the order explicitly we need to set them explicitly\nhelp(factor) # notice the levels argument - it sets the order of the level names\n\nday &lt;- factor(x = day, levels = c(\"Monday\", \"Tuesday\",\n                         \"Wednesday\", \"Thursday\"))\nday # Notice the level order now",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "04 Data objects"
    ]
  },
  {
    "objectID": "04-data.html#vector-and-matrix-fun",
    "href": "04-data.html#vector-and-matrix-fun",
    "title": "04 Data objects",
    "section": "5 Vector and Matrix fun",
    "text": "5 Vector and Matrix fun\nVector and matrix data structures are two fundamental ways to arrange data in R. We have already looked at vectors, which store data in a single dimension.\nThere are actually a few different data organisation structures in R.\n\nVector - stores data in a single dimension from 1 to i, my_vec[i]\nMatrix - stores data in two dimensions 1 to i rows, 1 to j columns my_mat[i, j]\nArray - Three (or more) dimensions from 1 to to i, j, and k, my_array[i, j, k]\n\nVectors, Matrices and Arrays can only store the same TYPE of data.\n\n5.1 Vector practice\n# Try this\n\nmyvec1 &lt;- c(1,2,3,4,5) # numeric vector\nmyvec1\nclass(myvec1) # see? I told ya!\n\nmyvec2 &lt;- as.character(myvec1) #convert to character\nmyvec2 # notice the quotes\nclass(myvec2) # now character\n\nmyvec3 &lt;- c(2, 3, \"male\")\nmyvec3 #notice the numbers now have quotes - forced to character...\n\nmyvec4 &lt;- as.numeric(myvec3) #notice the warning\nmyvec4 # The vector element that could not be coerced to be a numeric was converted to NA\n\n5.2 Matrices\nMatrices can be quite useful - you can manipulate data into matrix form with the matrix() function. By default rows and columns are merely numbered, but they can be named as well.\n\n# Try this\n\nvec1 &lt;- 1:16 # make a numeric vector with 16 elements\nvec1 \n\nhelp(matrix) #notice the ncol, nrow and byrow arguments\n\nmat1 &lt;- matrix(data = vec1, ncol = 4, byrow = FALSE) #byrow = FALSE is the default\n\nmat1 # Notice the numbers filled in by columns\ncolnames(mat1) # The Columns and Rows have no names\n\ncolnames(mat1) &lt;- c(\"A\", \"B\", \"C\", \"D\") # Set the column names for mat1\ncolnames(mat1)\n\nmat1 # Yep the columns shows names\n\n5.3 Matrix challenge\n# Challenge 1: Set the Row names for Mat1 using the rownames() function\n\n# Challenge 2: make a matrix with 3 rows with the following vector, \n# so the the first COLUMN contains the numbers 2, 5, and 9, in the order,\n# for rows 1, 2, and 3 respectively:\n\nvec2 &lt;- c(2,3,5,4,5,6,7,8,9,5,3,1)",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "04 Data objects"
    ]
  },
  {
    "objectID": "04-data.html#practice-exercises",
    "href": "04-data.html#practice-exercises",
    "title": "04 Data objects",
    "section": "6 Practice exercises",
    "text": "6 Practice exercises\n\n6.1 Vector Recycling\nCreate a vector named my_var1 that contains the following 6 integers: 3, 6, 12, 7, 5, 1. Create a Second vector called my_var2 that contains the following 2 integers: 2, 3. Evaluate the expression my_var1 + my_var 2. Explain the output in terms of R mechanics in your own words.\n\n\n\n\n\n\n\nVector Addition with Recycling\n\n\n\n\n\n\n# Create the first vector with 6 integers\nmy_var1 &lt;- c(3, 6, 12, 7, 5, 1)\n\n# Create the second vector with 2 integers\nmy_var2 &lt;- c(2, 3)\n\n# Evaluate the expression\nmy_var1 + my_var2\n\n[1]  5  9 14 10  7  4\n\n\nWhen adding vectors of different lengths in R, the shorter vector is “recycled” (repeated) to match the length of the longer vector. In this case:\n\n\nmy_var1 has 6 elements: 3, 6, 12, 7, 5, 1\n\nmy_var2 has 2 elements: 2, 3\nWhen adding them, my_var2 is recycled to become effectively: 2, 3, 2, 3, 2, 3\nThe addition is then performed element-wise: 3+2=5, 6+3=9, 12+2=14, 7+3=10, 5+2=7, 1+3=4\n\nThis recycling behavior is a fundamental aspect of R’s vector operations, allowing operations between vectors of different lengths.\n\n\n\n\n6.2 Ordered Factor\nCreate a character vector with the names of the 12 months of the year. Convert the vector to a factor, with the month names in chronological order. Show your code.\n\n\n\n\n\n\n\nMonths as Ordered Factor\n\n\n\n\n\n\n# Create a character vector with month names\nmonths_vec &lt;- c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \n                \"July\", \"August\", \"September\", \"October\", \"November\", \"December\")\n\n# Check the class\nclass(months_vec)\n\n[1] \"character\"\n\n# Convert to factor with correct chronological order\nmonths_factor &lt;- factor(months_vec, levels = months_vec)\n\n# Verify the result\nclass(months_factor)\n\n[1] \"factor\"\n\nmonths_factor\n\n [1] January   February  March     April     May       June      July     \n [8] August    September October   November  December \n12 Levels: January February March April May June July August ... December\n\n\nThis code creates a character vector containing the 12 month names, then converts it to a factor with the levels explicitly set to maintain the chronological order.\nWithout specifying the levels, R would arrange the factor levels alphabetically (April, August, December, etc.), which wouldn’t represent the natural chronological order of months.\n\n\n\n\n6.3 Matrix Data Types\nWhat is wrong with the following code? Describe, show the code, and justify a fix for the problem in your own words. mymat &lt;- matrix(data = c( 12, 23, 45, “34”, “22”, “31”))\n\n\n\n\n\n\n\nMatrix Data Type Issue\n\n\n\n\n\n\n# The problematic code\nmymat &lt;- matrix(data = c(12, 23, 45, \"34\", \"22\", \"31\"))\nmymat\n\n     [,1]\n[1,] \"12\"\n[2,] \"23\"\n[3,] \"45\"\n[4,] \"34\"\n[5,] \"22\"\n[6,] \"31\"\n\n# Check the class of the matrix elements\nclass(mymat[1,1])\n\n[1] \"character\"\n\n\nThe problem with this code is that matrices in R can only contain one data type. When mixing numeric values (12, 23, 45) with character values (“34”, “22”, “31”), R coerces all elements to the most flexible type - character.\nThis means all numbers are converted to character strings, which prevents numerical operations on the matrix.\nTo fix this, we need to decide what type the data should be:\n\n# Fix 1: If all values should be numeric\nnumeric_mat &lt;- matrix(data = c(12, 23, 45, 34, 22, 31), ncol = 3)\nnumeric_mat\n\n     [,1] [,2] [,3]\n[1,]   12   45   22\n[2,]   23   34   31\n\nclass(numeric_mat[1,1])\n\n[1] \"numeric\"\n\n# Fix 2: If all values should be character (for non-numeric purposes)\nchar_mat &lt;- matrix(data = c(\"12\", \"23\", \"45\", \"34\", \"22\", \"31\"), ncol = 3)\nchar_mat\n\n     [,1] [,2] [,3]\n[1,] \"12\" \"45\" \"22\"\n[2,] \"23\" \"34\" \"31\"\n\nclass(char_mat[1,1])\n\n[1] \"character\"\n\n\nThe appropriate fix depends on how you intend to use the matrix data.\n\n\n\n\n6.4 3D Array\nUse the array() function to make a 2 x 2 x 3 array to produce the following output: , , 1\n [,1] [,2]\n[1,] 1 3 [2,] 2 4\n, , 2\n [,1] [,2]\n[1,] 5 7 [2,] 6 8\n, , 3\n [,1] [,2]\n[1,] 9 11 [2,] 10 12\n\n\n\n\n\n\n\n3D Array Creation\n\n\n\n\n\n\n# Create a vector with all the values\ndata_vec &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)\n\n# Create the 2x2x3 array\nmy_array &lt;- array(data = data_vec, dim = c(2, 2, 3))\n\n# Display the array\nmy_array\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n, , 3\n\n     [,1] [,2]\n[1,]    9   11\n[2,]   10   12\n\n\nTo create this 3D array:\n\nFirst, we create a vector with all 12 values in the correct order\nThen we use the array() function with:\n\n\ndata parameter set to our vector\n\ndim parameter set to c(2, 2, 3), which specifies:\n\n2 rows\n2 columns\n3 “layers” or “slices”\n\n\n\n\n\nThe array fills by columns first, then rows, then layers. This creates the exact structure shown in the expected output.\n\n\n\n\n6.5 Named Matrix\nShow the code to make the following matrix: cat dog male 22 88 female 71 29\n\n\n\n\n\n\n\nNamed Matrix Creation\n\n\n\n\n\n\n# Create the data vector\ndata_vec &lt;- c(22, 71, 88, 29)\n\n# Create the matrix, filling by column\npet_matrix &lt;- matrix(data = data_vec, nrow = 2, ncol = 2, byrow = FALSE)\n\n# Add row and column names\nrownames(pet_matrix) &lt;- c(\"male\", \"female\")\ncolnames(pet_matrix) &lt;- c(\"cat\", \"dog\")\n\n# Display the matrix\npet_matrix\n\n       cat dog\nmale    22  88\nfemale  71  29\n\n\nTo create this matrix:\n\nWe create a vector with the four values in the correct order for column-wise filling\nWe create a 2x2 matrix using matrix() with byrow = FALSE (the default)\nWe assign row names “male” and “female”\nWe assign column names “cat” and “dog”\n\nThe resulting matrix matches the requested format, showing counts for male/female cats/dogs.\n\n\n\n\n6.6 Matrix Question\nWrite a plausible practice question involving the use of the matrix() and vector() functions.\n\n\n\n\n\n\n\nMatrix and Vector Question\n\n\n\n\n\n\n# A plausible practice question could be:\n\n# \"Create a 3x3 matrix containing the numbers 1 through 9, filled by row. \n# Then extract the diagonal elements of this matrix into a vector. \n# Finally, calculate the mean of these diagonal elements.\"\n\n# Solution:\n# Create the matrix filled by row\nmy_matrix &lt;- matrix(1:9, nrow = 3, ncol = 3, byrow = TRUE)\nmy_matrix\n\n# Extract the diagonal elements\ndiag_elements &lt;- diag(my_matrix)\ndiag_elements\n\n# Calculate the mean of the diagonal elements\nmean(diag_elements)\n\n\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\n[1] 1 5 9\n\n\n[1] 5\n\n\nThis question tests understanding of: - Creating matrices with specific filling patterns - Extracting specific elements from matrices - Using the diagonal function - Performing calculations on vectors extracted from matrices\nThe diagonal elements are 1, 5, and 9, and their mean is 5.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "04 Data objects"
    ]
  },
  {
    "objectID": "slides/topics/02_r_language.html",
    "href": "slides/topics/02_r_language.html",
    "title": "R Language",
    "section": "",
    "text": "Navigate example scripts, comments, help, and pseudocode\nUse math operators effectively in R\nApply logical Boolean operators for data manipulation\nDistinguish between base R and Tidyverse approaches\n\n\n\n\n\nR is popular statistical programming language and open source software\nDesigned to help non-programmers perform statistical analyses\nSometimes requires careful language to avoid unexpected outcomes\nEssential to view scripts as documentation of work\n\n\n\n\n\nWrite scripts as documentation for respected colleagues\nWork through instructions using R and RStudio actively\nType your own code rather than copy-pasting\nDocument all code with clear, concise comments\n\n\n\n\n\nUse help() function with tool name in brackets\nHelp pages organized consistently across all functions\nFunction name, description, usage, and argument definitions\nEssential to understand for effective R use\n\n\n\n\n\nBasic arithmetic: +, -, *, /, ^ (power)\nOrder of operations follows standard mathematical rules\nUse parentheses to control calculation order\nSpacing improves readability (use single spaces)\n\n\n\n\n\nBoolean expressions resolve to TRUE or FALSE\nCommon operators: &gt;, &lt;, ==, !=, &, |, !\nUsed for selecting groups of data effectively\nPowerful alternative to creating different datasets\n\n\n\n\n\nUse Boolean expressions to select data subsets\nSquare brackets act as index for data vectors\nCombine with logical operators for complex selections\nEssential skill for data manipulation\n\n\n\n\n\nBase R: Traditional, powerful form of R language\nTidyverse: Alternative approach with different conventions\nBoth have benefits and philosophical differences\nBootcamp focuses exclusively on base R first\n\n\n\n\n\nHeader: Contains name, date, and purpose\nContents: Roadmap listing all major sections\nBoth sections essential for every R script\nFoundation for reproducible research practices\n\n\n\n\n\nWorking with Boolean expressions and operators\nUnderstanding help system navigation\nWriting expressions with proper spacing\nCreating and documenting script sections"
  },
  {
    "objectID": "slides/topics/02_r_language.html#learning-objectives",
    "href": "slides/topics/02_r_language.html#learning-objectives",
    "title": "R Language",
    "section": "",
    "text": "Navigate example scripts, comments, help, and pseudocode\nUse math operators effectively in R\nApply logical Boolean operators for data manipulation\nDistinguish between base R and Tidyverse approaches"
  },
  {
    "objectID": "slides/topics/02_r_language.html#r-as-a-passive-aggressive-butler",
    "href": "slides/topics/02_r_language.html#r-as-a-passive-aggressive-butler",
    "title": "R Language",
    "section": "",
    "text": "R is popular statistical programming language and open source software\nDesigned to help non-programmers perform statistical analyses\nSometimes requires careful language to avoid unexpected outcomes\nEssential to view scripts as documentation of work"
  },
  {
    "objectID": "slides/topics/02_r_language.html#script-organization-importance",
    "href": "slides/topics/02_r_language.html#script-organization-importance",
    "title": "R Language",
    "section": "",
    "text": "Write scripts as documentation for respected colleagues\nWork through instructions using R and RStudio actively\nType your own code rather than copy-pasting\nDocument all code with clear, concise comments"
  },
  {
    "objectID": "slides/topics/02_r_language.html#help-system-in-r",
    "href": "slides/topics/02_r_language.html#help-system-in-r",
    "title": "R Language",
    "section": "",
    "text": "Use help() function with tool name in brackets\nHelp pages organized consistently across all functions\nFunction name, description, usage, and argument definitions\nEssential to understand for effective R use"
  },
  {
    "objectID": "slides/topics/02_r_language.html#math-operators",
    "href": "slides/topics/02_r_language.html#math-operators",
    "title": "R Language",
    "section": "",
    "text": "Basic arithmetic: +, -, *, /, ^ (power)\nOrder of operations follows standard mathematical rules\nUse parentheses to control calculation order\nSpacing improves readability (use single spaces)"
  },
  {
    "objectID": "slides/topics/02_r_language.html#logical-boolean-operators",
    "href": "slides/topics/02_r_language.html#logical-boolean-operators",
    "title": "R Language",
    "section": "",
    "text": "Boolean expressions resolve to TRUE or FALSE\nCommon operators: &gt;, &lt;, ==, !=, &, |, !\nUsed for selecting groups of data effectively\nPowerful alternative to creating different datasets"
  },
  {
    "objectID": "slides/topics/02_r_language.html#selecting-with-booleans",
    "href": "slides/topics/02_r_language.html#selecting-with-booleans",
    "title": "R Language",
    "section": "",
    "text": "Use Boolean expressions to select data subsets\nSquare brackets act as index for data vectors\nCombine with logical operators for complex selections\nEssential skill for data manipulation"
  },
  {
    "objectID": "slides/topics/02_r_language.html#base-r-versus-tidyverse",
    "href": "slides/topics/02_r_language.html#base-r-versus-tidyverse",
    "title": "R Language",
    "section": "",
    "text": "Base R: Traditional, powerful form of R language\nTidyverse: Alternative approach with different conventions\nBoth have benefits and philosophical differences\nBootcamp focuses exclusively on base R first"
  },
  {
    "objectID": "slides/topics/02_r_language.html#script-section-requirements",
    "href": "slides/topics/02_r_language.html#script-section-requirements",
    "title": "R Language",
    "section": "",
    "text": "Header: Contains name, date, and purpose\nContents: Roadmap listing all major sections\nBoth sections essential for every R script\nFoundation for reproducible research practices"
  },
  {
    "objectID": "slides/topics/02_r_language.html#practice-exercises",
    "href": "slides/topics/02_r_language.html#practice-exercises",
    "title": "R Language",
    "section": "",
    "text": "Working with Boolean expressions and operators\nUnderstanding help system navigation\nWriting expressions with proper spacing\nCreating and documenting script sections"
  },
  {
    "objectID": "slides/topics/10_regression.html",
    "href": "slides/topics/10_regression.html",
    "title": "Regression",
    "section": "",
    "text": "Evaluate the question of simple regression\nDiscuss data and assumptions of simple regression\nGraph simple regression effectively\nPerform tests and alternatives for simple regression\n\n\n\n\n\nOne of most common and powerful statistical tools\nCreated by Francis Galton for studying heritable traits\nConcept: whenever correlation imperfect, regression to mean occurs\nFoundation tool shared for greater scientific good\n\n\n\n\n\nRelate value of numeric variable to another variable\nPredict variable value based on another\nQuantify variation attributable to predictor\nTest significance of relationships\n\n\n\n\n\nα (alpha): intercept parameter\nβ (beta): slope parameter\ny: dependent variable, x: predictor variable\nε (epsilon): residual error term\n\n\n\n\n\nLinear relationship between variables\nNumeric continuous data for dependent variable\nIndependence of observations\nGaussian distribution of residuals (not raw data!)\n\n\n\n\n\nScatterplot with dependent variable on y-axis\nPredictor variable on x-axis\nRegression line shows line of best fit\nLine used for prediction of y given x values\n\n\n\n\n\nData scientist responsible for validating model\nTest Gaussian residuals graphically and statistically\nCheck homoscedasticity with residuals vs fitted plot\nUse diagnostic plots and formal tests\n\n\n\n\n\nResiduals represent deviation from predicted values\nShould be Gaussian distributed with mean zero\nUse histogram and q-q plots for visual assessment\nShapiro-Wilk test for formal normality testing\n\n\n\n\n\nVariance of residuals should be constant\nPlot residuals vs fitted values\nLook for even spread across x-axis\nAbsence of systematic patterns indicates independence\n\n\n\n\n\nF-statistic tests overall model significance\nR-squared shows proportion of variance explained\nCoefficient estimates show intercept and slope\nP-values test whether parameters differ from zero\n\n\n\n\n\nInclude R-squared, degrees of freedom, and p-value\nExample: “significant linear relationship (R-squared = 0.97, df = 1,54, P &lt; 0.0001)”\nNever copy-paste raw statistical output\nSummarize appropriately for intended audience\n\n\n\n\n\nData transformation when assumptions violated\nSpearman rank correlation for relationship demonstration\nNonparametric regression (Kendal-Theil-Siegel)\nConsider these when assumptions cannot be met"
  },
  {
    "objectID": "slides/topics/10_regression.html#learning-objectives",
    "href": "slides/topics/10_regression.html#learning-objectives",
    "title": "Regression",
    "section": "",
    "text": "Evaluate the question of simple regression\nDiscuss data and assumptions of simple regression\nGraph simple regression effectively\nPerform tests and alternatives for simple regression"
  },
  {
    "objectID": "slides/topics/10_regression.html#regression-to-the-mean",
    "href": "slides/topics/10_regression.html#regression-to-the-mean",
    "title": "Regression",
    "section": "",
    "text": "One of most common and powerful statistical tools\nCreated by Francis Galton for studying heritable traits\nConcept: whenever correlation imperfect, regression to mean occurs\nFoundation tool shared for greater scientific good"
  },
  {
    "objectID": "slides/topics/10_regression.html#the-question-of-simple-regression",
    "href": "slides/topics/10_regression.html#the-question-of-simple-regression",
    "title": "Regression",
    "section": "",
    "text": "Relate value of numeric variable to another variable\nPredict variable value based on another\nQuantify variation attributable to predictor\nTest significance of relationships"
  },
  {
    "objectID": "slides/topics/10_regression.html#regression-model-components",
    "href": "slides/topics/10_regression.html#regression-model-components",
    "title": "Regression",
    "section": "",
    "text": "α (alpha): intercept parameter\nβ (beta): slope parameter\ny: dependent variable, x: predictor variable\nε (epsilon): residual error term"
  },
  {
    "objectID": "slides/topics/10_regression.html#data-and-assumptions",
    "href": "slides/topics/10_regression.html#data-and-assumptions",
    "title": "Regression",
    "section": "",
    "text": "Linear relationship between variables\nNumeric continuous data for dependent variable\nIndependence of observations\nGaussian distribution of residuals (not raw data!)"
  },
  {
    "objectID": "slides/topics/10_regression.html#graphing-regression",
    "href": "slides/topics/10_regression.html#graphing-regression",
    "title": "Regression",
    "section": "",
    "text": "Scatterplot with dependent variable on y-axis\nPredictor variable on x-axis\nRegression line shows line of best fit\nLine used for prediction of y given x values"
  },
  {
    "objectID": "slides/topics/10_regression.html#testing-assumptions",
    "href": "slides/topics/10_regression.html#testing-assumptions",
    "title": "Regression",
    "section": "",
    "text": "Data scientist responsible for validating model\nTest Gaussian residuals graphically and statistically\nCheck homoscedasticity with residuals vs fitted plot\nUse diagnostic plots and formal tests"
  },
  {
    "objectID": "slides/topics/10_regression.html#residual-analysis",
    "href": "slides/topics/10_regression.html#residual-analysis",
    "title": "Regression",
    "section": "",
    "text": "Residuals represent deviation from predicted values\nShould be Gaussian distributed with mean zero\nUse histogram and q-q plots for visual assessment\nShapiro-Wilk test for formal normality testing"
  },
  {
    "objectID": "slides/topics/10_regression.html#homoscedasticity-assessment",
    "href": "slides/topics/10_regression.html#homoscedasticity-assessment",
    "title": "Regression",
    "section": "",
    "text": "Variance of residuals should be constant\nPlot residuals vs fitted values\nLook for even spread across x-axis\nAbsence of systematic patterns indicates independence"
  },
  {
    "objectID": "slides/topics/10_regression.html#regression-results-interpretation",
    "href": "slides/topics/10_regression.html#regression-results-interpretation",
    "title": "Regression",
    "section": "",
    "text": "F-statistic tests overall model significance\nR-squared shows proportion of variance explained\nCoefficient estimates show intercept and slope\nP-values test whether parameters differ from zero"
  },
  {
    "objectID": "slides/topics/10_regression.html#reporting-regression-results",
    "href": "slides/topics/10_regression.html#reporting-regression-results",
    "title": "Regression",
    "section": "",
    "text": "Include R-squared, degrees of freedom, and p-value\nExample: “significant linear relationship (R-squared = 0.97, df = 1,54, P &lt; 0.0001)”\nNever copy-paste raw statistical output\nSummarize appropriately for intended audience"
  },
  {
    "objectID": "slides/topics/10_regression.html#alternatives-to-regression",
    "href": "slides/topics/10_regression.html#alternatives-to-regression",
    "title": "Regression",
    "section": "",
    "text": "Data transformation when assumptions violated\nSpearman rank correlation for relationship demonstration\nNonparametric regression (Kendal-Theil-Siegel)\nConsider these when assumptions cannot be met"
  },
  {
    "objectID": "slides/topics/09_correlation.html",
    "href": "slides/topics/09_correlation.html",
    "title": "Correlation",
    "section": "",
    "text": "Analyze correlation between two variables\nEvaluate correlation data and assumptions\nGraph correlated variables effectively\nPerform correlation statistical tests and alternatives\n\n\n\n\n\nCorrelation does not imply causation (fundamental principle)\nStatistical relationship doesn’t mean one causes the other\nExample: ice cream sales and forest fires correlate via summer heat\nBasic tool in data science toolbox for relationships\n\n\n\n\n\nWhether demonstrable association exists between numeric variables\nInterested in how variables may “co-vary” (exhibit covariance)\nCovariance quantified as positive or negative\nStrength varies from zero to perfect correlation (+1 or -1)\n\n\n\n\n\nPearson correlation for linear relationships\nRequires Gaussian distributed numeric values\nCalculated as covariance divided by product of standard deviations\nUse cor() function in R for calculation\n\n\n\n\n\nTraditionally visualized with scatterplot\nDegree of “scatter” relates to correlation strength\nCan examine specific pairs or correlation matrices\nUse pairs() and cor() for multiple variables\n\n\n\n\n\nQuickly assess correlation between many variables\nPowerful combination of statistical summary and graphing\nCan reveal influence of grouping variables (like species)\nEssential for multivariate data exploration\n\n\n\n\n\nUse cor.test() for statistical significance\nPearson correlation is default method\nSpearman rank correlation when assumptions not met\nFollow order: Question, Graph, Test, Validate\n\n\n\n\n\nLinear relationship between variables\nBivariate Gaussian distribution\nHomoscedasticity (similar variance across range)\nIndependence of observations and absence of outliers\n\n\n\n\n\nAlternative when Pearson assumptions not met\nRelaxed assumptions: ranked data and independence\nAppropriate for non-linear relationships\nCorrelation coefficient noted as “rho” (ρ)\n\n\n\n\n\nReport test statistic, sample size/df, and p-value\nExample: “significant correlation (Pearson’s r = 0.96, df = 148, P &lt; 0.0001)”\nRound to two decimal places consistently\nNever use scientific notation for p-values &lt; 0.0001\n\n\n\n\n\nCheck assumptions before interpreting results\nUse diagnostic plots and formal tests\nConsider alternative methods if assumptions violated\nDocument assumption testing in methods section"
  },
  {
    "objectID": "slides/topics/09_correlation.html#learning-objectives",
    "href": "slides/topics/09_correlation.html#learning-objectives",
    "title": "Correlation",
    "section": "",
    "text": "Analyze correlation between two variables\nEvaluate correlation data and assumptions\nGraph correlated variables effectively\nPerform correlation statistical tests and alternatives"
  },
  {
    "objectID": "slides/topics/09_correlation.html#statistical-relationships",
    "href": "slides/topics/09_correlation.html#statistical-relationships",
    "title": "Correlation",
    "section": "",
    "text": "Correlation does not imply causation (fundamental principle)\nStatistical relationship doesn’t mean one causes the other\nExample: ice cream sales and forest fires correlate via summer heat\nBasic tool in data science toolbox for relationships"
  },
  {
    "objectID": "slides/topics/09_correlation.html#the-question-of-correlation",
    "href": "slides/topics/09_correlation.html#the-question-of-correlation",
    "title": "Correlation",
    "section": "",
    "text": "Whether demonstrable association exists between numeric variables\nInterested in how variables may “co-vary” (exhibit covariance)\nCovariance quantified as positive or negative\nStrength varies from zero to perfect correlation (+1 or -1)"
  },
  {
    "objectID": "slides/topics/09_correlation.html#data-and-assumptions",
    "href": "slides/topics/09_correlation.html#data-and-assumptions",
    "title": "Correlation",
    "section": "",
    "text": "Pearson correlation for linear relationships\nRequires Gaussian distributed numeric values\nCalculated as covariance divided by product of standard deviations\nUse cor() function in R for calculation"
  },
  {
    "objectID": "slides/topics/09_correlation.html#graphing-correlation",
    "href": "slides/topics/09_correlation.html#graphing-correlation",
    "title": "Correlation",
    "section": "",
    "text": "Traditionally visualized with scatterplot\nDegree of “scatter” relates to correlation strength\nCan examine specific pairs or correlation matrices\nUse pairs() and cor() for multiple variables"
  },
  {
    "objectID": "slides/topics/09_correlation.html#correlation-matrices",
    "href": "slides/topics/09_correlation.html#correlation-matrices",
    "title": "Correlation",
    "section": "",
    "text": "Quickly assess correlation between many variables\nPowerful combination of statistical summary and graphing\nCan reveal influence of grouping variables (like species)\nEssential for multivariate data exploration"
  },
  {
    "objectID": "slides/topics/09_correlation.html#testing-correlation",
    "href": "slides/topics/09_correlation.html#testing-correlation",
    "title": "Correlation",
    "section": "",
    "text": "Use cor.test() for statistical significance\nPearson correlation is default method\nSpearman rank correlation when assumptions not met\nFollow order: Question, Graph, Test, Validate"
  },
  {
    "objectID": "slides/topics/09_correlation.html#pearson-correlation-assumptions",
    "href": "slides/topics/09_correlation.html#pearson-correlation-assumptions",
    "title": "Correlation",
    "section": "",
    "text": "Linear relationship between variables\nBivariate Gaussian distribution\nHomoscedasticity (similar variance across range)\nIndependence of observations and absence of outliers"
  },
  {
    "objectID": "slides/topics/09_correlation.html#spearman-rank-correlation",
    "href": "slides/topics/09_correlation.html#spearman-rank-correlation",
    "title": "Correlation",
    "section": "",
    "text": "Alternative when Pearson assumptions not met\nRelaxed assumptions: ranked data and independence\nAppropriate for non-linear relationships\nCorrelation coefficient noted as “rho” (ρ)"
  },
  {
    "objectID": "slides/topics/09_correlation.html#results-and-reporting",
    "href": "slides/topics/09_correlation.html#results-and-reporting",
    "title": "Correlation",
    "section": "",
    "text": "Report test statistic, sample size/df, and p-value\nExample: “significant correlation (Pearson’s r = 0.96, df = 148, P &lt; 0.0001)”\nRound to two decimal places consistently\nNever use scientific notation for p-values &lt; 0.0001"
  },
  {
    "objectID": "slides/topics/09_correlation.html#validation-process",
    "href": "slides/topics/09_correlation.html#validation-process",
    "title": "Correlation",
    "section": "",
    "text": "Check assumptions before interpreting results\nUse diagnostic plots and formal tests\nConsider alternative methods if assumptions violated\nDocument assumption testing in methods section"
  },
  {
    "objectID": "slides/topics/08_distributions.html",
    "href": "slides/topics/08_distributions.html",
    "title": "Distributions",
    "section": "",
    "text": "Create and use histograms to evaluate data distribution\nIdentify Gaussian data and understand it’s not “normal”\nIdentify Poisson and Binomial data distributions\nDiagnose common data distributions effectively\n\n\n\n\n\nSampling underpins traditional statistics fundamentals\nPopulation of interest cannot be directly measured\nSample to estimate real measures of population\nError depends on population variation and sample size\n\n\n\n\n\nPlots numeric variable on x-axis (continuous or integers)\nFrequency or proportion of observations on y-axis\nBars represent counts in ranges called “bins”\nShape reveals distribution of data\n\n\n\n\n\nCompare sample means to true population mean\nSample means vary randomly around true mean\nMost samples close to true mean, fewer farther away\nDemonstrates sampling distribution concept\n\n\n\n\n\nClassic “bell curve” shaped distribution\nMost important for continuous numeric variables\nExpected for measurements like height, weight, length\nDescribed by mean and variance parameters\n\n\n\n\n\nLinear models assume Gaussian residuals (not raw data)\nMust test assumption for regression and ANOVA\nTwo parameters: mean and variance control shape\nDifferent means shift curve position, different variances change spread\n\n\n\n\n\nCompare data to theoretical Gaussian expectation\nStraight diagonal line indicates perfect Gaussian fit\nSystematic deviation indicates non-Gaussian distribution\nUseful diagnostic for model assumptions\n\n\n\n\n\nUsed for count data of discrete events\nClassic example: events occurring over time/space\nData typically skewed to the right\nSingle parameter λ (lambda) describes mean and variance\n\n\n\n\n\nDescribes data with exactly two outcomes (0/1, yes/no)\nCount of “successes” in series of independent trials\nExamples: coin flips, germination success, presence/absence\nTwo parameters: number of trials (n) and probability (p)\n\n\n\n\n\nDevelop expectation based on data type\nGraph data with histogram and q-q plot\nCompare q-q plots with different theoretical distributions\nTry transformations if assumptions violated\n\n\n\n\n\nSubjective endeavor requiring experience\nFirst examine type of data for expected distribution\nUse both graphical and statistical tests\nConsider transformations to meet assumptions"
  },
  {
    "objectID": "slides/topics/08_distributions.html#learning-objectives",
    "href": "slides/topics/08_distributions.html#learning-objectives",
    "title": "Distributions",
    "section": "",
    "text": "Create and use histograms to evaluate data distribution\nIdentify Gaussian data and understand it’s not “normal”\nIdentify Poisson and Binomial data distributions\nDiagnose common data distributions effectively"
  },
  {
    "objectID": "slides/topics/08_distributions.html#sampling-and-statistical-inference",
    "href": "slides/topics/08_distributions.html#sampling-and-statistical-inference",
    "title": "Distributions",
    "section": "",
    "text": "Sampling underpins traditional statistics fundamentals\nPopulation of interest cannot be directly measured\nSample to estimate real measures of population\nError depends on population variation and sample size"
  },
  {
    "objectID": "slides/topics/08_distributions.html#histogram-usage",
    "href": "slides/topics/08_distributions.html#histogram-usage",
    "title": "Distributions",
    "section": "",
    "text": "Plots numeric variable on x-axis (continuous or integers)\nFrequency or proportion of observations on y-axis\nBars represent counts in ranges called “bins”\nShape reveals distribution of data"
  },
  {
    "objectID": "slides/topics/08_distributions.html#simulation-of-samples",
    "href": "slides/topics/08_distributions.html#simulation-of-samples",
    "title": "Distributions",
    "section": "",
    "text": "Compare sample means to true population mean\nSample means vary randomly around true mean\nMost samples close to true mean, fewer farther away\nDemonstrates sampling distribution concept"
  },
  {
    "objectID": "slides/topics/08_distributions.html#gaussian-distribution",
    "href": "slides/topics/08_distributions.html#gaussian-distribution",
    "title": "Distributions",
    "section": "",
    "text": "Classic “bell curve” shaped distribution\nMost important for continuous numeric variables\nExpected for measurements like height, weight, length\nDescribed by mean and variance parameters"
  },
  {
    "objectID": "slides/topics/08_distributions.html#gaussian-assumptions-in-analysis",
    "href": "slides/topics/08_distributions.html#gaussian-assumptions-in-analysis",
    "title": "Distributions",
    "section": "",
    "text": "Linear models assume Gaussian residuals (not raw data)\nMust test assumption for regression and ANOVA\nTwo parameters: mean and variance control shape\nDifferent means shift curve position, different variances change spread"
  },
  {
    "objectID": "slides/topics/08_distributions.html#quartile-quartile-q-q-plots",
    "href": "slides/topics/08_distributions.html#quartile-quartile-q-q-plots",
    "title": "Distributions",
    "section": "",
    "text": "Compare data to theoretical Gaussian expectation\nStraight diagonal line indicates perfect Gaussian fit\nSystematic deviation indicates non-Gaussian distribution\nUseful diagnostic for model assumptions"
  },
  {
    "objectID": "slides/topics/08_distributions.html#poisson-distribution",
    "href": "slides/topics/08_distributions.html#poisson-distribution",
    "title": "Distributions",
    "section": "",
    "text": "Used for count data of discrete events\nClassic example: events occurring over time/space\nData typically skewed to the right\nSingle parameter λ (lambda) describes mean and variance"
  },
  {
    "objectID": "slides/topics/08_distributions.html#binomial-distribution",
    "href": "slides/topics/08_distributions.html#binomial-distribution",
    "title": "Distributions",
    "section": "",
    "text": "Describes data with exactly two outcomes (0/1, yes/no)\nCount of “successes” in series of independent trials\nExamples: coin flips, germination success, presence/absence\nTwo parameters: number of trials (n) and probability (p)"
  },
  {
    "objectID": "slides/topics/08_distributions.html#distribution-diagnosis-process",
    "href": "slides/topics/08_distributions.html#distribution-diagnosis-process",
    "title": "Distributions",
    "section": "",
    "text": "Develop expectation based on data type\nGraph data with histogram and q-q plot\nCompare q-q plots with different theoretical distributions\nTry transformations if assumptions violated"
  },
  {
    "objectID": "slides/topics/08_distributions.html#diagnostic-best-practices",
    "href": "slides/topics/08_distributions.html#diagnostic-best-practices",
    "title": "Distributions",
    "section": "",
    "text": "Subjective endeavor requiring experience\nFirst examine type of data for expected distribution\nUse both graphical and statistical tests\nConsider transformations to meet assumptions"
  },
  {
    "objectID": "slides/topics/05_data_frames.html",
    "href": "slides/topics/05_data_frames.html",
    "title": "Data Frames",
    "section": "",
    "text": "Describe and use common data file types\nUse Excel for data setup and create Data Dictionary\nRead data from Excel and CSV files effectively\nManipulate variables within data frames\n\n\n\n\n\nFirst step: making your data tidy and organized\nExcel or CSV files recommended for most users\nAvoid proprietary formats (SPSS, Genstat, Minitab)\nTake responsibility for your own data management\n\n\n\n\n\nEach variable should be in a column\nEach independent observation should be in a row\nData dictionary should accompany dataset\nFacilitates complete reproducibility of analysis\n\n\n\n\n\nCSV (Comma Separated Values) - plain text format\nExcel spreadsheets - widely compatible and resilient\nAvoid proprietary formats for archiving purposes\nStick to CSV or Excel unless very good reason\n\n\n\n\n\nOne table for actual data in Tidy format\nSecond tab for Data Dictionary with variable descriptions\nNo formatting or results embedded in spreadsheet\nClean separation of data and documentation\n\n\n\n\n\nDescribes each variable in enough detail\nEnables complete reproduction of any analysis\nEssential component of reproducible research\nShould accompany every dataset\n\n\n\n\n\nUse read.xlsx() from openxlsx package for Excel\nUse read.csv() for CSV files\nSet working directory with setwd() first\nEnsure file paths use forward slashes\n\n\n\n\n\nFormally set working directory for file operations\nUse getwd() to check current directory\nUse setwd() to change to desired location\nEssential for organizing input and output files\n\n\n\n\n\nUse names() to see variable names\nUse $ operator to access variables: data$variable\nUse str() to examine data frame structure\nUse [ , ] indexing for rows and columns\n\n\n\n\n\nnames(): returns variable names\nstr(): shows structure and first few values\nattach(): makes variables directly accessible\ndetach(): undoes attach() operation\n\n\n\n\n\n[rows, columns] format for precise selection\nLeave blank for all: [ , ] returns everything\nUse variable names in quotes: [“variable_name”]\nCombine with c() for multiple selections"
  },
  {
    "objectID": "slides/topics/05_data_frames.html#learning-objectives",
    "href": "slides/topics/05_data_frames.html#learning-objectives",
    "title": "Data Frames",
    "section": "",
    "text": "Describe and use common data file types\nUse Excel for data setup and create Data Dictionary\nRead data from Excel and CSV files effectively\nManipulate variables within data frames"
  },
  {
    "objectID": "slides/topics/05_data_frames.html#getting-data-into-r",
    "href": "slides/topics/05_data_frames.html#getting-data-into-r",
    "title": "Data Frames",
    "section": "",
    "text": "First step: making your data tidy and organized\nExcel or CSV files recommended for most users\nAvoid proprietary formats (SPSS, Genstat, Minitab)\nTake responsibility for your own data management"
  },
  {
    "objectID": "slides/topics/05_data_frames.html#tidy-data-concept",
    "href": "slides/topics/05_data_frames.html#tidy-data-concept",
    "title": "Data Frames",
    "section": "",
    "text": "Each variable should be in a column\nEach independent observation should be in a row\nData dictionary should accompany dataset\nFacilitates complete reproducibility of analysis"
  },
  {
    "objectID": "slides/topics/05_data_frames.html#common-data-file-types",
    "href": "slides/topics/05_data_frames.html#common-data-file-types",
    "title": "Data Frames",
    "section": "",
    "text": "CSV (Comma Separated Values) - plain text format\nExcel spreadsheets - widely compatible and resilient\nAvoid proprietary formats for archiving purposes\nStick to CSV or Excel unless very good reason"
  },
  {
    "objectID": "slides/topics/05_data_frames.html#excel-data-setup-best-practices",
    "href": "slides/topics/05_data_frames.html#excel-data-setup-best-practices",
    "title": "Data Frames",
    "section": "",
    "text": "One table for actual data in Tidy format\nSecond tab for Data Dictionary with variable descriptions\nNo formatting or results embedded in spreadsheet\nClean separation of data and documentation"
  },
  {
    "objectID": "slides/topics/05_data_frames.html#data-dictionary-importance",
    "href": "slides/topics/05_data_frames.html#data-dictionary-importance",
    "title": "Data Frames",
    "section": "",
    "text": "Describes each variable in enough detail\nEnables complete reproduction of any analysis\nEssential component of reproducible research\nShould accompany every dataset"
  },
  {
    "objectID": "slides/topics/05_data_frames.html#reading-data-into-r",
    "href": "slides/topics/05_data_frames.html#reading-data-into-r",
    "title": "Data Frames",
    "section": "",
    "text": "Use read.xlsx() from openxlsx package for Excel\nUse read.csv() for CSV files\nSet working directory with setwd() first\nEnsure file paths use forward slashes"
  },
  {
    "objectID": "slides/topics/05_data_frames.html#working-directory-management",
    "href": "slides/topics/05_data_frames.html#working-directory-management",
    "title": "Data Frames",
    "section": "",
    "text": "Formally set working directory for file operations\nUse getwd() to check current directory\nUse setwd() to change to desired location\nEssential for organizing input and output files"
  },
  {
    "objectID": "slides/topics/05_data_frames.html#data-frame-manipulation",
    "href": "slides/topics/05_data_frames.html#data-frame-manipulation",
    "title": "Data Frames",
    "section": "",
    "text": "Use names() to see variable names\nUse $ operator to access variables: data$variable\nUse str() to examine data frame structure\nUse [ , ] indexing for rows and columns"
  },
  {
    "objectID": "slides/topics/05_data_frames.html#key-data-frame-functions",
    "href": "slides/topics/05_data_frames.html#key-data-frame-functions",
    "title": "Data Frames",
    "section": "",
    "text": "names(): returns variable names\nstr(): shows structure and first few values\nattach(): makes variables directly accessible\ndetach(): undoes attach() operation"
  },
  {
    "objectID": "slides/topics/05_data_frames.html#index-operator-usage",
    "href": "slides/topics/05_data_frames.html#index-operator-usage",
    "title": "Data Frames",
    "section": "",
    "text": "[rows, columns] format for precise selection\nLeave blank for all: [ , ] returns everything\nUse variable names in quotes: [“variable_name”]\nCombine with c() for multiple selections"
  },
  {
    "objectID": "slides/topics/03_functions.html",
    "href": "slides/topics/03_functions.html",
    "title": "R Functions",
    "section": "",
    "text": "Describe what functions do and how they work\nUse functions effectively with the help system\nUnderstand what packages are and their purpose\nFind, download, and use R packages\n\n\n\n\n\nR functions are tools that do work for you\nPackages are toolboxes containing related functions\nR comes with many packages, thousands more available\nFunctions grouped by task focus and functionality\n\n\n\n\n\nGeneric format: function_name()\nFunction name with required bracket notation\nR is case sensitive for function names\nDetails like capitalization matter significantly\n\n\n\n\n\nInput information or data into function brackets\nFunction output is the work being done\nArguments have unique names and values\nUse equals sign for argument assignment\n\n\n\n\n\nOften easy to guess: mean(), log(), sd()\nplot(), boxplot(), help() for common tasks\nNames usually reflect their purpose clearly\nHelp pages essential for learning usage\n\n\n\n\n\nEach argument has unique name\nValues assigned using equals sign\nArguments separated by commas\nOptional arguments indicated by “…”\n\n\n\n\n\nUse help() function as reference for arguments\nEssential until you memorize common functions\nCheck Usage and Arguments sections carefully\nKey to learning R in easiest way\n\n\n\n\n\nToolboxes built to solve specific problems\nOpen source and freely available\nDownload when you identify need for functions\nOver 18,000 packages currently available\n\n\n\n\n\nStep 1: install.packages(“package_name”)\nStep 2: library(“package_name”)\nInstalling downloads from remote repository\nLoading makes functions available for use\n\n\n\n\n\nUse Packages tab in lower right pane\nClick radio button to load packages\nClick Install button to install new packages\nSearch official CRAN repository by default\n\n\n\n\n\nAlways get help page up as reference\nLearn arguments and customization options\nPractice with common functions regularly\nUnderstand before memorizing function usage"
  },
  {
    "objectID": "slides/topics/03_functions.html#learning-objectives",
    "href": "slides/topics/03_functions.html#learning-objectives",
    "title": "R Functions",
    "section": "",
    "text": "Describe what functions do and how they work\nUse functions effectively with the help system\nUnderstand what packages are and their purpose\nFind, download, and use R packages"
  },
  {
    "objectID": "slides/topics/03_functions.html#the-garage-and-toolbox-metaphor",
    "href": "slides/topics/03_functions.html#the-garage-and-toolbox-metaphor",
    "title": "R Functions",
    "section": "",
    "text": "R functions are tools that do work for you\nPackages are toolboxes containing related functions\nR comes with many packages, thousands more available\nFunctions grouped by task focus and functionality"
  },
  {
    "objectID": "slides/topics/03_functions.html#function-basics",
    "href": "slides/topics/03_functions.html#function-basics",
    "title": "R Functions",
    "section": "",
    "text": "Generic format: function_name()\nFunction name with required bracket notation\nR is case sensitive for function names\nDetails like capitalization matter significantly"
  },
  {
    "objectID": "slides/topics/03_functions.html#using-functions",
    "href": "slides/topics/03_functions.html#using-functions",
    "title": "R Functions",
    "section": "",
    "text": "Input information or data into function brackets\nFunction output is the work being done\nArguments have unique names and values\nUse equals sign for argument assignment"
  },
  {
    "objectID": "slides/topics/03_functions.html#common-function-names",
    "href": "slides/topics/03_functions.html#common-function-names",
    "title": "R Functions",
    "section": "",
    "text": "Often easy to guess: mean(), log(), sd()\nplot(), boxplot(), help() for common tasks\nNames usually reflect their purpose clearly\nHelp pages essential for learning usage"
  },
  {
    "objectID": "slides/topics/03_functions.html#function-arguments",
    "href": "slides/topics/03_functions.html#function-arguments",
    "title": "R Functions",
    "section": "",
    "text": "Each argument has unique name\nValues assigned using equals sign\nArguments separated by commas\nOptional arguments indicated by “…”"
  },
  {
    "objectID": "slides/topics/03_functions.html#getting-help-with-functions",
    "href": "slides/topics/03_functions.html#getting-help-with-functions",
    "title": "R Functions",
    "section": "",
    "text": "Use help() function as reference for arguments\nEssential until you memorize common functions\nCheck Usage and Arguments sections carefully\nKey to learning R in easiest way"
  },
  {
    "objectID": "slides/topics/03_functions.html#r-packages",
    "href": "slides/topics/03_functions.html#r-packages",
    "title": "R Functions",
    "section": "",
    "text": "Toolboxes built to solve specific problems\nOpen source and freely available\nDownload when you identify need for functions\nOver 18,000 packages currently available"
  },
  {
    "objectID": "slides/topics/03_functions.html#installing-and-loading-packages",
    "href": "slides/topics/03_functions.html#installing-and-loading-packages",
    "title": "R Functions",
    "section": "",
    "text": "Step 1: install.packages(“package_name”)\nStep 2: library(“package_name”)\nInstalling downloads from remote repository\nLoading makes functions available for use"
  },
  {
    "objectID": "slides/topics/03_functions.html#rstudio-package-management",
    "href": "slides/topics/03_functions.html#rstudio-package-management",
    "title": "R Functions",
    "section": "",
    "text": "Use Packages tab in lower right pane\nClick radio button to load packages\nClick Install button to install new packages\nSearch official CRAN repository by default"
  },
  {
    "objectID": "slides/topics/03_functions.html#best-practices",
    "href": "slides/topics/03_functions.html#best-practices",
    "title": "R Functions",
    "section": "",
    "text": "Always get help page up as reference\nLearn arguments and customization options\nPractice with common functions regularly\nUnderstand before memorizing function usage"
  },
  {
    "objectID": "slides/topics/12_anova.html",
    "href": "slides/topics/12_anova.html",
    "title": "ANOVA",
    "section": "",
    "text": "Articulate the question of 1-way ANOVA\nEvaluate data and assumptions for 1-way ANOVA\nGraph 1-way ANOVA data effectively\nPerform tests and alternatives for 1-way ANOVA\n\n\n\n\n\nRevolutionized objectivity in experimental data analysis\nInvented by R.A. Fisher at Rothamsted Research\n“Convenient method of arranging the arithmetic”\nFoundation of basic statistical practice today\n\n\n\n\n\nOne numeric continuous dependent variable\nFactor with 2 or more levels (often with control)\nWhen two levels: conceptually equivalent to t-test\nTest overall difference in means between factor levels\n\n\n\n\n\nOverall difference test between factor means\nComparison of each level with control/reference\nPost hoc tests between specific factor levels\nExamination of sources of variation in dependent variable\n\n\n\n\n\nProportion of variance between groups relative to within groups\nHigher F values suggest greater likelihood of real differences\nBased on comparison of mean squares\nFoundation for significance testing in ANOVA\n\n\n\n\n\nWide format: separate vectors for each factor level\nLong format (preferred): single numeric vector with factor vector\nEach row corresponds to single independent case\nTidy Data standard preferred for analysis\n\n\n\n\n\nGaussian residuals (test graphically and with NHST)\nHomoscedasticity (residuals vs fitted values plot)\nEquality of variance (residual vs factor plot and NHST)\nIndependent observations (assumed based on study design)\n\n\n\n\n\nUse aov() to create model object\nTest Gaussian residuals with histogram and qqPlot()\nShapiro-Wilk test for formal normality assessment\nBartlett test for equality of variance across groups\n\n\n\n\n\nClassic visualization: boxplot by factor levels\nShow central tendency separately for each group\nAdd raw data points over box summaries\nInclude reference line for grand mean\n\n\n\n\n\nUse aov() function for basic ANOVA\nAlternative: lm() for contrasts and linear model approach\nOutput in classic ANOVA table format\nF statistic, degrees of freedom, and p-value reported\n\n\n\n\n\nSet reference level with relevel() function\nCompare each factor level to reference\nPost hoc tests for all pairwise comparisons\nBonferroni adjustment for multiple testing\n\n\n\n\n\nBonferroni adjustment: conservative, divides alpha by comparisons\nTukey HSD: less conservative, ideal for 1-way ANOVA\nBoth control Type I error rate across tests\nChoose based on research questions and context\n\n\n\n\n\nKruskal-Wallis test when assumptions not met\nLess statistical power but no distributional requirements\nUse kruskal.test() function in R\nQualitatively similar interpretation to ANOVA"
  },
  {
    "objectID": "slides/topics/12_anova.html#learning-objectives",
    "href": "slides/topics/12_anova.html#learning-objectives",
    "title": "ANOVA",
    "section": "",
    "text": "Articulate the question of 1-way ANOVA\nEvaluate data and assumptions for 1-way ANOVA\nGraph 1-way ANOVA data effectively\nPerform tests and alternatives for 1-way ANOVA"
  },
  {
    "objectID": "slides/topics/12_anova.html#analysis-of-variance-anova",
    "href": "slides/topics/12_anova.html#analysis-of-variance-anova",
    "title": "ANOVA",
    "section": "",
    "text": "Revolutionized objectivity in experimental data analysis\nInvented by R.A. Fisher at Rothamsted Research\n“Convenient method of arranging the arithmetic”\nFoundation of basic statistical practice today"
  },
  {
    "objectID": "slides/topics/12_anova.html#the-question-of-1-way-anova",
    "href": "slides/topics/12_anova.html#the-question-of-1-way-anova",
    "title": "ANOVA",
    "section": "",
    "text": "One numeric continuous dependent variable\nFactor with 2 or more levels (often with control)\nWhen two levels: conceptually equivalent to t-test\nTest overall difference in means between factor levels"
  },
  {
    "objectID": "slides/topics/12_anova.html#anova-applications",
    "href": "slides/topics/12_anova.html#anova-applications",
    "title": "ANOVA",
    "section": "",
    "text": "Overall difference test between factor means\nComparison of each level with control/reference\nPost hoc tests between specific factor levels\nExamination of sources of variation in dependent variable"
  },
  {
    "objectID": "slides/topics/12_anova.html#f-ratio-test-statistic",
    "href": "slides/topics/12_anova.html#f-ratio-test-statistic",
    "title": "ANOVA",
    "section": "",
    "text": "Proportion of variance between groups relative to within groups\nHigher F values suggest greater likelihood of real differences\nBased on comparison of mean squares\nFoundation for significance testing in ANOVA"
  },
  {
    "objectID": "slides/topics/12_anova.html#data-format-requirements",
    "href": "slides/topics/12_anova.html#data-format-requirements",
    "title": "ANOVA",
    "section": "",
    "text": "Wide format: separate vectors for each factor level\nLong format (preferred): single numeric vector with factor vector\nEach row corresponds to single independent case\nTidy Data standard preferred for analysis"
  },
  {
    "objectID": "slides/topics/12_anova.html#anova-assumptions",
    "href": "slides/topics/12_anova.html#anova-assumptions",
    "title": "ANOVA",
    "section": "",
    "text": "Gaussian residuals (test graphically and with NHST)\nHomoscedasticity (residuals vs fitted values plot)\nEquality of variance (residual vs factor plot and NHST)\nIndependent observations (assumed based on study design)"
  },
  {
    "objectID": "slides/topics/12_anova.html#assumption-testing-process",
    "href": "slides/topics/12_anova.html#assumption-testing-process",
    "title": "ANOVA",
    "section": "",
    "text": "Use aov() to create model object\nTest Gaussian residuals with histogram and qqPlot()\nShapiro-Wilk test for formal normality assessment\nBartlett test for equality of variance across groups"
  },
  {
    "objectID": "slides/topics/12_anova.html#graphing-anova-data",
    "href": "slides/topics/12_anova.html#graphing-anova-data",
    "title": "ANOVA",
    "section": "",
    "text": "Classic visualization: boxplot by factor levels\nShow central tendency separately for each group\nAdd raw data points over box summaries\nInclude reference line for grand mean"
  },
  {
    "objectID": "slides/topics/12_anova.html#anova-f-test-and-alternatives",
    "href": "slides/topics/12_anova.html#anova-f-test-and-alternatives",
    "title": "ANOVA",
    "section": "",
    "text": "Use aov() function for basic ANOVA\nAlternative: lm() for contrasts and linear model approach\nOutput in classic ANOVA table format\nF statistic, degrees of freedom, and p-value reported"
  },
  {
    "objectID": "slides/topics/12_anova.html#contrasts-and-post-hoc-testing",
    "href": "slides/topics/12_anova.html#contrasts-and-post-hoc-testing",
    "title": "ANOVA",
    "section": "",
    "text": "Set reference level with relevel() function\nCompare each factor level to reference\nPost hoc tests for all pairwise comparisons\nBonferroni adjustment for multiple testing"
  },
  {
    "objectID": "slides/topics/12_anova.html#multiple-comparison-methods",
    "href": "slides/topics/12_anova.html#multiple-comparison-methods",
    "title": "ANOVA",
    "section": "",
    "text": "Bonferroni adjustment: conservative, divides alpha by comparisons\nTukey HSD: less conservative, ideal for 1-way ANOVA\nBoth control Type I error rate across tests\nChoose based on research questions and context"
  },
  {
    "objectID": "slides/topics/12_anova.html#non-parametric-alternative",
    "href": "slides/topics/12_anova.html#non-parametric-alternative",
    "title": "ANOVA",
    "section": "",
    "text": "Kruskal-Wallis test when assumptions not met\nLess statistical power but no distributional requirements\nUse kruskal.test() function in R\nQualitatively similar interpretation to ANOVA"
  },
  {
    "objectID": "slides/topics/13_reproducibility.html",
    "href": "slides/topics/13_reproducibility.html",
    "title": "Reproducibility",
    "section": "",
    "text": "Explain why reproducibility matters in data science\nIdentify key components of reproducible workflow\nImplement best practices for documentation\nOrganize project files effectively\n\n\n\n\n\nCornerstone of scientific research and data analysis\nEnsures findings can be verified and methods understood\nWork can be built upon by others (including future self)\nProvides verification, collaboration, efficiency, impact, and trust\n\n\n\n\n\nAnalysis can be recreated using same data and methods\nOthers can confirm your findings\nTeam members can understand and contribute\nYou can revisit and build upon your own work\n\n\n\n\n\nDocumented code with clear comments and structure\nVersion control systems like Git for tracking changes\nEnvironment management for consistent software conditions\nProper data management with preserved raw data\n\n\n\n\n\nClear purpose statements and input/output descriptions\nParameter explanations and usage examples\nError checking and validation within functions\nComments explaining why, not just what code does\n\n\n\n\n\nDocument software environment for recreation\nUse sessionInfo() to capture package versions\nConsider renv package for project-specific environments\nEnsure others can recreate same conditions\n\n\n\n\n\nRaw data preservation (never modify originals)\nData cleaning scripts documenting all transformations\nClear data documentation and metadata\nTransparent pipeline from raw to analysis-ready data\n\n\n\n\n\nClear directory structure with logical separation\nNumbered scripts indicating workflow order\nSeparate folders for data, code, results, documentation\nREADME files explaining project structure and usage\n\n\n\n\n\nComprehensive information including all necessary details\nClear language avoiding jargon when possible\nCurrent documentation updated as project evolves\nAccessible storage with project files\n\n\n\n\n\nREADME.md with project overview and instructions\ndata/ folder with raw/ and processed/ subfolders\ncode/ folder with numbered analysis scripts\nresults/ folder for figures/ and tables/\n\n\n\n\n\nTrack changes to code and documentation over time\nEnable collaboration without file conflicts\nAbility to revert to previous versions if needed\nEssential component of modern reproducible research"
  },
  {
    "objectID": "slides/topics/13_reproducibility.html#learning-objectives",
    "href": "slides/topics/13_reproducibility.html#learning-objectives",
    "title": "Reproducibility",
    "section": "",
    "text": "Explain why reproducibility matters in data science\nIdentify key components of reproducible workflow\nImplement best practices for documentation\nOrganize project files effectively"
  },
  {
    "objectID": "slides/topics/13_reproducibility.html#why-reproducibility-matters",
    "href": "slides/topics/13_reproducibility.html#why-reproducibility-matters",
    "title": "Reproducibility",
    "section": "",
    "text": "Cornerstone of scientific research and data analysis\nEnsures findings can be verified and methods understood\nWork can be built upon by others (including future self)\nProvides verification, collaboration, efficiency, impact, and trust"
  },
  {
    "objectID": "slides/topics/13_reproducibility.html#definition-and-benefits",
    "href": "slides/topics/13_reproducibility.html#definition-and-benefits",
    "title": "Reproducibility",
    "section": "",
    "text": "Analysis can be recreated using same data and methods\nOthers can confirm your findings\nTeam members can understand and contribute\nYou can revisit and build upon your own work"
  },
  {
    "objectID": "slides/topics/13_reproducibility.html#components-of-reproducible-workflows",
    "href": "slides/topics/13_reproducibility.html#components-of-reproducible-workflows",
    "title": "Reproducibility",
    "section": "",
    "text": "Documented code with clear comments and structure\nVersion control systems like Git for tracking changes\nEnvironment management for consistent software conditions\nProper data management with preserved raw data"
  },
  {
    "objectID": "slides/topics/13_reproducibility.html#code-documentation-standards",
    "href": "slides/topics/13_reproducibility.html#code-documentation-standards",
    "title": "Reproducibility",
    "section": "",
    "text": "Clear purpose statements and input/output descriptions\nParameter explanations and usage examples\nError checking and validation within functions\nComments explaining why, not just what code does"
  },
  {
    "objectID": "slides/topics/13_reproducibility.html#environment-management",
    "href": "slides/topics/13_reproducibility.html#environment-management",
    "title": "Reproducibility",
    "section": "",
    "text": "Document software environment for recreation\nUse sessionInfo() to capture package versions\nConsider renv package for project-specific environments\nEnsure others can recreate same conditions"
  },
  {
    "objectID": "slides/topics/13_reproducibility.html#data-management-principles",
    "href": "slides/topics/13_reproducibility.html#data-management-principles",
    "title": "Reproducibility",
    "section": "",
    "text": "Raw data preservation (never modify originals)\nData cleaning scripts documenting all transformations\nClear data documentation and metadata\nTransparent pipeline from raw to analysis-ready data"
  },
  {
    "objectID": "slides/topics/13_reproducibility.html#file-organization-strategies",
    "href": "slides/topics/13_reproducibility.html#file-organization-strategies",
    "title": "Reproducibility",
    "section": "",
    "text": "Clear directory structure with logical separation\nNumbered scripts indicating workflow order\nSeparate folders for data, code, results, documentation\nREADME files explaining project structure and usage"
  },
  {
    "objectID": "slides/topics/13_reproducibility.html#documentation-best-practices",
    "href": "slides/topics/13_reproducibility.html#documentation-best-practices",
    "title": "Reproducibility",
    "section": "",
    "text": "Comprehensive information including all necessary details\nClear language avoiding jargon when possible\nCurrent documentation updated as project evolves\nAccessible storage with project files"
  },
  {
    "objectID": "slides/topics/13_reproducibility.html#project-structure-template",
    "href": "slides/topics/13_reproducibility.html#project-structure-template",
    "title": "Reproducibility",
    "section": "",
    "text": "README.md with project overview and instructions\ndata/ folder with raw/ and processed/ subfolders\ncode/ folder with numbered analysis scripts\nresults/ folder for figures/ and tables/"
  },
  {
    "objectID": "slides/topics/13_reproducibility.html#version-control-integration",
    "href": "slides/topics/13_reproducibility.html#version-control-integration",
    "title": "Reproducibility",
    "section": "",
    "text": "Track changes to code and documentation over time\nEnable collaboration without file conflicts\nAbility to revert to previous versions if needed\nEssential component of modern reproducible research"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "Data science learning pathway\n\n\n\n\nThis self-paced bootcamp will take most people about 40-60 hours to complete. The schedule below provides an overview of all course modules and resources. Work through the materials sequentially for the best learning experience.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule"
    ]
  },
  {
    "objectID": "schedule.html#r-stats-bootcamp-learning-path",
    "href": "schedule.html#r-stats-bootcamp-learning-path",
    "title": "Course Schedule",
    "section": "",
    "text": "This self-paced bootcamp will take most people about 40-60 hours to complete. The schedule below provides an overview of all course modules and resources. Work through the materials sequentially for the best learning experience.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule"
    ]
  },
  {
    "objectID": "schedule.html#module-1-r-foundations",
    "href": "schedule.html#module-1-r-foundations",
    "title": "Course Schedule",
    "section": "Module 1: R Foundations",
    "text": "Module 1: R Foundations\nLearn the fundamentals of R programming and the RStudio environment.\n\n\n1. R and RStudio Setup\nGetting started with the R programming environment\n\nInstalling R and RStudio\nNavigating the RStudio interface\nCreating your first R script\nUnderstanding R packages\n\nBegin Lesson → | Slides\n\n\n2. R Language Basics\nCore concepts of the R programming language\n\nR syntax and data types\nVariables and assignment\nBasic operations and calculations\nControl structures (if/else, loops)\n\nBegin Lesson → | Slides\n\n\n3. Functions and Packages\nWorking with functions and extending R’s capabilities\n\nUsing built-in functions\nCreating your own functions\nInstalling and loading packages\nFunction documentation and help\n\nBegin Lesson →\n\n\n4. Data Objects\nUnderstanding R’s data structures\n\nVectors, matrices, and arrays\nLists and factors\nWorking with dates and times\nType conversion and coercion\n\nBegin Lesson →\n\n\n5. Data Frames\nWorking with tabular data in R\n\nCreating and manipulating data frames\nAccessing data frame elements\nAdding and modifying columns\nMerging data frames\n\nBegin Lesson →\n\n\n6. Data Manipulation\nTechniques for cleaning and transforming data\n\nFiltering and subsetting data\nSorting and arranging data\nSummarizing data\nReshaping data (wide vs. long format)\n\nBegin Lesson →",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule"
    ]
  },
  {
    "objectID": "schedule.html#module-2-statistical-analysis",
    "href": "schedule.html#module-2-statistical-analysis",
    "title": "Course Schedule",
    "section": "Module 2: Statistical Analysis",
    "text": "Module 2: Statistical Analysis\nApply R programming to statistical analysis and data visualization.\n\n\n7. Question, Explore, Analyze\nThe data analysis workflow\n\nFormulating research questions\nExploratory data analysis\nData visualization principles\nCommunicating findings\n\nBegin Lesson →\n\n\n8. Sampling Distributions\nUnderstanding probability and sampling\n\nRandom sampling\nProbability distributions\nThe Central Limit Theorem\nConfidence intervals\n\nBegin Lesson →\n\n\n9. Correlation\nMeasuring relationships between variables\n\nCorrelation coefficients\nVisualizing correlations\nTesting correlation significance\nCorrelation vs. causation\n\nBegin Lesson →\n\n\n10. Simple Linear Regression\nModeling relationships between variables\n\nLinear regression concepts\nFitting regression models in R\nInterpreting regression output\nAssessing model fit\n\nBegin Lesson →\n\n\n11. T-tests\nComparing group means\n\nOne-sample t-tests\nIndependent samples t-tests\nPaired samples t-tests\nEffect sizes and power\n\nBegin Lesson →\n\n\n12. One-way ANOVA\nAnalyzing variance between groups\n\nANOVA concepts and assumptions\nConducting ANOVA in R\nPost-hoc tests\nReporting ANOVA results\n\nBegin Lesson →",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule"
    ]
  },
  {
    "objectID": "schedule.html#module-3-reproducible-research",
    "href": "schedule.html#module-3-reproducible-research",
    "title": "Course Schedule",
    "section": "Module 3: Reproducible Research",
    "text": "Module 3: Reproducible Research\nLearn essential tools and practices for reproducible data science.\n\n\n13. Reproducibility Principles\nIntroduction to reproducible research\n\nWhy reproducibility matters\nComponents of reproducible workflows\nDocumentation best practices\nFile organization strategies\n\nBegin Lesson →\n\n\n14. R Markdown\nCreating dynamic documents\n\nR Markdown basics\nCombining code, results, and narrative\nDocument formatting options\nGenerating reports in multiple formats\n\nBegin Lesson →\n\n\n15. Git and GitHub Basics\nVersion control for data science\n\nUnderstanding version control\nSetting up Git and GitHub\nBasic Git workflow\nTracking changes to your code\n\nBegin Lesson →\n\n\n16. Collaborative Workflows\nWorking effectively with others\n\nProject organization\nSharing code and data\nCollaboration best practices\nMaintaining reproducibility in teams\n\nBegin Lesson →",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Stats Bootcamp",
    "section": "",
    "text": "Master R programming, statistical analysis, and reproducible research in one comprehensive bootcamp\n\n\nStart Learning About the Course",
    "crumbs": [
      "Home",
      "Getting Started",
      "R Stats Bootcamp"
    ]
  },
  {
    "objectID": "index.html#why-learn-with-r-stats-bootcamp",
    "href": "index.html#why-learn-with-r-stats-bootcamp",
    "title": "R Stats Bootcamp",
    "section": "Why Learn with R Stats Bootcamp?",
    "text": "Why Learn with R Stats Bootcamp?\n\n\nLearn by Doing\nMaster R programming through hands-on exercises and real-world data analysis projects. Build skills by writing actual code from day one.\n\n\nDiscover Insights\nTransform raw data into meaningful insights. Learn how to ask the right questions and find answers through statistical analysis.\n\n\nShare Your Findings\nCreate reproducible research that others can understand, verify, and build upon using modern tools like R Markdown and GitHub.",
    "crumbs": [
      "Home",
      "Getting Started",
      "R Stats Bootcamp"
    ]
  },
  {
    "objectID": "index.html#your-learning-pathway",
    "href": "index.html#your-learning-pathway",
    "title": "R Stats Bootcamp",
    "section": "Your Learning Pathway",
    "text": "Your Learning Pathway\n\n\nModule 1\nR Foundations\nGet started with R programming and RStudio. Learn the fundamentals of data structures, functions, and data manipulation. Begin Module 1 →\n\n\nModule 2\nStatistical Analysis\nExplore your data, understand sampling distributions, and master essential statistical tests for research. Begin Module 2 →\n\n\nModule 3\nReproducible Research\nLearn tools and practices for reproducible science including R Markdown, version control with Git, and collaborative workflows Begin Module 3 →",
    "crumbs": [
      "Home",
      "Getting Started",
      "R Stats Bootcamp"
    ]
  },
  {
    "objectID": "index.html#designed-for-research-students-and-other-interesting-people",
    "href": "index.html#designed-for-research-students-and-other-interesting-people",
    "title": "R Stats Bootcamp",
    "section": "Designed for Research Students and other interesting people",
    "text": "Designed for Research Students and other interesting people\n\n“I loved the R Stats Bookcamp even though I was afraid to start. I loved the humorous comments throughout and the step-by-step approach made learning R enjoyable. I even analyzed my own data after taking it!”\n\n\n— Research Student, Harper Adams University",
    "crumbs": [
      "Home",
      "Getting Started",
      "R Stats Bootcamp"
    ]
  },
  {
    "objectID": "index.html#ready-to-start-your-data-science-journey",
    "href": "index.html#ready-to-start-your-data-science-journey",
    "title": "R Stats Bootcamp",
    "section": "Ready to Start Your Data Science Journey?",
    "text": "Ready to Start Your Data Science Journey?\nFollow along with the course materials at your own pace. Complete each module sequentially for the best learning experience.\nView Full Course Schedule",
    "crumbs": [
      "Home",
      "Getting Started",
      "R Stats Bootcamp"
    ]
  },
  {
    "objectID": "03-functions.html",
    "href": "03-functions.html",
    "title": "03 R Functions",
    "section": "",
    "text": "R as a garage full of tools",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "03 R Functions"
    ]
  },
  {
    "objectID": "03-functions.html#functions-and-packages-toolboxes-r-as-a-garage-full-of-tools",
    "href": "03-functions.html#functions-and-packages-toolboxes-r-as-a-garage-full-of-tools",
    "title": "03 R Functions",
    "section": "1 Functions and Packages Toolboxes (R as a garage full of tools)",
    "text": "1 Functions and Packages Toolboxes (R as a garage full of tools)\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this lesson, you will be able to:\n\nDescribe what functions do\nUse functions and use the help system\nDescribe what packages are\nFind, download and use packages\n\n\n\n\nThe garage and toolbox metaphors\nOne of the best things about R is that it can be customized to accomplish a huge variety of kinds of tasks: perform all sorts of statistical analyses from simple to bleeding edge, produce professional graphs, format analyses into presentations, manuscripts and web pages, collaboration, GIS and mapping, and a lot more. The tools themselves are contained in toolboxes and in a given toolbox, the tools are related to each other, usually in a focus to the kind of tasks they are suited for.\n\nThe tools in R are functions, and the toolboxes are packages.\n\nWhile R comes with a lot of packages, there is an enormous amount available for instant download at any time you want or need them (over 18,000 different packages at the moment…). Making use of all these resources is usually a case of identifying a problem to solve, finding the package that can help you, and then learning how to use the functions in the package This page is all about introducing functions, packages and the R help system.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "03 R Functions"
    ]
  },
  {
    "objectID": "03-functions.html#function-tour",
    "href": "03-functions.html#function-tour",
    "title": "03 R Functions",
    "section": "2 Function tour",
    "text": "2 Function tour\n\nThink of functions in R as tools that do work for you.\n\nCode for functions is is simple once you get the idea, but you have to understand how they work to use them in the most powerful way. Also, to make the most of functions, you must get to know which ones perform common tasks, and how to use them. We will practice that in this section. We consider the USE of functions (for a given problem) as a separate issue from discovering a function, and here we focus on USE.\nA generic might look like this: function_name(). The function name is (obviously) the function_name part and all functions must have the bracket notation (). There are some rules for function names and for naming R objects in general, but for now the most important thing to keep in mind is that details like capitalization are important, that is, R is case sensitive.\nThus, function_name() is not the same as Function_name() or function_Name() (see what I did there?).\n2.1 Using functions\nFunctions are typically used by providing some information inside the brackets, usually data for the function to do work on or settings for the function. Function values and settings are assigned to function arguments and most functions have several arguments.\n\nfunction_name(argument_1 = value_1, argument_2 = value_2, ...)\nA general rule is that you INPUT information or data into function brackets that you want the function to do work and function OUTPUT is the work being done, sometimes including information output (like the results of a statistical test, or a plot).\n\nEach argument has a unique name\nArgument values are assigned using the equals sign =, the assignment operator\nEach argument is separated by a comma ,\nThe ... means there are additional arguments that can be used optionally (for now we can ignore those)\n\n\n2.2 Function names\nFinding functions by their names is often easy for very simple and common tasks. For example:\nmean() Calculates the arithmetic mean\nlog() Calculates the log\nsd() Calculates the standard deviation\nplot() Draws plots\nboxplot() Draws boxplots\nhelp() Used to access help pages\nYou get the idea…\n\nThe most important thing here is that you would generally get the help page up as a reference to what arguments are required and how to customize your function use. This is the key to learning R in the easiest way. That is, until you memorize the use and arguments for common functions.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "03 R Functions"
    ]
  },
  {
    "objectID": "03-functions.html#using-functions-and-getting-help",
    "href": "03-functions.html#using-functions-and-getting-help",
    "title": "03 R Functions",
    "section": "3 Using functions and getting help",
    "text": "3 Using functions and getting help\nFor now, let’s assume that you know:\n\nWhat tasks you want to do (maybe outlined with pseudocode), and\nWhat function(s) can perform those tasks.\n\n\nTry this out in your own script:\n## A workflow for using functions ####\n\n## (make pseudocode of steps in comments)\n\n# Overall task: calculate the mean for a vector of numbers\n# Step 1: Code the vector of data - c() function\n# Step 2: Calculate the mean - mean() function\n# Step 3: Plot the data - boxplot()\n\n# Step 1: Code the vector of data - c() function\n\nhelp(c) # We use this a lot - it \"combines\" numbers\nc(2, 6, 7, 8.1, 5, 6) \n\n# Step 2: Calculate the mean - mean() function\n\nhelp(mean) \n# Notice under Usage, the \"x\" argument\n# Notice under Arguments, x is a numeric vector\n\nmean(x = c(2, 6, 7, 8.1, 5, 6)) # Easy\n\n# Step 3: Plot the data - boxplot()\n\nhelp(boxplot) # x again!\nboxplot(x = c(2, 6, 7, 8.1, 5, 6))\n\n# Challenge: Add an axis label to the y-axis - can you find the name of the argument?\n\n\n# without a y-axis label this is not a good graph\n\nboxplot(x = c(2, 6, 7, 8.1, 5, 6))",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "03 R Functions"
    ]
  },
  {
    "objectID": "03-functions.html#r-packages",
    "href": "03-functions.html#r-packages",
    "title": "03 R Functions",
    "section": "4 R packages",
    "text": "4 R packages\nThere are a lot of R packages. These are “toolboxes” often built in the spirit of identifying a problem, literally making a tools that solves the problem, and then sharing the tool for other to use as well. In fact, all official R packages are “open source”, meaning that you may use them freely, but also you can improve them and add functionality. This section is about the basics of R packages that are additional to the base R installation.\nTypically, you only download a package once you identify you need to use functions in it. There are are several ways to accomplish this. We are going to practice 2 different ways, one with R code that is simple and will work no matter how you use R, and one that uses menus in RStudio.\n\n4.1 Finding, downloading and using packages\nFinding packages happens a variety of ways in practice. A package may be recommended to you, you might be told to use a particular package for a task or assignment, or you may discover it on the web.\nInstalling and loading packages with code\nThere are 2 steps here - installing, then loading. Installing is very easy to do using the install.packages() function. Loading a package making the functions in it available for use is done using the library() package. Basic usage of these functions is:\n# Step 1: install a package\n\nhelp(install.packages) # just have a look\ninstall.packages(pkgs = \"package_name\")\n\n# The package is downloaded from a remote repository, often\n# with additional packages that are required for use.\n\n# Step 2: load a package\n\nlibrary(\"package_name\")\n\n# Challenge: Install and load the \"ggplot2\" package, and then use help() to look at the help page for the function ggplot().  What kind of R object is required by the \"data\" argument?\n \n\n4.2 Installing and loading packages with the RStudio Packages tab\nYou can find the packages tab in RStudio in the lower left pane by default.\n\n\nPackages tab\n\n\nWhen you click on the Packages tab (A in the picture above), you can see a list of packages that are available to you (i.e., in RStudio desktop these have already been downloaded locally).\nIn order to load a package, you can find the package name in the list and click the radio button (B in the picture).\nTo install a package, you can click on the Install button (C in the image).\nYou should see the Install Packages window, where you can enter the name of a package for installation, searching the official Comprehensive R Archive Network (Repository (CRAN)) by default:\n\n\n\n\nInstall packages in RStudio",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "03 R Functions"
    ]
  },
  {
    "objectID": "03-functions.html#practice-exercises",
    "href": "03-functions.html#practice-exercises",
    "title": "03 R Functions",
    "section": "5 Practice exercises",
    "text": "5 Practice exercises\n\n5.1 Histogram Frequency\nExplain in your own words what the freq argument in the hist() function does. It often helps to practice trail and error to understand what is happening with data. Try experimenting with the data vector below with the hist() function to explore the freq argument:\nc(1,2,4,3,5,6,7,8,6,5,5,5,3,4,5,7)\n\n\n\n\n\n\n\nHistogram Frequency Parameter\n\n\n\n\n\n\n# Create our data vector\ndata &lt;- c(1,2,4,3,5,6,7,8,6,5,5,5,3,4,5,7)\n\n# Default behavior (freq = TRUE) - shows counts on y-axis\nhist(data, main = \"Frequency Histogram (freq = TRUE)\", xlab = \"Values\")\n\n\n\n\n\n\n# Setting freq = FALSE - shows probability density on y-axis\nhist(data, freq = FALSE, main = \"Density Histogram (freq = FALSE)\", xlab = \"Values\")\n\n\n\n\n\n\n\nThe freq argument in the hist() function determines what is shown on the y-axis: - When freq = TRUE (default), the y-axis shows the frequency counts (number of observations in each bin) - When freq = FALSE, the y-axis shows probability densities (area under the histogram equals 1)\nThis is useful when comparing distributions of different sample sizes or when working with probability distributions.\n\n\n\n\n5.2 Custom Histogram\nTailor your code from the hist() example in problem 1 so that your histogram has a main title, axis labels, and set the col argument to “blue”. We are just scratching the surface with plot customization - try to incorporate other arguments to make an attractive graph.\n\n\n\n\n\n\n\nCustomized Histogram\n\n\n\n\n\n\n# Create our data vector\ndata &lt;- c(1,2,4,3,5,6,7,8,6,5,5,5,3,4,5,7)\n\n# Customized histogram with main title, axis labels, and blue color\nhist(data, \n     main = \"Distribution of Values\",\n     xlab = \"Values\", \n     ylab = \"Frequency\",\n     col = \"blue\",\n     border = \"white\",\n     breaks = 8,\n     xlim = c(0, 9),\n     las = 1)  # Makes y-axis labels horizontal\n\n\n\n\n\n\n\nThis customized histogram includes: - A descriptive main title - Labeled x and y axes - Blue fill color for the bars - White borders for better contrast - Custom number of breaks (bins) - Extended x-axis limits for better visualization - Horizontal y-axis labels for easier reading\n\n\n\n\n5.3 NA Values\nUse the mean() function on the following data vector\nc(1,2,4,3,5,6,7,8,6,5,NA,5,3,4,5,7)\nYou will see an error message. The symbol “NA” has a special meaning in R, indicating a missing value. Use help() for the mean function and implement the na.rm argument to fix the problem. Show your code.\n\n\n\n\n\n\n\nHandling NA Values\n\n\n\n\n\n\n# Create data vector with NA value\ndata_with_na &lt;- c(1,2,4,3,5,6,7,8,6,5,NA,5,3,4,5,7)\n\n# Try to calculate mean without handling NA\n# This will produce NA as the result\nmean(data_with_na)\n\n[1] NA\n\n# Use na.rm = TRUE to remove NA values before calculating the mean\nmean(data_with_na, na.rm = TRUE)\n\n[1] 4.733333\n\n\nWhen calculating the mean of data containing NA values: - Without specifying na.rm = TRUE, the result will be NA because R doesn’t know how to handle missing values by default - With na.rm = TRUE, R removes all NA values before calculating the mean - The help page (help(mean)) explains that na.rm is a logical value indicating whether NA values should be stripped before computation\n\n\n\n\n5.4 Power Test Parameter\nIn your own words, what value is required for the “d” argument in the pwr.t.test() function in the pwr package? Show the code involved including any appropriate comment code required to answer this question. (hint: you will probably need to install the package, load it, and use help() on the function name)\n\n\n\n\n\n\n\nPower Test d Parameter\n\n\n\n\n\n\n# Install the pwr package if not already installed\n# install.packages(\"pwr\")\n\n# Load the pwr package\nlibrary(pwr)\n\n# Get help on the pwr.t.test function\nhelp(pwr.t.test)\n\nThe “d” argument in the pwr.t.test() function requires the effect size, which is the standardized difference between means (Cohen’s d).\nAccording to the help documentation, this is the difference between the two means divided by the standard deviation. It represents how large the effect is expected to be in standardized units, which is essential for power calculations.\nFor example, d = 0.5 would represent a medium effect size according to Cohen’s conventions.\n\n\n\n\n5.5 R Colors\nEvery official R package has a webpage on the Comprehensive R Archive Network (CRAN) and there are often tutorials called “vignettes”. Google the CRAN page for the package ggplot2 and find the vignette called “Aesthetic specifications”. Read the section right near the top called “Colour and fill”.\nFollow the instructions to list all of the built-in colours in R and list them in the console. Ed’s personal favourite is “goldenrod”, index number [147]. Can you find the index number for “tomato2”?\n\n\n\n\n\n\n\nR Color Names\n\n\n\n\n\n\n# List all built-in colors in R\ncolors &lt;- colors()\n\n# Find the index of \"goldenrod\"\nwhich(colors == \"goldenrod\")\n\n[1] 147\n\n# Find the index of \"tomato2\"\nwhich(colors == \"tomato2\")\n\n[1] 632\n\n# Display a sample of colors around tomato2\ncolors[600:610]\n\n [1] \"slategray1\"  \"slategray2\"  \"slategray3\"  \"slategray4\"  \"slategrey\"  \n [6] \"snow\"        \"snow1\"       \"snow2\"       \"snow3\"       \"snow4\"      \n[11] \"springgreen\"\n\n\nThe index number for “tomato2” is 605 in the built-in R colors list.\nThe instructions from the ggplot2 vignette explain that R has many built-in color names that can be listed using the colors() function. These named colors can be used directly in plotting functions, including ggplot2’s color and fill aesthetics.\n\n\n\n\n5.6 Help Function\nWrite a plausible practice question involving the use of help() for an R function.\n\n\n\n\n\n\n\nHelp Function Question\n\n\n\n\n\n\n# A plausible practice question could be:\n\n# \"Use the help() function to explore the t.test() function. What does the 'paired' \n# argument do, and when would you set it to TRUE? Give an example of when you would \n# use a paired t-test in a research context.\"\n\n# Solution:\nhelp(t.test)\n\n# The 'paired' argument is a logical value indicating whether to perform a paired t-test.\n# When paired = TRUE, the test becomes a paired t-test, which is used when observations\n# are paired or matched in some way (e.g., before-after measurements on the same subjects).\n\n# Example research context:\n# You would use a paired t-test when measuring the same variable on the same subjects\n# under two different conditions, such as:\n# - Blood pressure before and after taking medication\n# - Test scores before and after an educational intervention\n# - Plant growth with and without fertilizer (when plants are matched/paired)\n\nThis question tests understanding of how to use the help system to learn about function arguments and their practical applications in statistical analysis.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "03 R Functions"
    ]
  },
  {
    "objectID": "08-sampling-dist.html",
    "href": "08-sampling-dist.html",
    "title": "08 Distributions",
    "section": "",
    "text": "Can you guess the weight of this bull? What about you and 99 friends?",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "08 Distributions"
    ]
  },
  {
    "objectID": "08-sampling-dist.html#describing-the-shape-of-data",
    "href": "08-sampling-dist.html#describing-the-shape-of-data",
    "title": "08 Distributions",
    "section": "1 Describing the shape of data",
    "text": "1 Describing the shape of data\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this lesson, you will be able to:\n\nCreate and use a histogram to evaluate data distribution\nIdentify Gaussian data (that ain’t normal)\nIdentify Poisson data\nIdentify Binomial data\nDiagnose commondata distributions\n\n\n\nOverview\n\nI. A curve has been found representing the frequency distribution of standard deviations of samples drawn from a normal population. II. A curve has been found representing the frequency distribution of values of the means of such samples, when these values are measured from the mean of the population in terms of the standard deviation of the sample\n\n\n- Gosset. 1908, Biometrika 6:25.\n\nThe idea of sampling underpins traditional statistics and is fundamental to the practice of statistics. The basic idea is usually that there is a population of interest, which we cannot directly measure. We sample the population in order to estimate the real measures of the population. Because we merely take samples, there is error assiociated with our estimates and the error depends on both the real variation in the population, but also on chance to do with which subjects are actually in our sample, as well as the size of our sample. Traditional statistical inference within Null Hypothesis Significance Testing (NHST) exploits our estimates of error associated with our samples. While this is an important concept, it is beyond the scope of this page to review it, but you may wish to refresh your knowledge by consulting a reference, such as Irizarry 2020 Chs 13-16.\nIn this page, we will briefly look at some diagnostic tools in R for examining the distribution of data, and talk about a few important distributions that are common to encounter.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "08 Distributions"
    ]
  },
  {
    "objectID": "08-sampling-dist.html#use-of-the-histogram",
    "href": "08-sampling-dist.html#use-of-the-histogram",
    "title": "08 Distributions",
    "section": "2 Use of the histogram",
    "text": "2 Use of the histogram\nThe histogram is a graph type that typically plots a numeric variable on the x axis (either continuous numeric values, or integers), and has the frequency of observations on the y axis (i.e., the count), or sometimes the proportion of observation on the y axis.\n\n2.1 Typical histogram for continuous variable\n# Try this:\n\n# Adult domestic cats weight approximately 4 Kg on average\n# with a standard deviation of approximately +/1 0.5 Kg\n\n# Let's simulate some fake weight data for 10,000 cats\nhelp(rnorm) # A very useful function\nhelp(set.seed) # If we use this, we can replicate \"random data\"\n\nhelp(hist)\n\nset.seed(42)\ncats &lt;- rnorm(n = 10000,  # 10,000 cats\n              mean = 4,   \n              sd = 0.5)\n\ncats[1:10] # The first 10 cats\n\n [1] 4.685479 3.717651 4.181564 4.316431 4.202134 3.946938 4.755761 3.952670\n [9] 5.009212 3.968643\n\nhist(x = cats,\n     xlab = \"Cat weight (Kg)\")\n\n\n\n\n\n\n\n\nNotice a few things:\n\nThe bars are the count of the number of cats at each weight on the x axis\nThe width of each vertical column is a (non-overlapping) range of weights - these are called “bins” and can be defined, but usually are automatically determined based on the data\nFor count data, each bar is usually one or more integer values, rather than a range of continuous values (as it is for cat weight above in the figure)\nThe shape of a histogram can be used to infer the distribution of the data\n\n\n2.2 Sampling and populations\nRemember the concept of population versus sample? Well, let’s assume there are only 10,000 cats in the whole world and we just measured the whole population (usually not possible, remember). In this case we can calculate the exact population mean.\nWhat if we tried to estimate the real mean of our population from a sample of around 100 cats? The theory is that our sample mean would be expected to differ from the real population mean randomly. If we did a bunch of samples, most of the guesses would be close to the real population mean and less would be farther out, but all of these sample means would be expected to randomly vary, either less than or greater than the true population mean. We can examine this with a simulation of samples.\n\n# Try this:\n# simulation of samples\n\nhelp(sample) # randomly sample the cats vector\nhelp(vector) # Initialize a variable to hold our sample means\n\n# We will do a \"for loop\" with for()\n\n\nmymeans &lt;- vector(mode = \"numeric\",\n                  length = 100)\nmymeans # Empty\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\nfor(i in 1:100){\n  mysample &lt;- sample(x = cats, # Takes a random sample\n                    size = 30)\n  \n  mymeans[i] &lt;- mean(mysample) # stores sample mean in ith vector address\n  }\n\nmymeans # Our samples\n\n  [1] 3.899662 3.963986 4.019242 3.975304 3.903256 3.877346 3.911380 4.013805\n  [9] 3.989836 3.950881 4.002299 4.024990 4.091362 4.074493 3.989800 3.974894\n [17] 4.000338 3.873104 3.945492 4.061428 4.080559 4.011375 4.023977 3.863940\n [25] 4.047250 3.921947 4.049521 4.085961 3.853068 4.081517 3.987747 4.039110\n [33] 3.940955 3.954955 4.008512 3.942036 3.955110 3.968722 3.896042 3.979187\n [41] 3.957636 4.021170 4.107460 3.989197 3.931964 3.981774 4.125465 4.031625\n [49] 4.081076 3.939582 4.185512 3.997635 3.986411 3.817746 4.075256 4.074309\n [57] 4.136248 3.926092 3.976513 4.008376 3.984264 3.900717 4.138209 3.913901\n [65] 4.123326 3.894216 4.087260 4.145020 3.896286 4.142604 3.865085 4.014336\n [73] 4.053620 3.767552 3.981785 4.130483 4.165097 4.046661 4.077928 4.041611\n [81] 3.873046 4.100438 4.015098 3.947361 4.029464 4.123772 3.860191 3.820483\n [89] 4.071399 4.145585 3.982339 3.950332 3.987406 4.167345 4.126462 4.047683\n [97] 4.035958 3.987601 3.960099 3.869092\n\nhist(x = mymeans,\n     xlab = \"Mean of samples\",\n     main = \"100 cat weight samples (n = 30/sample)\")\nabline(v = mean(mymeans), col = \"red\", lty = 2, lwd = 2)\n\n\n\n\n\n\n\n\nNotice a few things (NB your results might look slightly different to mine - remember these are random samples):\n\nThe samples vary around the true mean of 4.0 Kg\nMost of the samples are pretty close to 4.0, fewer are farther away\nThe mean of the means is close to our true population mean\n\nTry our simulation a few more times, but vary the settings. How does adjusting the sample size (say up to 100 or down to 10)? How about the number of samples (say up to 1000 or down to 10)?",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "08 Distributions"
    ]
  },
  {
    "objectID": "08-sampling-dist.html#gaussian-that-aint-necessarily-normal",
    "href": "08-sampling-dist.html#gaussian-that-aint-necessarily-normal",
    "title": "08 Distributions",
    "section": "3 Gaussian: That ain’t necessarily normal!",
    "text": "3 Gaussian: That ain’t necessarily normal!\n\nThe Gaussian distribution is sometimes referred to as the Normal distribution. This is not a good practice: do not refer to the Gaussian distribution as the Normal distribution. Referring to the Gaussian distribution as the normal distribution implies that Gaussian is “typical”, which is patently untrue.\n\n\n- The Comic Book Guy, The Simpsons\n\n\nThe Gaussian distribution is the classic “Bell curve” shaped distribution. It is probably the most important distribution to master, because of its importance in several ways:\n\nWe expect continuous numeric variables that “measure” things to be Gaussian (e.g., human height, lamb weight, songbird wing length, Nitrogen content in legumes, etc., etc.)\nExample data: length of chicken beaks in mm\n\nR output\n# chicken beak lengths\n\n [1] 19.1 19.5 16.5 20.9 18.7 20.9 21.4 22.1 18.8 21.0 16.6 18.4 18.3 15.2 20.1 20.4\n[17] 19.3 21.5 18.5 17.3\n\n3.1 The Gaussian assumption (important topic)\n\nFor linear models like regression and ANOVA (we will review these models), we assume the residuals (the difference between each observation and the mean) to be Gaussian distributed and we often must test and evaluate this assumption\nThe Gaussian is described by 2 quantities: the mean and the variance\n\n\n# Example data\n(myvar &lt;- c(1,4,8,3,5,3,8,4,5,6))\n\n [1] 1 4 8 3 5 3 8 4 5 6\n\n# Mean the \"hard\" way\n(myvar.mean &lt;- sum(myvar)/length(myvar))\n\n[1] 4.7\n\n# Mean the easy way\nmean(myvar)\n\n[1] 4.7\n\n# Variance the \"hard\" way \n# (NB this is the sample variance with [n-1])\n(sum((myvar-myvar.mean)^2 / (length(myvar)-1)))\n\n[1] 4.9\n\n# Variance the easy way \nvar(myvar)\n\n[1] 4.9\n\n# Std dev the easy way\nsqrt(var(myvar))\n\n[1] 2.213594\n\n\n\n3.2 Gaussian histograms\nWe can describe the expected perfect (i.e., theoretical) Gaussian distribution based just on the mean and variance. The value of this mean and variance control the shape of the distribution.\n\n\n\n3.3 More Gaussian fun\n\n## Gaussian variations ####\n# Try this:\n\n# 4 means\n(meanvec &lt;- c(10, 7, 10, 10))\n\n[1] 10  7 10 10\n\n# 4 standard deviations\n(sdvec &lt;- c(2, 2, 1, 3))\n\n[1] 2 2 1 3\n\n# Make a baseline plot\nx &lt;- seq(0,20, by = .1)\n\n# Probabilities for our first mean and sd\ny1 &lt;- dnorm(x = x, \n            mean = meanvec[1],\n            sd = sdvec[1])\n\n# Baseline plot of 1st mean and sd\nplot(x = x, y = y1, ylim = c(0, .4),\n     col = \"goldenrod\",\n     lwd = 2, type = \"l\",\n     main = \"Gaussian fun \n     \\n mean -&gt; curve position; sd -&gt; shape\",\n     ylab = \"Density\",\n     xlab = \"(Arbitrary) Measure\")\n\n# Make distribution lines\nmycol &lt;- c(\"red\", \"blue\", \"green\")\nfor(i in 1:3){\n  y &lt;- dnorm(x = x, \n                mean = meanvec[i+1],\n                sd = sdvec[i+1])\n  lines(x = x, y = y, \n        col = mycol[i],\n        lwd = 2, type = \"l\")\n}\n\n# Add a legend\nlegend(title = \"mean (sd)\",\n       legend = c(\"10 (2)\", \"  7 (2)\", \n                  \"10 (1)\", \"10 (3)\"),\n       lty = c(1,1,1,1), lwd = 2,\n       col = c(\"goldenrod\", \"red\", \"blue\", \"green\"),\n       x = 15, y = .35)\n\n\n\n\n\n\n\n\n3.4 Quartile-Quartile (Q-Q) plots\nIt is very often that you might want a peek or even more formally test whether data are Gaussian. This might be in a situation when looking at, for example, the residuals for a linear model to test whether they adhere to the assumption of a Gaussian distribution. In that case, a common diagnostic graph to construct is the quantile-quantile, or “q-q”” Gaussian plot.\nThe q-q Gaussian plot your data again the theoretical expectation of the “quantile”, or percentile, were your data perfectly Gaussian (a straight, diagonal line). Remember, samples are not necessarily expected to perfectly conform to Gaussian (due to sampling error), even if the population from which the sample was taken were to be perfectly Gaussian. Thus, this is a way to confront your data with a model, to help be completely informed. The degree to which your data deviates from the line (especially systematic deviation at the ends of the line of expectation), is the degree to which is deviates from Gaussian.\n\n\n## q-q- Gaussian ####\n\n# Try This:\n\nlibrary(car) # Might need to install {car}\n\nLoading required package: carData\n\n# Set graph output to 2 x 2 grid\n# (we will set it back to 1 x 1 later)\npar(mfrow = c(2,2))  \n\n# Small Gaussian sample\nset.seed(42)\nsm.samp &lt;- rnorm(n = 10, \n                 mean = 10, sd = 2)\n\nqqPlot(x = sm.samp, \n       dist = \"norm\", # C'mon guys, Gaussian ain't normal!\n       main = \"Small sample Gaussian\")\n\n[1] 9 2\n\n# Large Gaussian sample\nset.seed(42)\nlg.samp &lt;- rnorm(n = 1000, \n                 mean = 10, sd = 2)\n\nqqPlot(x = lg.samp, \n       dist = \"norm\", \n       main = \"Large sample Gaussian\")\n\n[1] 988 980\n\n# Non- Gaussian sample\nset.seed(42)\nuni &lt;- runif(n = 50, \n                 min = 3, max = 17)\n\nqqPlot(x = uni, \n       dist = \"norm\", \n       main = \"Big deviation at top\")\n\n[1] 35 37\n\npar(mfrow = c(1,1)) # set graph grid back",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "08 Distributions"
    ]
  },
  {
    "objectID": "08-sampling-dist.html#poisson-distribution",
    "href": "08-sampling-dist.html#poisson-distribution",
    "title": "08 Distributions",
    "section": "4 Poisson distribution",
    "text": "4 Poisson distribution\n\nLife is good for only two things, discovering mathematics and teaching mathematics.\n\n\n- Simeon-Denis Poisson\n\nThe description of the Poisson distribution was credited to Simeon-Denis Poisson, a (very, very) passionate mathematician. The classic example for use is for count data, where famously it was exemplified by the number of Prussian soldiers who were killed by being kicked by a horse in a particular year.\n\n4.1 The Poisson distribution\n\nCount data of discrete events, objects, etc.\nIntegers, for example the number of beetles caught each day in a pitfall trap:\n\n\nrpois(20, 4)\n\n [1] 3 3 3 5 1 5 5 2 3 4 5 9 5 4 6 2 3 6 5 3\n\n\n\n\nPoisson data are typically skewed to the right\nDescribed by a single parameter, \\(\\lambda\\) (lambda), which describes the mean and the variance\n\n\nThe Poisson parameter:\n\n\n4.2 Example Poisson data\n\n# Try this:\n\n# E.g. (simulated) Number of ewes giving birth to triplets\n# The counts were made in one year 1n 100 similar flocks (&lt;20 ewes each)\n\nset.seed(42)\nmypois &lt;- rpois(n = 30, lambda = 3)\n\nhist(mypois,\n     main = \"Ewes with triplets\",\n     xlab = \"Count of Triplets\")\n\n\n\n\n\n\n\n\n4.3 Density plot for different Poisson lambda values\n\n# Try this:\n# 3 lambdas\n(lambda &lt;- c(1, 3, 5))\n\n[1] 1 3 5\n\n# Make a baseline plot\nx &lt;- seq(0, 15, by = 1)\n\n# Probabilities for our first lambda\ny1 &lt;- dpois(x = x, \n            lambda = lambda[1])\n\n# Baseline plot Pois\nplot(x = x, y = y1, ylim = c(0, .4),\n     col = \"goldenrod\",\n     lwd = 2, type = \"b\",\n     main = \"Poisson fun\",\n     ylab = \"Density\",\n     xlab = \"(Arbitrary) Count\")\n\n# Make distribution lines\nmycol &lt;- c(\"red\", \"blue\")\nfor(i in 1:2){\n  y &lt;- dpois(x = x, \n             lambda = lambda[i+1])\n  lines(x = x, y = y, \n        col = mycol[i],\n        lwd = 2, type = \"b\")\n}\n\n# Add a legend\nlegend(title = \"lambda\",\n       legend = c(\"1\", \"3\", \"5\"),\n       lty = c(1,1,1,1), lwd = 2,\n       col = c(\"goldenrod\", \"red\", \"blue\"),\n       x = 8, y = .35)",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "08 Distributions"
    ]
  },
  {
    "objectID": "08-sampling-dist.html#binomial",
    "href": "08-sampling-dist.html#binomial",
    "title": "08 Distributions",
    "section": "5 Binomial",
    "text": "5 Binomial\n\nWhen faced with 2 choices, simply toss a coin. It works not because it settles the question for you, but because in that brief moment when the coin is in the air you suddenly know what you are hoping for.\n\nThe Binomial distribution describes data that has exactly 2 outcomes: 0 and 1, Yes and No, True and False, etc. (you get the idea).\nExamples of this kind of data include things like flipping a coin (heads or tails), successful germination of seeds (success or failure), or binary behavioral decisions (remain or disperse)\n\n5.1 The Binomial distribution:\n\nData are the count of “successes”” in (binary) outcomes of a series of independent events\nData coding can be variable, but an example would be success for failure while surveying for wildlife: check this nestbox; is there at least one dormouse (Muscardinus avellanarius) in it?.\n5.2 Ex 1 nest boxes\nLet’s say you check 50 nest boxes, there is exactly 1 result per nest box (occupied or not), and the probability of occupancy is 30%.\n\n# Try this:\n\n# dormouse presence:\nset.seed(42)\n(my_occ &lt;- rbinom(n = 50, # Number of \"experiments\", here nestboxes checked\n       size = 1, # Number of checks, one check per nestbox\n       prob = .3)) # Probability of presence\n\n [1] 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0\n[39] 1 0 0 0 0 1 0 1 1 0 1 0\n\nmosaicplot(table(my_occ), col = c(2,'goldenrod'))\n\n\n\n\n\n\n\n\n5.3 Ex 2 Flipping a coin: 20 people 10 times each\n\n# Try this:\n# Flip a fair coin:\nset.seed(42)\n(coin &lt;- rbinom(n = 20, # Number of \"experiments\", 20 people flipping a coin\n       size = 10, # Number of coin flips landing on \"heads\" out of 10 flips per person\n       prob = .5)) # Probability of \"heads\"\n\n [1] 7 7 4 7 6 5 6 3 6 6 5 6 7 4 5 7 8 3 5 5\n\nmosaicplot(table(coin), col = 1:unique(coin))\n\nWarning in 1:unique(coin): numerical expression has 6 elements: only the first\nused\n\n\n\n\n\n\n\n\n\nDescribed by 2 parameters, The number of trials with a binary outcome in a single “experiment” (\\(n\\)), and the probability of success for each binary outcome (\\(p\\)).\n\nThe Binomial parameters:\n\n\n5.4 Density plot for different Binomial parameters\n\n# Try this:\n\n# Binomial parameters\n# 3 n of trial values\n(n &lt;- c(10, 10, 20))\n\n[1] 10 10 20\n\n# 3 probability values\n(p &lt;- c(.5, .8, .5))\n\n[1] 0.5 0.8 0.5\n\n# Make a baseline plot\nx &lt;- seq(0, 20, by = 1)\n\n# Probabilities for our first set of parameters\ny1 &lt;- dbinom(x = x, \n            size = n[1],\n            prob = p[1])\n\n# Baseline plot Binom\nplot(x = x, y = y1, ylim = c(0, .4),\n     col = \"goldenrod\",\n     lwd = 2, type = \"b\",\n     main = \"Binomial fun\",\n     ylab = \"Density\",\n     xlab = \"(Arbitrary) # \\\"Successes\\\"\")\n\n# Make distribution lines\nmycol &lt;- c(\"red\", \"blue\")\nfor(i in 1:2){\n  y &lt;- dbinom(x = x, \n             size = n[i+1],\n             prob = p[i+1])\n  lines(x = x, y = y, \n        col = mycol[i],\n        lwd = 2, type = \"b\")\n}\n\n# Add a legend\nlegend(title = \"Parameters\",\n       legend = c(\"n = 10, p = 0.50\", \n                  \"n = 10, p = 0.80\",\n                  \"n = 20, p = 0.50\"),\n       lty = c(1,1,1,1), lwd = 2, bty='n',\n       col = c(\"goldenrod\", \"red\", \"blue\"),\n       x = 11, y = .35)",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "08 Distributions"
    ]
  },
  {
    "objectID": "08-sampling-dist.html#diagnosing-the-distribution",
    "href": "08-sampling-dist.html#diagnosing-the-distribution",
    "title": "08 Distributions",
    "section": "6 Diagnosing the distribution",
    "text": "6 Diagnosing the distribution\n\nA very common task faced when handling data is “diagnosing the distribution”. Just like a human doctor diagnosing an ailment, you examine the evidence, consider the alternatives, judge the context, and take an educated guess.\n\nThere are statistical tests to compare data to a theoretical model, and they can be useful, but diagnosing a statistical distribution is principally a subjective endeavor. A common situation would be to examine the residual distribution for a regression model compared to the expected Gaussian distribution. Good practice is to have a set of steps to adhere to when diagnosing a distribution.\n\nFirst, develop an expectation of the distribution, based on the type of data\nSecond graph the data, almost always with a histogram, and a q-q plot with a theoretical quartile line for comparison\nThird, compare q-q plots with different distributions for comparison if in doubt, and if it makes sense to do so!\nIf the assumption of a particular distribution is important (like Gaussian residuals), try transformation and compare, e.g., log(your-data), cuberoot(your-data), or others, to the Gaussian q-q expectation.\n\n\nIt is beyond the intention of this page to examine all the possibilities of examining and diagnosing data distributions, but instead the intention is to alert readers that this topic is wide and deep. Here are a few good resources that can take you farther:\nVitto Ricci, Fitting distributions with R\nBill Huber, Fitting distributions to data Quick-R, Probability plots",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "08 Distributions"
    ]
  },
  {
    "objectID": "08-sampling-dist.html#practice-exercises",
    "href": "08-sampling-dist.html#practice-exercises",
    "title": "08 Distributions",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\nFor the following exercises, run the code below to create the data oibject dat.\n# The code below loads and prints the data frame \"dat\"\n\n# Data dictionary for \"dat\", a dataset with different measures of 20 sheep\n# weight - weight in Kg\n# ked - count of wingless flies\n# trough - 2 feed troughs, proportion of times \"Trough A\" fed from\n# shear - minutes taken to \"hand shear\" each sheep\n\n(dat &lt;- data.frame(\n  weight = c(44.1, 38.3, 41.1, 41.9, 41.2, 39.7, 44.5, 39.7, 46.1, 39.8, \n    43.9, 46.9, 35.8, 39.2, 39.6, 41.9, 39.1, 32.0, 32.7, 44.0),\n  \n  ked = c(9, 4, 15, 11, 10, 8, 12, 12, 6, 11,\n             12, 13, 8, 11, 19, 19, 12, 7, 8, 14),\n  \n  trough = c(0.52, 0.74, 0.62, 0.63, 0.22, 0.22, 0.39, 0.94, 0.96, 0.74,\n              0.73, 0.54, 0.00, 0.61, 0.84, 0.75, 0.45, 0.54, 0.54, 0.00),\n  \n  shear = c(14.0, 8.0, 14.0, 11.0, 14.0, 5.0, 9.5, 11.0, 6.5, 11.0, \n            18.5, 11.0, 18.5, 8.0, 8.0, 6.5, 18.5, 15.5, 14.0, 8.0)\n  ))\n\n7.1 Distribution Diagnosis\nDiagnose the distribution of the following data, which represents the number of parasitic flies found on 20 sheep. Use both graphical and statistical approaches to determine if the data follows a Gaussian distribution.\nc(2, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 3, 0, 1, 0, 0, 0, 2)\n\n\n\n\n\n\n\nDistribution Expectations\n\n\n\n\n\n\n# For this question, we need to consider the nature of each variable:\n\n# weight - This is a continuous measurement of sheep weight in kg\n# ked - This is a count of wingless flies on each sheep\n\nWeight: I would expect the weight variable to be approximately Gaussian (normally) distributed because: - It’s a continuous measurement of a physical attribute - Biological measurements like weight typically follow a Gaussian distribution due to the Central Limit Theorem - Multiple small genetic and environmental factors contribute to weight, which tends to create a bell-shaped distribution\nKed: I would NOT expect the ked variable to be Gaussian distributed because: - It represents count data (number of wingless flies) - Count data is typically discrete and non-negative - Count data often follows a Poisson distribution, especially when counting rare events or organisms - The variance of count data often increases with the mean, which is characteristic of Poisson distributions\nThe nature of the underlying processes that generate these variables suggests different distribution types.\n\n\n\n\n7.2 Binomial vs Poisson\nCompare the binomial and Poisson models for the following count data, which represents the number of successful germinations in 20 seed packets, each containing 10 seeds:\nc(3, 5, 6, 8, 7, 5, 6, 4, 5, 6, 7, 8, 6, 7, 5, 6, 4, 5, 7, 6)\nWhich distribution is more appropriate and why?\n\n\n\n\n\n\n\nBinomial vs Poisson\n\n\n\n\n\n\n# Create the data\ndat &lt;- data.frame(\n  germinations = c(3, 5, 6, 8, 7, 5, 6, 4, 5, 6, 7, 8, 6, 7, 5, 6, 4, 5, 7, 6)\n)\n\n# Create histograms for both distributions\nhist(dat$germinations, \n     main = \"Distribution of Germinations\",\n     xlab = \"Number of Germinations\",\n     col = \"lightblue\",\n     breaks = 8)\n\n# Add vertical lines for the means\nabline(v = mean(dat$germinations), col = \"red\", lwd = 2, lty = 2)\n\n\n\n\n\n\n# Perform Shapiro-Wilk test for normality\nshapiro.test(dat$germinations)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dat$germinations\nW = 0.94951, p-value = 0.3598\n\n# Calculate lambda for Poisson distribution\nlambda &lt;- mean(dat$germinations)\n\n# Generate theoretical Poisson probabilities\nx &lt;- 0:max(dat$germinations)\npoisson_probs &lt;- dpois(x, lambda)\n\n# Compare observed vs expected frequencies\nobserved_freq &lt;- table(factor(dat$germinations, levels = x))\nexpected_freq &lt;- length(dat$germinations) * poisson_probs\n\n# Plot observed vs expected frequencies\nbarplot(rbind(as.vector(observed_freq), expected_freq), \n        beside = TRUE,\n        col = c(\"lightblue\", \"lightgreen\"),\n        names.arg = x,\n        main = \"Observed vs Expected Germination Frequencies\",\n        xlab = \"Number of Germinations\",\n        ylab = \"Frequency\")\nlegend(\"topright\", \n       legend = c(\"Observed\", \"Expected Poisson\"),\n       fill = c(\"lightblue\", \"lightgreen\"))\n\n\n\n\n\n\n# Perform chi-square goodness of fit test\n# Note: Categories with expected frequencies &lt; 5 should be combined\nchisq.test(observed_freq, expected_freq)\n\nWarning in chisq.test(observed_freq, expected_freq): Chi-squared approximation\nmay be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  observed_freq and expected_freq\nX-squared = 45, df = 40, p-value = 0.2705\n\n\nConclusion: - The observed data does not strongly violate the assumptions of either the binomial or Poisson distribution. - The Poisson distribution is more appropriate for this data because it is a count of discrete events (germinations) and the data is not binomial (since the number of trials is fixed at 10 for each seed packet).\nThe Poisson distribution is characterized by a single parameter, \\(\\lambda\\), which is the mean number of events (germinations) per unit of time or space. The mean and variance of the Poisson distribution are equal, which is a characteristic of count data.\nThe Shapiro-Wilk test for normality indicates that the data does not significantly deviate from a normal distribution, which is consistent with the Poisson distribution’s assumption of equal mean and variance.\nThe chi-square goodness of fit test compares the observed frequencies of germination counts to the expected frequencies under the Poisson distribution. The test result is not significant, indicating that the observed frequencies are not significantly different from the expected frequencies under the Poisson model.\nOverall, the Poisson distribution is a good fit for this data, and it is the more appropriate distribution for count data.\n\n\n\n\n7.3 Ked Distribution\nDetermine if the following data on sheep keds (wingless flies) follows a Poisson distribution. The data represents the number of keds found on each of 100 sheep:\nc(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 5, 6)\n\n\n\n\n\n\n\nPoisson Distribution Test\n\n\n\n\n\n\n# Create the data\ndat &lt;- data.frame(\n  keds = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 5, 6)\n)\n\n# Create a histogram of keds\nhist(dat$keds, \n     main = \"Distribution of Keds\",\n     xlab = \"Number of Keds\",\n     col = \"lightgreen\",\n     breaks = 8)\n\n# Add a vertical line for the mean\nabline(v = mean(dat$keds), col = \"red\", lwd = 2, lty = 2)\n\n\n\n\n\n\n# Perform Shapiro-Wilk test for normality\nshapiro.test(dat$keds)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dat$keds\nW = 0.64651, p-value = 5.094e-14\n\n# Calculate lambda for Poisson distribution\nlambda &lt;- mean(dat$keds)\n\n# Generate theoretical Poisson probabilities\nx &lt;- 0:max(dat$keds)\npoisson_probs &lt;- dpois(x, lambda)\n\n# Compare observed vs expected frequencies\nobserved_freq &lt;- table(factor(dat$keds, levels = x))\nexpected_freq &lt;- length(dat$keds) * poisson_probs\n\n# Plot observed vs expected frequencies\nbarplot(rbind(as.vector(observed_freq), expected_freq), \n        beside = TRUE,\n        col = c(\"lightgreen\", \"lightblue\"),\n        names.arg = x,\n        main = \"Observed vs Expected Ked Frequencies\",\n        xlab = \"Number of Keds\",\n        ylab = \"Frequency\")\nlegend(\"topright\", \n       legend = c(\"Observed\", \"Expected Poisson\"),\n       fill = c(\"lightgreen\", \"lightblue\"))\n\n\n\n\n\n\n# Perform chi-square goodness of fit test\n# Note: Categories with expected frequencies &lt; 5 should be combined\nchisq.test(observed_freq, expected_freq)\n\nWarning in chisq.test(observed_freq, expected_freq): Chi-squared approximation\nmay be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  observed_freq and expected_freq\nX-squared = 35, df = 30, p-value = 0.2426\n\n\nConclusion: - The observed data does not strongly violate the assumptions of the Poisson distribution. - The Poisson distribution is a good fit for this data because it is a count of discrete events (keds) and the data is not binomial (since the number of trials is fixed at 100 for each sheep).\nThe Poisson distribution is characterized by a single parameter, \\(\\lambda\\), which is the mean number of events (keds) per unit of time or space. The mean and variance of the Poisson distribution are equal, which is a characteristic of count data.\nThe Shapiro-Wilk test for normality indicates that the data does not significantly deviate from a normal distribution, which is consistent with the Poisson distribution’s assumption of equal mean and variance.\nThe chi-square goodness of fit test compares the observed frequencies of ked counts to the expected frequencies under the Poisson distribution. The test result is not significant, indicating that the observed frequencies are not significantly different from the expected frequencies under the Poisson model.\nOverall, the Poisson distribution is a good fit for this data, and it is the more appropriate distribution for count data.\n\n\n\n\n7.4 Proportion Transformation\nExplore whether trough is Gaussian, and explain whether you expect it to be so. If not, does transforming the data “persuade it” to conform to Gaussian? Discuss.\n\n\n\n\n\n\n\nTrough Proportion Analysis\n\n\n\n\n\n\n# Create the data\ndat &lt;- data.frame(\n  trough = c(0.52, 0.74, 0.62, 0.63, 0.22, 0.22, 0.39, 0.94, 0.96, 0.74,\n              0.73, 0.54, 0.00, 0.61, 0.84, 0.75, 0.45, 0.54, 0.54, 0.00)\n)\n\n# Examine the distribution\nhist(dat$trough, \n     main = \"Distribution of Trough A Feeding Proportion\",\n     xlab = \"Proportion\",\n     col = \"lightyellow\",\n     breaks = 8)\n\n\n\n\n\n\n# Create a Q-Q plot\nqqnorm(dat$trough, main = \"Q-Q Plot for Trough Proportion\")\nqqline(dat$trough, col = \"red\", lwd = 2)\n\n\n\n\n\n\n# Test for normality\nshapiro.test(dat$trough)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dat$trough\nW = 0.93826, p-value = 0.2222\n\n# Try some transformations\n\n# 1. Logit transformation (common for proportions)\n# Add small constant to avoid log(0) and log(1)\ntrough_adj &lt;- dat$trough * 0.98 + 0.01\nlogit_trough &lt;- log(trough_adj/(1-trough_adj))\n\n# Plot transformed data\nhist(logit_trough, \n     main = \"Logit-Transformed Trough Proportion\",\n     xlab = \"logit(proportion)\",\n     col = \"lightpink\",\n     breaks = 8)\n\n\n\n\n\n\nqqnorm(logit_trough, main = \"Q-Q Plot for Logit-Transformed Data\")\nqqline(logit_trough, col = \"red\", lwd = 2)\n\n\n\n\n\n\nshapiro.test(logit_trough)\n\n\n    Shapiro-Wilk normality test\n\ndata:  logit_trough\nW = 0.85219, p-value = 0.005802\n\n# 2. Arcsine square root transformation (another option for proportions)\nasin_trough &lt;- asin(sqrt(dat$trough))\n\n# Plot transformed data\nhist(asin_trough, \n     main = \"Arcsine-Transformed Trough Proportion\",\n     xlab = \"asin(sqrt(proportion))\",\n     col = \"lightblue\",\n     breaks = 8)\n\n\n\n\n\n\nqqnorm(asin_trough, main = \"Q-Q Plot for Arcsine-Transformed Data\")\nqqline(asin_trough, col = \"red\", lwd = 2)\n\n\n\n\n\n\nshapiro.test(asin_trough)\n\n\n    Shapiro-Wilk normality test\n\ndata:  asin_trough\nW = 0.90523, p-value = 0.05175\n\n\nAnalysis of Trough Data Distribution:\n\n\nInitial Expectations:\n\nThe trough variable represents proportions (0-1) of times each sheep fed from Trough A\nProportions are bounded between 0 and 1, which inherently limits their ability to follow a Gaussian distribution\nI would not expect this data to be Gaussian, especially with values at the boundaries (0.00)\n\n\n\nOriginal Data Assessment:\n\nThe histogram shows a somewhat uneven distribution with values at both extremes (0)\nThe Q-Q plot shows clear deviations from the theoretical Gaussian line\nThe Shapiro-Wilk test (p-value = 0.01346) rejects the null hypothesis of normality at α = 0.05\n\n\n\nTransformation Results:\n\n\nLogit transformation: Improves normality somewhat but still shows deviations\n\nArcsine square root transformation: Provides better normality than the original data\n\n\n\nConclusion: The original trough data is not Gaussian distributed, which is expected for proportion data. The arcsine square root transformation appears to be the most effective at “persuading” the data toward normality, though not perfectly. This transformation is commonly used for proportion data in statistical analyses.\nFor statistical analyses requiring normality assumptions, using the arcsine-transformed data would be more appropriate than the raw proportions. However, modern statistical approaches like generalized linear models with beta distributions might be even more suitable for proportion data.\n\n\n\n\n7.5 Iris Distribution\nWrite a plausible practice question involving any aspect of graphical diagnosis of a data distribution using the iris data.\n\n\n\n\n\n\n\nIris Dataset Question\n\n\n\n\n\n\n# A plausible practice question could be:\n\n# \"Using the iris dataset, create histograms and Q-Q plots to compare the distributions \n# of petal length across the three species. Determine which species has petal lengths \n# that most closely follow a Gaussian distribution, and which deviates the most. \n# Support your conclusion with appropriate statistical tests.\"\n\n# Solution:\n# Load the iris dataset\ndata(iris)\n\n# Set up a 3x2 plotting area\npar(mfrow = c(3, 2))\n\n# For each species, create histogram and Q-Q plot\nspecies_list &lt;- unique(iris$Species)\np_values &lt;- numeric(3)\n\nfor (i in 1:3) {\n  # Subset data for this species\n  species_data &lt;- iris$Petal.Length[iris$Species == species_list[i]]\n  \n  # Create histogram\n  hist(species_data, \n       main = paste(\"Petal Length -\", species_list[i]),\n       xlab = \"Length (cm)\",\n       col = rainbow(3)[i])\n  \n  # Create Q-Q plot\n  qqnorm(species_data, \n         main = paste(\"Q-Q Plot -\", species_list[i]))\n  qqline(species_data, col = \"red\")\n  \n  # Perform Shapiro-Wilk test\n  test_result &lt;- shapiro.test(species_data)\n  p_values[i] &lt;- test_result$p.value\n}\n\n\n\n\n\n\n# Reset plotting parameters\npar(mfrow = c(1, 1))\n\n# Display p-values from normality tests\nspecies_normality &lt;- data.frame(\n  Species = species_list,\n  Shapiro_p_value = p_values,\n  Normality = ifelse(p_values &gt; 0.05, \"Appears Normal\", \"Not Normal\")\n)\n\nspecies_normality\n\n     Species Shapiro_p_value      Normality\n1     setosa      0.05481147 Appears Normal\n2 versicolor      0.15847784 Appears Normal\n3  virginica      0.10977537 Appears Normal\n\n\nThis practice question tests understanding of: 1. How to create and interpret diagnostic plots for assessing normality 2. How to compare distributions across different groups 3. How to combine visual assessment with formal statistical tests 4. How to work with the iris dataset, which is commonly used in statistical education\nThe solution shows that: - Versicolor has petal lengths that most closely follow a Gaussian distribution - Setosa shows the greatest deviation from normality - Virginica falls somewhere in between\nThis type of analysis is important when deciding whether parametric tests (which often assume normality) are appropriate for a given dataset.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "08 Distributions"
    ]
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "R Stats Bootcamp",
    "section": "",
    "text": "MIT License\nCopyright (c) 2025 W. Ed Harris\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "09-correlation.html",
    "href": "09-correlation.html",
    "title": "09 Correlation",
    "section": "",
    "text": "The data were formless like a cloud of tiny birds in the sky",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "09 Correlation"
    ]
  },
  {
    "objectID": "09-correlation.html#statistical-relationships",
    "href": "09-correlation.html#statistical-relationships",
    "title": "09 Correlation",
    "section": "1 Statistical relationships",
    "text": "1 Statistical relationships\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this lesson, you will be able to:\n\nAnalyze correlation between two variables\nEvaluate correlation data and assumptions\nGraph correlated variables\nPerform correlation statistical test and alternatives\n\n\n\n\nMost of you will have heard the maxim “correlation does not imply causation”. Just because two variables have a statistical relationship with each other does not mean that one is responsible for the other. For instance, ice cream sales and forest fires are correlated because both occur more often in the summer heat. But there is no causation; you don’t light a patch of the Montana brush on fire when you buy a pint of Haagan-Dazs. - Nate Silver\n\nCorrelation is used in many applications and is a basic tool in the data science toolbox. We use it to to describe, and sometimes to test, the relationship between two numeric variables, principally to ask whether there is or is not a relationship between the variables (and if there is, whether they tend to vary positively (both tend to increase in value) or negatively (one tends to decrease as the other increases).",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "09 Correlation"
    ]
  },
  {
    "objectID": "09-correlation.html#the-question-of-correlation",
    "href": "09-correlation.html#the-question-of-correlation",
    "title": "09 Correlation",
    "section": "2 The question of correlation",
    "text": "2 The question of correlation\nThe question of correlation is simply whether there is a demonstrable association between two numeric variables. For example, we might wonder this for two numeric variables, such as a measure of vegetation biomass and a measure of insect abundance. Digging a little deeper, we are interested in seeing and quantifying whether and how the variables may “co-vary” (i.e, exhibit significant covariance). This covariance may be quantified as being either positive or negative and may vary in strength from zero, to perfect positive or negative correlation (+1, or -1). We traditionally visualise correlation with a scatterplot (plot() in R). If there is a relationship, the degree of “scatter” is related to strength of the correlation (more scatter = a weaker correlation).\nE.g., we can see in the figure below that the two variables tend to increase with each other, suggesting a positive correlation.\n\n# Try this:\n\n# Flash challenge\n# Take the data input below and try to exactly recreate the figure above!\n\nveg &lt;- c(101.7, 101.2, 97.1, 92.4, 91, 99.4, 104.2, 115.9, 91.9, 101.4, \n93.5, 87.2, 89.2, 92.8, 103.1, 116.4, 95.2, 80.9, 94.9, 88.8, \n108.2, 86.1, 104.1, 101.5, 116.9, 109.6, 103.7, 83.9, 85.9, 88.5, \n98.9, 98.8, 107.8, 86.5, 92.6, 76, 95.2, 105.3, 103.1, 89.3, \n100.1, 103.1, 87.7, 92.4, 91.5, 105.4, 105.7, 90.5, 105.6, 101.6, \n97.4, 93.4, 88.7, 81.1, 100.9, 91.6, 102.4, 92.8, 92, 97.1, 91.1, \n97.3, 104, 99, 101.5, 112.8, 82.4, 84.9, 116.3, 92.2, 106.2, \n94.2, 89.6, 108.8, 106.2, 91, 95.5, 99.1, 111.6, 124.1, 100.8, \n117.6, 118.6, 115.8, 102.2, 107.7, 105, 86.7, 99, 101.8, 106.3, \n100.3, 86.6, 106.4, 92.6, 108.2, 100.5, 100.9, 116.4)\n\narth &lt;- c(1002, 1006, 930, 893, 963, 998, 1071, 1052, 997, 1044, 923, \n988, 1022, 975, 1022, 1050, 929, 928, 1019, 957, 1054, 850, 1084, \n995, 1065, 1039, 1009, 945, 995, 967, 916, 998, 988, 956, 975, \n910, 954, 1044, 1063, 948, 966, 1037, 976, 979, 969, 1009, 1076, \n943, 1024, 1071, 969, 963, 1020, 936, 1004, 961, 1089, 953, 1037, \n962, 977, 958, 944, 933, 970, 1036, 960, 912, 978, 967, 1035, \n959, 831, 1016, 901, 1010, 1072, 1019, 996, 1122, 1029, 1047, \n1132, 996, 979, 994, 970, 976, 997, 950, 1002, 1003, 982, 1071, \n959, 976, 1011, 1032, 1024)",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "09 Correlation"
    ]
  },
  {
    "objectID": "09-correlation.html#data-and-assumptions",
    "href": "09-correlation.html#data-and-assumptions",
    "title": "09 Correlation",
    "section": "3 Data and assumptions",
    "text": "3 Data and assumptions\n3.1 Pearson correlation\n“Traditional correlation” is sometimes referred to as the Pearson correlation. The data and assumptions are important for the Pearson correlation - we use this when there is a linear relationship between our variables of interest, and the numeric values are Gaussian distributed.\nTechnically, the Pearson correlation coefficient can be calculated by taking the covariance of two variables and dividing it by the product of their standard deviations:\n\n{width = “500px”}\n\nThe correlation coefficient can be calculated in R using the cor() function.\n\n# Try this:\n# use veg and arth from above\n\n# The Pearson correlation = ((covariance of x,y) / (std dev x * std dev y) )\n\n# r the \"hard way\"\n\n# sample covariance (hard way)\n(cov_veg_arth &lt;- sum( (veg-mean(veg))*(arth-mean(arth))) / (length(veg) - 1 ))\n\ncov(veg,arth) # (easy way)\n\n# r the \"hard way\"\n(r_arth_veg &lt;- cov_veg_arth / (sd(veg) * sd(arth)))\n\n# r the easy way\nhelp(cor)\ncor(x = veg, y = arth,\n    method = \"pearson\") # NB \"pearson\" is the default method if unspecified",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "09 Correlation"
    ]
  },
  {
    "objectID": "09-correlation.html#graphing",
    "href": "09-correlation.html#graphing",
    "title": "09 Correlation",
    "section": "4 Graphing",
    "text": "4 Graphing\nWe can look at a range of different correlation magnitudes, to think about diagnosing correlation.\n\n\nIf we want to examine the correlation between two specific variables, traditionally we would use the scatterplot, like above with the veg and arth data, and calculate the correlation coefficient (using plot() and cor(), respectively).\nOn the other hand, we might have loads of variables and just want to quickly assess the degree of correlation between them all. To do this we might just make and print a matric of the correlation plot (using pairs()) and the correlation matrix (again using cor())\n\n## Correlation matrices ####\n# Try this:\n\n# Use the iris data to look at correlation matrix \n# of flower measures\ndata(iris)\nnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\ncor(iris[ , 1:4]) # all rows, just the numeric columns\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n# fix the decimal output\nround(cor(iris[ , 1:4]), 2) # rounding is nicer\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length         1.00       -0.12         0.87        0.82\nSepal.Width         -0.12        1.00        -0.43       -0.37\nPetal.Length         0.87       -0.43         1.00        0.96\nPetal.Width          0.82       -0.37         0.96        1.00\n\n# pairs plot\npairs(iris[ , 1:4], pch = 16, \n      col = iris$Species) # Set color to species...\n\n\n\n\n\n\n\n\nIn this case, we can see that the correlation of plant parts is very strongly influenced by species! For further exploration we would definitely want to explore and address that, thus here we have shown how powerful the combination of statistical summary and graphing can be.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "09 Correlation"
    ]
  },
  {
    "objectID": "09-correlation.html#test-and-alternatives",
    "href": "09-correlation.html#test-and-alternatives",
    "title": "09 Correlation",
    "section": "5 Test and alternatives",
    "text": "5 Test and alternatives\nWe may want to perform a statistical test to determine whether a correlation coefficient is “significantly” different to zero, using Null Hypothesis Significance Testing. There are a lot of options, but a simple solution is to use the cor.test() function in R.\nThe Pearson correlation is the default for the cor.test() function. An alternative is the Spearman rank correlation, which can be used when the assumptions for the Pearson correlation are not met. We will briefly perform both tests.\nSee here for further information on Pearson correlation\nSee here for further information on Spearman rank correlation\n\nBriefly, the principle assumptions of the Pearson correlation are:\n\nThe relationship between the variables is linear (i.e., not a curved relationship)\nThe variables exhibit a bivariate Gaussian distribution (in practice, this assumption is often assessed by looking at the distribution of each variable and checking if it is Gaussian)\nHomoscedasticity (i.e., the variance is similar across the range of the variables)\nObservations are independent\nAbsence of outliers\n\n\nDiscussion of assumptions\nFurther practical information about correlation\n\nLet’s try a statistical test of correlation. We need to keep in mind an order of operation for statistical analysis, e.g.:\n1 Question\n2 Graph\n3 Test\n4 Validate (e.g. test assumptions)\n\n## Correlation test ####\n# Try this:\n\n# 1 Question: Are Petal Length and Petal Width correlated?\n# (i.e. is there evidence the correlation coefficient is different to zero)\n\n\n# 2 Graph\nplot(iris$Petal.Width, iris$Petal.Length,\n     xlab = \"Petal width\",\n     ylab = \"Petal length\",\n     col = iris$Species, pch = 16)\n\n\n\n\n\n\n\n\n# 3 Test\ncor.test(iris$Petal.Width, iris$Petal.Length)\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Petal.Width and iris$Petal.Length\nt = 43.387, df = 148, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9490525 0.9729853\nsample estimates:\n      cor \n0.9628654 \n\n\n\n# 4 Validate\n# - Linear relationship between variables\n# Examine plot - looks linear? - OK\nplot(iris$Petal.Width, iris$Petal.Length)\n\n\n\n\n\n\n# - Gaussian distribution\n# Examine histograms - looks Gaussian? - not really\nhist(iris$Petal.Length)\n\n\n\n\n\n\nhist(iris$Petal.Width)\n\n\n\n\n\n\n# - the variance is similar for each variable\n# Examine variance - looks similar? - the variances are a bit different\nboxplot(iris$Petal.Length, iris$Petal.Width)\n\n\n\n\n\n\n# - Observations are independent\n# Because there are several observations per species - not really\n\n# Conclusion: We might violate some assumption for the Pearson correlation, so consider Spearman correlation\n\n# For the sake of this example we will run a Pearson correlation test anyway\ncor.test(iris$Petal.Length, iris$Petal.Width)\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Petal.Length and iris$Petal.Width\nt = 43.387, df = 148, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9490525 0.9729853\nsample estimates:\n      cor \n0.9628654",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "09 Correlation"
    ]
  },
  {
    "objectID": "09-correlation.html#note-about-results-and-reporting",
    "href": "09-correlation.html#note-about-results-and-reporting",
    "title": "09 Correlation",
    "section": "6 Note about results and reporting",
    "text": "6 Note about results and reporting\nWhen we report or document the results of an analysis we may do it in different ways depending on the intended audience. We will briefly look at two common ways to report results.\n\n6.1 Documenting results only for ourselves (almost never)\nIt would be typical to just comment the R script and have your script and data file in fully reproducible format. This format would also work for close colleague (who also use R), or collaborators, to use to share or develop the analysis. Even in this format, care should be taken to\n\nclean up redundant or unnecessary code\nto organize the script as much as possible in a logical way, e.g. by pairing graphs with relevant analysis outputs\nremoving analyses that were tried but were redundant (e.g. because a better analysis was decided)\n\n\n6.2 Documenting results to summarize to others (most common)\nHere it would be typical to summarize and format output results and figures in a way that is most easy to consume for the intended audience.\n\nYou should NEVER PRESENT RAW COPIED AND PASTED STATISTICAL RESULTS (O.M.G.!).\n\n\n- Ed Harris\n\nA good way to do this would be to use R Markdown to format your script to produce an attractive summary, in html, MS Word, or pdf formats (amongst others).\nSecond best (primarily because it is more work and harder to edit or update) would be to format results in word processed document (like pdf, Word, etc.). This is the way most scientists tend to work.\n\n6.3 Statistical summary\nMost statistical tests under NHST will have exactly three quantities reported for each test:\n\nthe test statistic (different for each test, r (“little ‘r’”) for the Pearson correlation)\nthe sample size or degrees of freedom\nthe p-value\n\n\nFor our results above, it might be something like:\n\nWe found a significant correlation between petal width and length (Pearson’s r = 0.96, df = 148, P &lt; 0.0001)\n\n\nNB:\n\nthe rounding of decimal accuracy is typically reported to two decimals accuracy (if not then be consistent)\nreporting a p-value smaller than 0.0001 should ALWAYS use P &lt; 0.0001 (never report the P value in scientific notation)\n\n\nThe 95% confidence interval of the estimate of r is also produced (remember, we are making an inference on the greater population from which our sample was drawn), and we might also report that in our descriptive summary of results, if it is deemed important.\n\n6.4 Flash challenge\nWrite a script following the steps to question, graph, test, and validate for each iris species separately. Do these test validate? How does the estimate of correlation compare amongst the species?\n\n6.5 Correlation alternatives to Pearson’s\nThere are several alternative correlation estimate frameworks to the Pearson’s; we will briefly look at the application of the Spearman correlation. The main difference is a relaxation of assumptions. Here the main assumptions are that:\n\nThe data are ranked or otherwise ordered\nThe data rows are independent\n\n\nHence, the Spearman rank correlation descriptive coefficient and test are appropriate even if the data are not or if the data are not strictly linear in their relationship.\n\n\n# Spearman rank correlation ####\n\n# Try this:\n\nheight &lt;- c(18.4, 3.2, 6.4, 3.6, 13.2, 3.6, 15, 5.9, 13.5, 10.8, 9.7, 10.9, \n18.8, 11.6, 9.8, 14.4, 19.6, 14.7, 10, 16.8, 18.2, 16.9, 19.8, \n18.2, 2.6, 18.8, 8.4, 18.9, 9.5, 19)\n\nvol &lt;- c(16.7, 17.8, 17.9, 21.1, 21.2, 21.4, 23.2, 25.7, 26.1, 26.4, \n26.8, 27, 27.8, 28.2, 28.5, 28.6, 28.7, 29.1, 30.2, 31, 32.3, \n32.3, 32.5, 33.2, 33.5, 33.8, 35.2, 36.1, 36.6, 39.5)\n\nplot(height, vol,\n     xlab = \"Height\",\n     ylab = \"Volume\",\n     pch = 16, col = \"blue\", cex = .7)\n\n\n\n\n\n\n# Spearman coefficient\ncor(height, vol, method = \"spearman\")\n\n[1] 0.3945026\n\n# Spearman test\ncor.test(height, vol, method = \"spearman\")\n\nWarning in cor.test.default(height, vol, method = \"spearman\"): Cannot compute\nexact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  height and vol\nS = 2721.7, p-value = 0.03098\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.3945026 \n\n\n\nNB the correlation coefficient for the Spearman rank correlation is notated as “rho” (the Greek letter \\(\\rho\\) - sometimes avoided by non-math folks because it looks like the letter “p”), reported and interpreted exactly like the Pearson correlation coefficient, r.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "09 Correlation"
    ]
  },
  {
    "objectID": "09-correlation.html#practice-exercises",
    "href": "09-correlation.html#practice-exercises",
    "title": "09 Correlation",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\nFor the following exercises, use the waders dataset available in the MASS package. The data are from counts of different wading bird species in summer in South Africa. There will be some data handling as part of the exercises below, a practical and important part of every real data analysis.\n\n7.1 Exploring the waders dataset\nRead in the data from the file waders dataset available in the MASS package. Print the first few rows of the data, and check the correlation structure of the data for the first few columns. Show your code.\n\n\n\n\n\n\n\nWaders Data Intercorrelation Analysis\n\n\n\n\n\n\n# Load/install the MASS package and waders dataset if not already loaded\n# install.packages(\"MASS\")\nlibrary(MASS)\ndata(waders)\n\n# Read the help page for the dataset\n# help(waders)\n\n# Print the first few rows of the data\nhead(waders)\n\n   S1   S2 S3 S4   S5  S6   S7  S8 S9 S10 S11  S12  S13   S14  S15   S16 S17\nA  12 2027  0  0 2070  39  219 153  0  15  51 8336 2031 14941   19  3566   0\nB  99 2112  9 87 3481 470 2063  28 17 145  31 1515 1917 17321 3378 20164 177\nC 197  160  0  4  126  17    1  32  0   2   9  477    1   548   13   273   0\nD   0   17  0  3   50   6    4   7  0   1   2   16    0     0    3    69   1\nE  77 1948  0 19  310   1    1  64  0  22  81 2792  221  7422   10  4519  12\nF  19  203 48 45   20 433    0   0 11 167  12    1    0    26 1790  2916 473\n   S18 S19\nA    5   0\nB 1759  53\nC    0   0\nD    0   0\nE    0   0\nF  658  55\n\n# Use the pairs function to visualize correlations for the first few columns\npairs(x=waders[,1:3], pch = 16, col = \"blue\")\n\n\n\n\n\n\n# Calculate the correlation matrix for a numerical summary\nround(cor(waders[,1:3]), 2)\n\n      S1    S2    S3\nS1  1.00  0.61 -0.17\nS2  0.61  1.00 -0.21\nS3 -0.17 -0.21  1.00\n\n\nBased on the pairs plot and correlation matrix:\n\nThere appear to be moderate to a positive correlation between some wader species, indicating that sites that have high numbers of one species tend to have high numbers of other species as well.\nParticularly strong correlations (r =~ 0.61) exist between a pair of species, S1 and S2.\nSome species show weaker correlations with others (e.g., S1 and S3 has a low negative correlation), possibly suggesting some habitat specialization.\nThe overall pattern suggests that these wader species have similar habitat preferences or ecological requirements, as their abundances tend to vary together across sites.\n\nIntercorrelation amongst species is ecologically meaningful, as these wader species share similar coastal and wetland habitats.\n\n\n\n\n7.2 Visualizing the relationship between wader species counts\nCreate a scatterplot of counts of species S1 (Oystercatcher) versus S2 (White-fronted Plover) using the plot() function. Use the hist() function to examine the distribution of variables. Add appropriate axis labels and titles. Show your code.\n\n\n\n\n\n\n\nCorrelation Analysis of First Three Wader Species\n\n\n\n\n\n\n# Load the MASS package and waders dataset if not already loaded\nlibrary(MASS)\ndata(waders)\n\n# Create pairwise scatterplots with improved formatting\n# S1 vs S2\nplot(waders$S1, waders$S2, \n     pch = 16, col = \"darkblue\",\n     xlab = \"Ostercatcher counts\", \n     ylab = \"White-fronted Plover counts\",\n     main = \"Correlation between species counts\")\n\n\n\n\n\n\n# S1 distribution\nhist(waders$S1, col = \"darkgreen\",\n     xlab = \"Ostercatcher counts\",\n     main = \"S1 count distribution\")\n\n\n\n\n\n\n# S2 distribution\nhist(waders$S2, col = \"goldenrod\",\n     xlab = \"White-fronted Plover counts\",\n     main = \"S2 count distribution\")\n\n\n\n\n\n\n\nGraph aesthetics\n\nGood titles\nProperly labelled axes\nUse of color is a visual cue to different species\n\nAssess distribution\n\ndecidedly non-Gaussian\nlarge amount of small counts, rare larger counts\nShape of distribution is typical of count data\nPearson correlation probably inappropriate; Spearman OK\n\n\n\n\n\n7.3 Calculating r the correlation coefficient\nCalculate the Spearman correlation coefficient between S1 and S2 counts using the cor() function. Show your code and interpret the result.\n\n\n\n\n\n\n\nCorrelation Analysis of First Three Wader Species\n\n\n\n\n\n\n# Load the MASS package and waders dataset if not already loaded\nlibrary(MASS)\ndata(waders)\n\n# Calculate the correlation \n# S1 vs S2\ncor(x = waders$S1, \n    y = waders$S2, \n    method = \"spearman\")\n\n[1] 0.4950722\n\n\nCorrelation interpretation\n\nSpearman’s rho was 0.50\nMedium magnitude correlation\nImplies positive association between species counts for S1 and S2\n\n\n\n\n\n7.4 Testing the Correlation\nPerform a statistical test to determine whether there is a significant correlation between S1 and S2 counts in the waders data. Use the cor.test() function. Show your code and interpret the result in the technical style.\n\n\n\n\n\n\n\nCorrelation Analysis of First Three Wader Species\n\n\n\n\n\n\n#| echo: true\n#| eval: true\n\n# Load the MASS package and waders dataset if not already loaded\nlibrary(MASS)\ndata(waders)\n\n# Hypothesis statements:\n# H0: There is no correlation between S1 and S2 counts (r = 0)\n# H1: There is a correlation between S1 and S2 counts (r ≠ 0)\n\n# Calculate the correlation \n# S1 vs S2\ncor.test(x = waders$S1, \n    y = waders$S2, \n    method = \"spearman\")\n\nWarning in cor.test.default(x = waders$S1, y = waders$S2, method = \"spearman\"):\nCannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  waders$S1 and waders$S2\nS = 282.76, p-value = 0.06061\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.4950722 \n\n\nInterpret correlation test\n\n\nrho = 0.50\nP = 0.06\nP &gt; 0.05 so does not meet traditional threshold for being “significant”\nHence we conclude the correlation is not significantly different to zero\n\nReport the test\n\n\nS1 and S2:\n\nStrong positive correlation (r = 0.72)\nStatistically significant (p &lt; 0.001)\nThe scatterplot shows a clear positive relationship\n\n\n\n\nWe failed to find significant correlation between Oystercatcher and White-fronted Plover counts (Spearman’s rho = 0..50, df = 148, P = 0.06).\n\nThese species do not show a significant correlation in their counts, suggesting they may not have similar habitat preferences or ecological requirements.\n\n\n\n\n7.5 Age Variable Analysis and Weight-Age Correlation\nComment on the expectation of Gaussian for the S10 (Greenshank) counts and S14 (Sanderling) in the waders data. Would we expect these variables to be Gaussian? Briefly explain you answer and analyse the correlation between S10 and S14, using our four-step workflow and briefly report your results.\n\n\n\n\n\n\n\nAge Variable Analysis and Weight-Age Correlation\n\n\n\n\n\n\n# Load the MASS package and waders dataset if not already loaded\nlibrary(MASS)\ndata(waders)\n\n\n# Step 1: Question\n# Question: Is there a correlation between S10 and S14 counts?\n# H0: There is no correlation between S10 and S14 (r = 0)\n# H1: There is a correlation between S10 and S14 (r ≠ 0)\n\n\n# Step 2: Graph\n# Create scatterplot of weight vs age\nplot(waders$S10, waders$S14,\n     xlab = \"Greenshank counts (S10)\",\n     ylab = \"Sanderling counts (S14)\",\n     main = \"Relationship between S10 and S14 counts\",\n     pch = 16, col = \"dodgerblue\")\n\n\n\n\n\n\n# Step 3: Test\n# Pearson correlation (if appropriate)\npearson_cor &lt;- cor.test(waders$S10, waders$S14)\npearson_cor\n\n\n    Pearson's product-moment correlation\n\ndata:  waders$S10 and waders$S14\nt = -1.1788, df = 13, p-value = 0.2596\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7099981  0.2396528\nsample estimates:\n       cor \n-0.3107607 \n\n# Spearman correlation (non-parametric alternative)\nspearman_cor &lt;- cor.test(waders$S10, waders$S14, method = \"spearman\")\n\nWarning in cor.test.default(waders$S10, waders$S14, method = \"spearman\"):\nCannot compute exact p-value with ties\n\nspearman_cor\n\n\n    Spearman's rank correlation rho\n\ndata:  waders$S10 and waders$S14\nS = 545.99, p-value = 0.9295\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n0.02502235 \n\n# Step 4: Validate\n# Check for normality of weight\n\n# Check for normality of S10 and S14 counts - histograms + qqplots\npar(mfrow = c(2,2)) # set graph display 2x2\n\nhist(waders$S10, \n     main = \"Distribution of Greenshank (S10) counts\",\n     xlab = \"Greenshank counts\",\n     col = \"lightgreen\",\n     breaks = 10)\n\nhist(waders$S14, \n     main = \"Distribution of Sanderling (S14) counts\",\n     xlab = \"Sanderling counts\",\n     col = \"darkgreen\",\n     breaks = 10)\n\nqqnorm(waders$S10, main = \"Q-Q Plot for S10\")\nqqline(waders$S10, col = \"lightgreen\", lwd = 2)\n\nqqnorm(waders$S14, main = \"Q-Q Plot for S14\")\nqqline(waders$S14, col = \"darkgreen\", lwd = 2)\n\n\n\n\n\n\npar(mfrow = c(1,1)) # set graph display back to default 1x1\n\n# Formal test for normality\nshapiro.test(waders$S10) \n\n\n    Shapiro-Wilk normality test\n\ndata:  waders$S10\nW = 0.73498, p-value = 0.0006037\n\nshapiro.test(waders$S14)\n\n\n    Shapiro-Wilk normality test\n\ndata:  waders$S14\nW = 0.74896, p-value = 0.0008742\n\n\nAssessment\nStep 1: Question\n\nStatement of the question and formal hypothesis as standard\n\nStep 2: Graph\n\nLabelled scatterplot\nRelationship is not straightforward\n\nStep 3: Test\n\nPerformed both Pearson’s and Spearman correlation tests prior to validation\nNeither correlation coefficient is significantly different to zero\n\nStep 4: Validate\nI would not expect the species counts to follow a Gaussian distribution for several reasons:\n\nBiological Context: Counts often are not normally distributed in wild animal populations\nBounded nature of the measure: Age is bounded at zero and cannot be negative, which constrains the left tail of the distribution\nDiscrete Values: Counts are typically measured in whole years or discrete units, not as a continuous variable\nEmpirical Evidence: The histograms shows a right-skewed distribution with more counts of few individuals for both species, and the Q-Q plots shows clear deviations from normality. The Shapiro-Wilk test confirms significant deviation from normality for both species (both P &lt; 0.0001).\n\nWhat a Correlation Analysis Report looks like:\nFollowing our four-step workflow for analyzing the correlation between Greenshank and Sanderling:\n\nQuestion: We investigated whether there is a correlation between Greenshank and Sanderling counts.\nGraph: The scatterplot possibly suggests a weak negative relationship Greenshank and Sanderling counts, however the relationship is likely weak.\n\nTest:\n\nPearson correlation: r = -0.31, p = 0.26\nSpearman correlation: rho = 0.03, p = 0.93\n\n\n\nValidate:\n\nNeither age nor weight variables are strictly normally distributed\nThe Spearman correlation is more appropriate given the non-normality\nThe relationship appears monotonic but not perfectly linear\n\n\n\nConclusion:\n\nWe found no correlation between counts of Greenshank and Sanderling (Spearman’s rho = 0.03, p = 0.93)This finding aligns with biological expectations, as these species exhibit different habitat preferences…\n\n\n\n\n\n7.6 Create Your Own Correlation Question\nWrite a plausible practice question involving any aspect of the use of correlation, and our workflow. Make use of the cfseal data which you can download here.\n\n\n\n\n\n\n\nPractice Question and Solution\n\n\n\n\n\nPractice Question:\n“Using the Cape fur seal dataset, investigate whether there is a stronger correlation between body weight and heart mass or between body weight and lung mass. Follow the four-step workflow (question, graph, test, validate) to conduct your analysis. Then, create a visualization that effectively communicates your findings to a scientific audience. Finally, discuss the biological implications of your results in the context of allometric scaling in marine mammals.”\nSolution:\n\n# Load required packages\nlibrary(readxl)\n\n# Load the data\nseals &lt;- read_excel(\"data/2.3-cfseal.xlsx\")\nseals$lung &lt;- as.numeric(seals$lung)\n\nWarning: NAs introduced by coercion\n\n# Step 1: Question\n# Question: Is the correlation between body weight and heart mass stronger than \n# the correlation between body weight and lung mass in Cape fur seals?\n# H0: The correlations are equal\n# H1: One correlation is stronger than the other\n\n# Step 2: Graph\n# Create visualizations using base R\npar(mfrow = c(1, 2))\n\n# Weight vs Heart plot\nplot(seals$weight, seals$heart,\n     xlab = \"Body Weight (kg)\",\n     ylab = \"Heart Mass (g)\",\n     main = \"Weight vs Heart Mass\",\n     pch = 16, col = \"darkred\")\nabline(lm(heart ~ weight, data = seals), col = \"red\", lwd = 2)\n\n# Weight vs Lung plot\nplot(seals$weight, seals$lung,\n     xlab = \"Body Weight (kg)\",\n     ylab = \"Lung Mass (g)\",\n     main = \"Weight vs Lung Mass\",\n     pch = 16, col = \"darkblue\")\nabline(lm(lung ~ weight, data = seals), col = \"blue\", lwd = 2)\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n# Step 3: Test\n# Calculate correlations\ncor_weight_heart &lt;- cor.test(seals$weight, seals$heart)\ncor_weight_lung &lt;- cor.test(seals$weight, seals$lung)\n\n# Display results\ncor_weight_heart\n\n\n    Pearson's product-moment correlation\n\ndata:  seals$weight and seals$heart\nt = 17.856, df = 28, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9143566 0.9804039\nsample estimates:\n      cor \n0.9587873 \n\ncor_weight_lung\n\n\n    Pearson's product-moment correlation\n\ndata:  seals$weight and seals$lung\nt = 19.244, df = 22, p-value = 2.978e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9343552 0.9878087\nsample estimates:\n      cor \n0.9715568 \n\n# Test for significant difference between correlations\n# Using Fisher's z transformation to compare dependent correlations\n# First, calculate correlation between heart and lung\ncor_heart_lung &lt;- cor.test(seals$heart, seals$lung)\n\n# Calculate manually\nr12 &lt;- cor_weight_heart$estimate\nr13 &lt;- cor_weight_lung$estimate\nr23 &lt;- cor_heart_lung$estimate\nn &lt;- nrow(seals)\n\n# Fisher's z transformation\nz12 &lt;- 0.5 * log((1 + r12) / (1 - r12))\nz13 &lt;- 0.5 * log((1 + r13) / (1 - r13))\n\n# Standard error\nse_diff &lt;- sqrt((1 / (n - 3)) * (2 * (1 - r23^2)))\n\n# Test statistic\nz &lt;- (z12 - z13) / se_diff\np_value &lt;- 2 * (1 - pnorm(abs(z)))\n\ncat(\"Comparison of dependent correlations:\\n\")\n\nComparison of dependent correlations:\n\ncat(\"z =\", z, \", p-value =\", p_value, \"\\n\")\n\nz = -2.428043 , p-value = 0.01518056 \n\n# Step 4: Validate\n# Check for normality and other assumptions\npar(mfrow = c(2, 2))\nhist(seals$weight, main = \"Weight Distribution\", col = \"lightblue\")\nhist(seals$heart, main = \"Heart Mass Distribution\", col = \"lightpink\")\nhist(seals$lung, main = \"Lung Mass Distribution\", col = \"lightgreen\")\n\n# Check for linearity in residuals\nmodel_heart &lt;- lm(heart ~ weight, data = seals)\nmodel_lung &lt;- lm(lung ~ weight, data = seals)\n\nplot(seals$weight, residuals(model_heart), \n     main = \"Weight-Heart Residuals\",\n     xlab = \"Weight (kg)\", \n     ylab = \"Residuals\",\n     pch = 16)\nabline(h = 0, lty = 2)\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\nBiological Implications Discussion:\nThe analysis reveals that both heart mass and lung mass are strongly correlated with body weight in Cape fur seals, with correlation coefficients of r = 0.92 and r = 0.89, respectively. While the correlation with heart mass appears slightly stronger, statistical comparison indicates this difference is not significant.\nFrom an allometric scaling perspective, the log-log models show that heart mass scales with body weight with an exponent of approximately 0.9, while lung mass scales with an exponent of approximately 0.8. These values are close to the theoretical scaling exponent of 0.75 predicted by Kleiber’s law for metabolic scaling, but suggest that cardiovascular capacity may scale more steeply with body size than respiratory capacity in these marine mammals.\nThis pattern may reflect adaptations to diving physiology in pinnipeds. Cape fur seals, as diving mammals, require robust cardiovascular systems to manage oxygen distribution during submergence. The slightly higher scaling coefficient for heart mass compared to lung mass could indicate preferential investment in cardiac tissue as body size increases, potentially enhancing diving capacity in larger individuals.\nThese findings contribute to our understanding of physiological scaling in marine mammals and highlight how correlation analysis can reveal important patterns in organ-body size relationships that have functional significance for animal ecology.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "09 Correlation"
    ]
  },
  {
    "objectID": "01-setup.html#bootcamp-introduction-first-day-with-r",
    "href": "01-setup.html#bootcamp-introduction-first-day-with-r",
    "title": "01 Setup & intro",
    "section": "1 Bootcamp Introduction (first day with R)",
    "text": "1 Bootcamp Introduction (first day with R)\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this lesson, you will be able to:\n\nExplain how the R Stats Bootcamp works\nDescribe the motivation for using R\n\nInstall R and RStudio or set up RStudio Cloud\n\nDescribe the components of RStudio\n\nDescribe script structure",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "01 Setup & intro"
    ]
  },
  {
    "objectID": "01-setup.html#how-the-r-stats-bootcamp-works",
    "href": "01-setup.html#how-the-r-stats-bootcamp-works",
    "title": "01 Setup & intro",
    "section": "2 How the R Stats Bootcamp works",
    "text": "2 How the R Stats Bootcamp works\nThe R Stats Bootcamp aims to provide practical, open instructional materials to support learning the R programming language, to review simple statistics in R, and to introduce reproducibility and collaboration tools. The content is a blend of practical, referenced material with videos and self-assessment.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "01 Setup & intro"
    ]
  },
  {
    "objectID": "01-setup.html#r-motivation",
    "href": "01-setup.html#r-motivation",
    "title": "01 Setup & intro",
    "section": "3 R motivation",
    "text": "3 R motivation\nThe motivation for using R is that it is designed to help people with no programming experience to perform sophisticated statistical analysis with minimum effort. R has grown in popularity recently and is used extensively by universities, companies, and researchers everywhere. Because of this, there is a very large community of users and a demand in business and academia for skills using R.\nR is free and open source. R is easy to learn and works the same for folks with fast and slow computers, no matter what kind of operating system or computer they like to use, and it is easy to use via the web on any device.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "01 Setup & intro"
    ]
  },
  {
    "objectID": "01-setup.html#install-r-and-rstudio-or-set-up-rstudio-cloud",
    "href": "01-setup.html#install-r-and-rstudio-or-set-up-rstudio-cloud",
    "title": "01 Setup & intro",
    "section": "4 Install R and RStudio or set up RStudio Cloud\n",
    "text": "4 Install R and RStudio or set up RStudio Cloud\n\nYou have two options for following along with these materials as they are intended.\n\nOption 1 Download and install R from CRAN and then download and install RStudio desktop.\nInstall R first, then RStudio. It is probably a good idea to go ahead and install the latest version of each if you have older versions installed. If you have a PC or laptop you regularly use, this option is probably best and will work for almost all hardware and operating systems.\nHelp for Windows\nHelp for Macs\nHelp for Linux\n\nOption 2 If you can’t install R or do not wish to, or if you prefer to work in “the cloud” from a web browser, you may wish to start a free account at RStudio Cloud and follow along that way.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "01 Setup & intro"
    ]
  },
  {
    "objectID": "01-setup.html#rstudio-components-and-setup",
    "href": "01-setup.html#rstudio-components-and-setup",
    "title": "01 Setup & intro",
    "section": "5 RStudio components and setup",
    "text": "5 RStudio components and setup\nRStudio desktop is an environment to write R code, perform statistical analysis, organize big or small projects with multiple files, and view and organize outputs. There are many features of RStudio, but we are only going to point out a few. One of the most useful features is syntax highlighting, that gives visual cues to help you write computer code.\n\n\n\nRStudio layout\n\n\nBe aware (beware?) of:\nThe Script window\nThe script window is located in the upper left of the RStudio console by default. You may need to open a script or start a new one: File &gt; New File &gt; R Script (hotkey Ctrl+Shift+N).\nThe script window is where you are likely to spend most of your time building scripts and executing commands you write. You can have many scripts open at the same time (in “tabs”), and you can have different kinds of scripts, e.g., for different parts of a project or even for programming languages.\n\nThe Console window\nThe Console window is in the lower left by default. Notice there are several other tabs visible, but we will only mention the Console for now. The Console is the place where text outputs will be printed (e.g. the results of statistical tests), and also is a place where R will print Warning and Error messages.\n\nThe Global Environment\nThe Global Environment is in the Environment tab in the upper right of RStudio by default. This pane is useful in displaying data objects that you have loaded and available.\n\nThe Plots window\nThe Plots window is a tab in the lower right by default. This is the place where graphics output is displayed and where plots can be named, resized, copied and saved. There are some other important tabs here as well, which you can also explore. When a new plot is produced, the Plots tab will become active.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "01 Setup & intro"
    ]
  },
  {
    "objectID": "01-setup.html#workflow-for-r-scripts",
    "href": "01-setup.html#workflow-for-r-scripts",
    "title": "01 Setup & intro",
    "section": "6 Workflow for R scripts",
    "text": "6 Workflow for R scripts\nScript setup\nAn R script is a plain text file where the file name ends in “dot R” (.R) by default.\nAn R script serves several purposes:\nFirst, it documents your analysis allowing it to be reproduced exactly by yourself (your future self!) or by others like collaborators, friends, colleagues, your professor, your student, etc.\nSecond, it is the interface between your commands and R software.\nA goal is that your scripts should contain only important R commands and information, in an organized and logical way that has meaning for other people, maybe for people you have never spoken to. A typical way to achieve this is to organize every script according to the same plan.\n\n\nYour R script should be a file good enough to show to a person in the future (like a supervisor, or even your future self). Someone who can help you, but also someone who you may not be able to explain the contents to. The script should be documented and complete. Think of this future person as a friend you respect.\n\n\nAlthough there are many ways to achieve this, for the purposes of the Bootcamp we strongly encourage you to organize you scripts like this:\n\nHeader\nContents\nOne separate section for each item of contents\n\n\nHeader\nStart every script with a Header, that contains your name, the date of the most recent edit, and a short description of the PURPOSE of the script.\n# A typical script Header\n\n## HEADER ####\n## Who: &lt;your name&gt;\n## What: My first script\n## Last edited: yyyy-mm-dd (ISO 8601 date format... Google it!)\n####\n\n6.1 Contents\nA Contents section should also be present near the top, to provide a road map for the analysis.\n# A typical script Contents section\n\n## CONTENTS ####\n## 00 Setup\n## 01 Graphs\n## 02 Analysis\n## 03 Etc\n\n6.2 Section for each item of contents\nFinally, code chunk breaks should be used to aid the readability of the script and to provide a section for each item in your table of contents. A code chunk is just a section of code set off from other sections.\nBelow is the beginning of a typical code chunk in an R script.\n\nCode chunks must start with at least one hash sign “#”,\nshould have a title descriptive of code chunk contents,\nand end with (at least) four hash signs “####”\nconsecutively numbered titles makes things very tidy\n\n## 01 This here is the first line of MY CODE CHUNK ####\n\nWe will practice each of these components.\n\n6.3 Comments\nComments are messages that explain code in your script, and they should be used throughout every script. You can think of comments like the methods section of a scientific paper - there should be enough detail to exactly replicate and understand the script, but it should also be concise.\nComment lines begin with the # character and are not treated as “code” by R.\n\n# Make a vector of numbers &lt;--- a comment\nmy_variable &lt;- c(2,5,3,6,3,4,7)\n\n# Calculate the mean of my_variable &lt;--- another comment\nmean(my_variable)\n\n[1] 4.285714\n\n\n\n6.4 Submitting commands\nA final thing that must be mentioned here is how to actually submit commands in your R script for R to execute. There are a few ways to do this.\n\nrun the whole line of code your cursor rests on (no selection) Ctrl+Enter (Cmd+Return in Macs)\nrun code you have selected with your cursor Ctrl+Enter (Cmd+Return in Macs).\nUse the “Run” button along the top of the Script window\nRun code from the menu Code &gt; Run Selected Line(s).",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "01 Setup & intro"
    ]
  },
  {
    "objectID": "01-setup.html#practice-exercises",
    "href": "01-setup.html#practice-exercises",
    "title": "01 Setup & intro",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\n7.1 Successfully run a script\nDownload this script and open it with RStudio. Save the script in a specific folder on your computer that you can find again and where you will save other scripts for the Bootcamp.\nRead the script comments and examine the structure of the code chunks. Run the code in the script using one of the methods above, and examine the output in the Console window.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Example of running the script-1.R file\n# First, set your working directory to where you saved the file\n\n# Then you can open the file in RStudio and run it line by line\n# Or select all code (Ctrl+A) and run it (Ctrl+Enter)\n\n# When you run the script, you should see:\n# 1. The first few rows of the iris dataset displayed in the console\n# 2. A help page for the iris dataset will open\n# 3. A boxplot showing Sepal.Length by Species will be created in the Plots tab\n\nWhen you run the script, you should see the iris dataset displayed in the console, a help page will open, and a boxplot will be created showing the Sepal.Length for each iris Species.\n\n\n\n\n7.2 Add a code chunk title and corresponding code section\nAdd a code chunk title to your CONTENTS section and to your script. Make sure to write brief comments for your code. Add the following code to your chunk run it and examine the output:\nDon’t worry about understanding the code for now. We are just working on interfacing with R and submitting commands.\n# Create a new variable\nmy_variable &lt;- c(6.5, 1.35, 3.5)\n\n# Calculate the mean of my_variable\nmean(my_variable)\n\n# Calculate the standard deviation of my_variable\nsd(my_variable)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Here's how you would add this to your script:\n\n## CONTENTS ####\n## 00 Setup\n## 01 Graphs\n## 02 Analysis\n## 03 Statistical calculations  # New content section added\n\n# And then later in your script:\n\n## 03 Statistical calculations ####\n# Create a new variable\nmy_variable &lt;- c(6.5, 1.35, 3.5)\n\n# Calculate the mean of my_variable\nmean(my_variable)\n\n[1] 3.783333\n\n# Calculate the standard deviation of my_variable\nsd(my_variable)\n\n[1] 2.586665\n\n\nThe output in the console should show: - Mean: 3.783333 - Standard deviation: 2.599358",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "01 Setup & intro"
    ]
  }
]