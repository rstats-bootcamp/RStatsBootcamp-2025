[
  {
    "objectID": "7-question-explore.html",
    "href": "7-question-explore.html",
    "title": "7 Question, explore, analyze",
    "section": "",
    "text": "We call sizing up the data “weighing the pig”"
  },
  {
    "objectID": "7-question-explore.html#question-explore-analyze-a-workflow-for-data-science",
    "href": "7-question-explore.html#question-explore-analyze-a-workflow-for-data-science",
    "title": "7 Question, explore, analyze",
    "section": "1 Question, explore, analyze (a workflow for data science)",
    "text": "1 Question, explore, analyze (a workflow for data science)\n\n\nA dataset often comes to the Data Scientist in an imperfect state, possibly incomplete, containing errors, and with minimal description. Likewise, it may contain wonderful knowledge, there to discover. Either way, your first task is to weigh the pig.\n\n\nThe very first task for any data analysis is to gain an understanding of the data itself. This typically involves examining the variables. Are they as we expect? Do we need to adjust the variable types?\nEDA Exploratory Data Analysis\nThis almost always involves graphing the data, and possibly examining numerical summaries and statistical assumptions. Further, it is necessary to look for errors in the data both trivial (e.g. misspelling factor level names like “control” with an extra space “control”), and more serious errors such as numerical typographical errors (e.g. misplacing a decimal point is a classic: height of 5 men in feet: c(5.5, 5.8, 6.1, 5.9, 52.).\nIn total, this part of data analysis is sometimes referred to as Exploratory Data Analysis.\nEDA is part practical and part philosophical in that is requires skill and experience, but is also subjective. Think of it as a step that might take a long while, where the data scientists decides what the analysis is that will be applied to the data, that the analysis is correct and appropriate. Ironically, while EDA is considered very important and can take a large proportion of the total time spent analyzing data, it is usually only reported on very briefly if at all.\nThe order of operation for most analyses should be\n\n1 question\n2 explore\n3 analyse\n\n\n\nYou choose your data analysis prior to collecting the first data point.\n\nFocus on the question and make sure it is clear in formulation, and choose an analysis approach that can resolve the question , given the data… But the data collection should be DESIGNED to fit the question and chosen analysis prior to collection. Explore the data to examine any assumptions required for the analysis, including the use of graphing and any diagnostic or summary statistics. Finally, perform and summarize the analysis. We will practice this workflow for different basic questions, with an emphasis on simple quantitative data.\n\n\n1.1 Objectives\n\nQuestion formulation and hypothesis testing\nSummarize: Weighing the Pig\nVariables and graphing\n“Analysis” versus “EDA”\nStatistical Analysis Plan: the concept\nPractice exercises"
  },
  {
    "objectID": "7-question-explore.html#question-formulation-and-hypothesis-testing",
    "href": "7-question-explore.html#question-formulation-and-hypothesis-testing",
    "title": "7 Question, explore, analyze",
    "section": "2 Question formulation and hypothesis testing",
    "text": "2 Question formulation and hypothesis testing\n\nIt is the primary responsibility of the scientist to agree on the specific details of generating evidence from data to answer questions (i.e., statistical analysis). When these roles are occupied by the same person, this matter should be settled before collecting any data.\n\nThe general topic of formulating statistical questions is vast; many books have been written on the subject. The tradition and practice of statistical analysis has evolved through time. Here we will focus on the traditional starting point for a “first statistics course”, within the context of Null Hypothesis Significance testing (NHST).\n\n2.1 Sampling concept and NHST\nThe gambit of NHST is that there is a population of interest but that the population cannot be directly measured because it is too big or otherwise inconvenient or impossible to measure. Thus, experimental samples are drawn randomly from the population, possibly subjected to experimental conditions, and the magnitude of observed differences or measured associations are summarized by various test statistics and compared to how likely such an observed difference or association would be to observe in the absence of the hypothesized effect.\nThe null hypothesis is the one consistent with no effect or difference. We evaluate whether to reject the null hypothesis using the P-value, the (conditional) probability that the observed effect is unlikely to arise duie to sampling or experimental error.\nTraditionally, the P-value is compared to the alpha value, almost always set to 0.05. This alpha value can be interpreted as the maximum probability that is acceptable of making a mistake and concluding there IS a difference, when in fact a difference does not exist. When the P-value is less than 0.05, we conclude there is a difference, rejecting the null hypothesis and “accepting” the hypothesis we predicted was true, usually referred to as the alternative hypothesis.\n\n\n\n2.2 NHST notes\nBenefits of NHST\n\nFamiliar and acceptable to majority of researchers\nTypically robust to assumptions when applied correctly\nStrong framework for evidence, especially for experiments\nThe basic idea is objective and simple\n\n\nCriticism of HNST\n\nOften conceived, applied and interpreted under error\nValidation of analysis (e.g. assumptions testing) is often neglected\nEducation for applied researchers often deficient\nThough simple, practitioners may be ignorant of subtle concepts\n\n\n\n\n2.3 Further reading\nIf the idea is new to you that NHST in statistics is not perfect and you want to get serious about understanding why, like most subjects, you will need to pursue further sources.\nAnderson, D.R., Burnham, K.P. and Thompson, W.L., 2000. Null hypothesis testing: problems, prevalence, and an alternative. The journal of wildlife management, pp.912-923.\nNickerson, R.S., 2000. Null hypothesis significance testing: a review of an old and continuing controversy. Psychological methods, 5(2), p.241.\nNix, T.W. and Barnette, J.J., 1998. The data analysis dilemma: Ban or abandon. A review of null hypothesis significance testing. Research in the Schools, 5(2), pp.3-14.\nStephens, P.A., Buskirk, S.W., Hayward, G.D. and Martinez Del Rio, C., 2005. Information theory and hypothesis testing: a call for pluralism. Journal of applied ecology, 42(1), pp.4-12."
  },
  {
    "objectID": "7-question-explore.html#summarize-weighing-the-pig",
    "href": "7-question-explore.html#summarize-weighing-the-pig",
    "title": "7 Question, explore, analyze",
    "section": "3 Summarize: Weighing the Pig",
    "text": "3 Summarize: Weighing the Pig\n\n\nThe best way gain skill in handling data is to practice.\n\nWeighing the pig is the term we use to describe creating a summary-at-a-glance of a dataset. Usually this includes graphics and statistical summary, as well a description of how much data we have. A key consideration is, also, the specification of the variables.\nWe will practice data handling with the data file chickwts.xlsx.\nDownload the file, read it into a data object in R called chicks, and convert the feed variable to a factor if necessary.\n# Try this:\n\n# Download the 7-chickwts.xlsx file, read it into a data \n# object in R called \"chicks\", \n# and convert the \"feed\" variable to a factor if necessary.\n\n# Do not neglect looking inside the \"raw\" data file\n# Is it as you expect?  Is the data dictionary present and clear?\n\n# Load necessary libraries\nlibrary(openxlsx)\n\n# Read file\nsetwd(\"D:/Dropbox/git/DSgarage/public/data\") # NB change to YOUR file path...\nchicks <- read.xlsx(\"7-chickwts.xlsx\")\n\n# Convert feed to factor if needed\nclass(chicks$feed) # Character\nchicks$feed <- factor(chicks$feed)\nclass(chicks$feed) # Factor\n\n\n3.1 Chick data\n\nThe hypothesis voices “how you think the world works” or what you predict to be true”\n\nThe hypothesis we believe is true for the chicks dataset might be phrased in different ways.\n\nChick weight differs after 6 weeks according to feed additive type\nMean chick weight varies according to feed additive type\nThe variance between chick weight for different feed additives is bigger than the variance within chick weight as a whole\n\n\n\n\n3.2 Hypothesis\nThe minimum amount of information we are usually interested in when sizing up a dataset is How much data is there?, What is the central tendency (e.g. the mean, variance, etc.)?, and possibly Are there rare values?.\nWe would typically start graphing the data right away. If we have a notion of what our questions or hypotheses are, they should inform the initial peek at the data. For example, in the chickwts data, we know our question will be related not to the overall central tendency of chick weight, but to chick weight for each individual feed type.\nWe do not approach this sizing up of the data in a workhorse fashion, merely to check a tick box. We are looking quickly for details in the data that give us insight into what the data is like. For example, we peek at whether the mean and median are close to each other (indicator our data may be Gaussian), we compare the standard deviation, variance or standard error of a numeric variable relative to different levels of a factor, to see if they are similar.\n# Try this:\n\n# Summarize the whole dataset\n# summary() provides summary statistics for numeric variables and counts\nsummary(chicks)\n\n# we might want to look at summary for different levels of feed\n?summary\nsummary(object = chicks$weight[which(chicks$feed == \"casein\")])\nsummary(object = chicks$weight[which(chicks$feed == \"horsebean\")])\n# etc. - this method is easy but inelegant?\n\n# aggregate()\n?aggregate\n\n# mean\naggregate(x = chicks$weight, by = list(chicks$feed), FUN = mean)\n\n# standard deviation\naggregate(x = chicks$weight, by = list(chicks$feed), FUN = sd)\n\n# You can make your own function for the FUN argument\n# stadard error of mean, SEM = standard deviation / square root of sample size\naggregate(x = chicks$weight, by = list(chicks$feed), \n          FUN = function(x){ sd(x)/sqrt(length(x)) })\n\n# You can apply several functions and name them!\naggregate(x = chicks$weight, by = list(feed = chicks$feed), \n          FUN = function(x){ c(mean = mean(x), \n                               sd = sd(x),  \n                               SEM = sd(x)/sqrt(length(x)))})"
  },
  {
    "objectID": "7-question-explore.html#variables-and-graphing",
    "href": "7-question-explore.html#variables-and-graphing",
    "title": "7 Question, explore, analyze",
    "section": "4 Variables and graphing",
    "text": "4 Variables and graphing\n\nA good graph usually tells the whole story, but a bad graph is worse than no graph at all.\n\n\n\n\n\n\nXKCD Convinced by data\n\n\n\n\nThere are a few topics in graphing data that are important to consider here, but the topic is wide and deep, analytical, creative, and even artistic. We make a distinction between graphs used to explore data during EDA (meant to be “consumed” only by the data scientist who made them and are of no use to document a pattern to others) and graphs intended to constitute evidence.\n\n\n4.1 Scientific graphs\nA few graphing principles:\n\nMust convey the relevant information\nShould be consistent in aesthetics\nMust be self-contained (meaning is contained 100% within the figure and legend)\nShould reflect a hypothesis or statistical concept (if not purely descriptive)\nShould be appropriate to the data\n\n\nYou can think of R graphics as a way to “build up information in layers” onto a graph. There are many aesthetic features of graph that can be controlled, like adding colors, text, lines, legends, etc. The R graphics system is very simple to use, but can also be very powerful (mastering this takes practice). We make a distinction here between R base graphics and packages that can be used to make specialized and varied graphs (like the powerful and popular package {ggplot})\n\n\n4.2 Layering information\nWe can look at graphing the chicks data in a few different ways. We will try a few different graphs in this way, building up features. We might build up features on a graph using arguments in a particular graph function.\n\nLike, adding\n\na main title with the argument main\nthe x axis title with the argument xlab\nadding lines with the functions abline() or lines()\n\n\n\n\n4.3 Types of graphs\nTypically you would choose the type of graph that both fits the type of data you have and that conveys the information you wish to examine or showcase. E.g., for a single numeric variable, you might wish to show:\n\nThe distribution of data with a histogram: hist()\nThe central tendency relative to a factor with a boxplot: boxplot()\n\n\nHistogram of the chicks data\n# The least you can do\nhelp(hist)\nhist(x = chicks$weight)\n\n\nAdd a title with main\n# Argument main\nhist(x = chicks$weight,\n     main = \"Distribution of chick weights (all feeds)\")\n\n\nAdd an x axis title with xlab\n# x axis title\nhist(x = chicks$weight,\n     main = \"Distribution of chick weights (all feeds)\",\n     xlab = \"Chick weight (grams)\")\n\n\nAdd a vertical line for the weight mean with abline()\n# Add vertical line for mean weight\nhist(x = chicks$weight,\n     main = \"Distribution of chick weights (all feeds)\",\n     xlab = \"Chick weight (grams)\")\n\nhelp(abline)\nabline(v = mean(chicks$weight), col = \"red\", lty = 2, lwd = 3)\n\n\n# Try a boxplot\n\nhelp(boxplot)\nboxplot(x = chicks$weight)\n\n# I have seen worse graphs, but I can't remember when.\n# Flash challenge: Improve the graph\n\n\n# weight as a function of feed\nboxplot(formula = weight ~ feed,\n        data = chicks)\n# This is probably a good representation of our hypothesis\n# Flash challenge: Improve the graph..."
  },
  {
    "objectID": "7-question-explore.html#analysis-versus-eda",
    "href": "7-question-explore.html#analysis-versus-eda",
    "title": "7 Question, explore, analyze",
    "section": "5 “Analysis” versus “EDA”",
    "text": "5 “Analysis” versus “EDA”\nAlthough you could consider Exploratory Data Analysis, EDA, an important part of the complete process of data analysis, we might make a distinction between “Analysis” the part of analysis that generates Evidence, and that of EDA which is used to explore data and test assumptions.\n\n\n5.1 Analysis\nA data analysis is\n\nDesigned to fit a specific question or hypothesis\nPart of a workflow: Informal hypothesis statement (in plain language) > Statistical hypothesis (specifies a or implies a statistical test) > Evidence (the specific results)\nDesigned and usually formatted to present to others, such as in a report or a scientific manuscript\nContains only bare essentials as relates to the initial hypothesis (e.g. a good graph, the summary of a statistical analysis)\nShould strictly be reproducible via a script and archived data\nDone in conjunction with EDA\n\n\n\n\n5.2 EDA\nExploratory data analysis is\n\nInformal and may be haphazard\nDesigned to explore or gain understanding of data\nAssumptions testing\nUsually not designed to document or show to others\nOccurs primarily before (every) analysis\nMay or may not be documented to be reproducible\nDone before the final, evidence-generating Analysis\n\n\nWe can keep this concept of EDA versus Analysis in our mind while we discuss the Statistical Analysis Plan."
  },
  {
    "objectID": "7-question-explore.html#statistical-analysis-plan-the-concept",
    "href": "7-question-explore.html#statistical-analysis-plan-the-concept",
    "title": "7 Question, explore, analyze",
    "section": "6 Statistical Analysis Plan: the concept",
    "text": "6 Statistical Analysis Plan: the concept\n\nI have a cunning (statistical analysis) plan -Baldrick\n\nA Statistical Analysis Plan (SAP) is a formal document that should be used to design data analysis. One of the most important functions of the SAP is to make a formal connection between the hypothesis, the data collected and and the method of analysis that will be used to generate evidence to support or refute the hypothesis. This is conducted before any data are collected.\nThe components of a basic SAP are:\n\nThe hypotheses stated in plain language\nEach hypothesis translated into a specific statistical model\nSpecification of data and and data collection methods\n\n-) Specification of effect size\n\nJustification of sample size through power analysis or other means\n\n\nDefinition of all of these components is beyond the boundaries of this Bootcamp, however the explicit connection of hypotheses with a statistical model is one of the very basic elements of best practice in science.\n\n\n6.1 The scientific method, Classic version\nWe usually learn the scientific method as a cycle where we conceive a problem, form a hypothesis, conduct an experiment, evaluate the result and so on. We learn and teach this as a literal cycle.\n\n\n\n\n\nThe classic view of the scientific process\n\n\n\n\nThis classic view of the scientific process implies that we plan the analysis only after we conduct the experiment and collect data. While many data scientists or statisticians would agree that this model is widely used in science, it is considered very poor practice for several reasons.\n\nThe expected difference or relationship (i.e., the effect size) should explicitly be part of the hypothesis and quantified BEFORE collecting data\nThe statistical test must be chosen prior to collect the data to insure the evidence matches the expectation\nThe sample size should be justified, using power analysis or a less formal means. Collecting too little data will likely result in failing to detect a difference (even if your hypothesis is correct!); Collecting too much data is simply a waste of resources.\n\n\n\n\n\n\nScientific Process - what we teach children in school is not quite right\n\n\n\n\n\n\n6.2 Best practice scientific method\nThe traditional view of the scientific method should probably be adjusted to explicitly accommodate planning the analysis at the same time as the hypothesis formulation stage. Likewise, the analysis plan should specifically influence the design of the data collection for the experiment.\n\n\n\n\n\nModern scientific process\n\n\n\n\nA modern view of best practice of scientific endeavor includes an experimental design phase, with consideration of effect size and power analysis, and the production of a statistical analysis plan that contains a formal statistical hypothesis. All off this happens prior to any data collection."
  },
  {
    "objectID": "7-question-explore.html#practice-exercises",
    "href": "7-question-explore.html#practice-exercises",
    "title": "7 Question, explore, analyze",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\nFor the following questions, use the field-trial.xlsx dataset.\nThis is real data in Tidy Data format, but our information for these exercises is limited precisely to the contents of the file, including the data dictionary. In this experiment, seeds were raised under field trial conditions for two weeks to look at the effect of different treatment conditions on mass of gain during germination. There are several measured variables, with the calculated pct variable probably intended to be the dependent variable, with the factor treatment being the main explanatory variable for variation in pct.\n\n\n7.1\nShow code to set up an R analysis file with a header, table of contents, and a setup section that sets your working directory, loads any required libraries and reads in the data. Call the data.frame object you create seed.\n\n\n\n7.2\n\npct, wet and dry should be numeric\nblock and trial should be factors\ntreatment should be a factor with the level “Control” set as the reference.\n\nShow the code to do this.\n\n\n\n7.3\nUse aggregate() to calculate the mean, standard deviation, standard error, and the count (e.g. length()) of pct for each level of treatment. Show the code.\n\n\n\n7.4\nMake a fully labelled boxplot of the pct variable as a function of treatment. Add a horizontal line (red and dashed) for the overall mean of pct, and two horizontal lines (gray, dotted) for the overall mean of pct +/- 1 standard deviation.\n\n\n\n7.5\n(hard: may require tinkering and problem solving)\nExperiment making a boxplot showing pct ~ treatment separated for each trial\n\n\n\n7.6\nWrite a plausible practice question involving aggregate() and boxplot() in-built R dataset iris."
  },
  {
    "objectID": "7-question-explore.html#harper-adams-data-science",
    "href": "7-question-explore.html#harper-adams-data-science",
    "title": "7 Question, explore, analyze",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis website supports students and staff at Harper Adams University and the MSc in Data Science for Global Agriculture, Food, and Environment led by Ed Harris."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "I call this “Robot farmer with calendar”"
  },
  {
    "objectID": "schedule.html#bootcamp-resources",
    "href": "schedule.html#bootcamp-resources",
    "title": "Schedule",
    "section": "Bootcamp resources",
    "text": "Bootcamp resources\nThis material is intended to require a significant time commitment to complete (most students report about 20 hours at minimum). The schedule below is intended to be a concise overview of the different resources for the Bootcamp. Some pages have associated slides and videos linked here.\nFor all of the lab pages it is intended that you type and run 100% of the code in examples and then explicitly answer the exercise questions, which are based on examples for given code on the pages. Thus, to answer the questions it is recommended to adapt code you have already run when completing the pages.\n\n\n\n\n\n\n\nTopics\n\n\n\n\n  Bootcamp overview\n\n\nModule 1: R scripting\n1 R and RStudio Setup\n2 R language\n3 Functions and packages\n4 Data objects\n5 Data frames\n6 Data sub-setting and manipulation\n\n\nModule 2: Statistics review\n7 Question, explore, analyze\n8 Sampling distributions\n9 Correlation\n10 Simple linear regression\n11 T-test\n12 1-way ANOVA\n\n\nModule 3: reproducibility, collaboration\n13 Reproducibility\n14 Automate reports\n15 Git and version control"
  },
  {
    "objectID": "schedule.html#harper-adams-data-science",
    "href": "schedule.html#harper-adams-data-science",
    "title": "Schedule",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis website supports students and staff at Harper Adams University and the MSc in Data Science for Global Agriculture, Food, and Environment led by Ed Harris."
  },
  {
    "objectID": "slides-setup.html",
    "href": "slides-setup.html",
    "title": "R Bootcamp RStudio & R Scripts",
    "section": "",
    "text": "https://dsgarage.netlify.app/bootcamp/0.1-bootcamp-intro/"
  },
  {
    "objectID": "slides-setup.html#what-will-you-learn",
    "href": "slides-setup.html#what-will-you-learn",
    "title": "R Bootcamp RStudio & R Scripts",
    "section": "What will you learn?",
    "text": "What will you learn?\n \n\nInstall R and RStudio\nPractice of good, reproducible scripting\nDesign vision of learning materials: Read the pages and type and run all code"
  },
  {
    "objectID": "slides-setup.html#tour-of-rstudio-interface",
    "href": "slides-setup.html#tour-of-rstudio-interface",
    "title": "R Bootcamp RStudio & R Scripts",
    "section": "Tour of RStudio interface",
    "text": "Tour of RStudio interface"
  },
  {
    "objectID": "slides-setup.html#reproducible-script",
    "href": "slides-setup.html#reproducible-script",
    "title": "R Bootcamp RStudio & R Scripts",
    "section": "Reproducible script",
    "text": "Reproducible script"
  },
  {
    "objectID": "8-sampling-dist.html",
    "href": "8-sampling-dist.html",
    "title": "8 Sampling and distributions",
    "section": "",
    "text": "Can you guess the weight of this bull? What about you and 99 firends?"
  },
  {
    "objectID": "8-sampling-dist.html#describing-the-shape-of-data",
    "href": "8-sampling-dist.html#describing-the-shape-of-data",
    "title": "8 Sampling and distributions",
    "section": "1 Describing the shape of data",
    "text": "1 Describing the shape of data\n\nOverview\n\nI. A curve has been found representing the frequency distribution of standard deviations of samples drawn from a normal population.\n\n\n\nA curve has been found representing the frequency distribution of values of the means of such samples, when these values are measured from the mean of the population in terms of the standard deviation of the sample\n\n\n\n- Gosset. 1908, Biometrika 6:25.\n\nThe idea of sampling underpins traditional statistics and is fundamental to the practice of statistics. The basic idea is usually that there is a population of interest, which we cannot directly measure. We sample the population in order to estimate the real measures of the population. Because we merely take samples, there is error assiociated with our estimates and the error depends on both the real variation in the population, but also on chance to do with which subjects are actually in our sample, as well as the size of our sample. Traditional statistical inference within Null Hypothesis Significance Testing (NHST) exploits our estimates of error associated with our samples. While this is an important concept, it is beyond the scope of this page to review it, but you may wish to refresh your knowledge by consulting a reference, such as Irizarry 2020 Chs 13-16.\nIn this page, we will briefly look at some diagnostic tools in R for examining the distribution of data, and talk about a few important distributions that are common to encounter.\n\n\n1.1 Objectives\n\nUse of the histogram\nGaussian: that ain’t normal\nPoisson\nBinomial\nDiagnosing the distribution\nPractice exercises"
  },
  {
    "objectID": "8-sampling-dist.html#use-of-the-histogram",
    "href": "8-sampling-dist.html#use-of-the-histogram",
    "title": "8 Sampling and distributions",
    "section": "2 Use of the histogram",
    "text": "2 Use of the histogram\nThe histogram is a graph type that typically plots a numeric variable on the x axis (either continuous numeric values, or integers), and has the frequency of observations on the y axis (i.e., the count), or sometimes the proportion of observation on the y axis.\n\n\n2.1 Typical histogram for continuous variable\n# Try this:\n\n# Adult domestic cats weight approximately 4 Kg on average\n# with a standard deviation of approximately +/1 0.5 Kg\n\n# Let's simulate some fake weight data for 10,000 cats\nhelp(rnorm) # A very useful function\nhelp(set.seed) # If we use this, we can replicate \"random data\"\n\nhelp(hist)\n\nset.seed(42)\ncats <- rnorm(n = 10000,  # 10,000 cats\n              mean = 4,   \n              sd = 0.5)\n\ncats[1:10] # The first 10 cats\n\n [1] 4.685479 3.717651 4.181564 4.316431 4.202134 3.946938 4.755761 3.952670\n [9] 5.009212 3.968643\n\nhist(x = cats,\n     xlab = \"Cat weight (Kg)\")\n\n\n\n\n\nNotice a few things:\n\nThe bars are the count of the number of cats at each weight on the x axis\nThe width of each vertical column is a (non-overlapping) range of weights - these are called “bins” and can be defined, but usually are automatically determined based on the data\nFor count data, each bar is usually one or more integer values, rather than a range of continuous values (as it is for cat weight above in the figure)\nThe shape of a histogram can be used to infer the distribution of the data\n\n\n\n\n2.2 Sampling and populations\nRemember the concept of population versus sample? Well, let’s assume there are only 10,000 cats in the whole world and we just measured the whole population (usually not possible, remember). In this case we can calculate the exact population mean.\nWhat if we tried to estimate the real mean of our population from a sample of around 100 cats? The theory is that our sample mean would be expected to differ from the real population mean randomly. If we did a bunch of samples, most of the guesses would be close to the real population mean and less would be farther out, but all of these sample means would be expected to randomly vary, either less than or greater than the true population mean. We can examine this with a simulation of samples.\n\n# Try this:\n# simulation of samples\n\nhelp(sample) # randomly sample the cats vector\nhelp(vector) # Initialize a variable to hold our sample means\n\n# We will do a \"for loop\" with for()\n\n\nmymeans <- vector(mode = \"numeric\",\n                  length = 100)\nmymeans # Empty\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\nfor(i in 1:100){\n  mysample <- sample(x = cats, # Takes a random sample\n                    size = 30)\n  \n  mymeans[i] <- mean(mysample) # stores sample mean in ith vector address\n  }\n\nmymeans # Our samples\n\n  [1] 3.899662 3.963986 4.019242 3.975304 3.903256 3.877346 3.911380 4.013805\n  [9] 3.989836 3.950881 4.002299 4.024990 4.091362 4.074493 3.989800 3.974894\n [17] 4.000338 3.873104 3.945492 4.061428 4.080559 4.011375 4.023977 3.863940\n [25] 4.047250 3.921947 4.049521 4.085961 3.853068 4.081517 3.987747 4.039110\n [33] 3.940955 3.954955 4.008512 3.942036 3.955110 3.968722 3.896042 3.979187\n [41] 3.957636 4.021170 4.107460 3.989197 3.931964 3.981774 4.125465 4.031625\n [49] 4.081076 3.939582 4.185512 3.997635 3.986411 3.817746 4.075256 4.074309\n [57] 4.136248 3.926092 3.976513 4.008376 3.984264 3.900717 4.138209 3.913901\n [65] 4.123326 3.894216 4.087260 4.145020 3.896286 4.142604 3.865085 4.014336\n [73] 4.053620 3.767552 3.981785 4.130483 4.165097 4.046661 4.077928 4.041611\n [81] 3.873046 4.100438 4.015098 3.947361 4.029464 4.123772 3.860191 3.820483\n [89] 4.071399 4.145585 3.982339 3.950332 3.987406 4.167345 4.126462 4.047683\n [97] 4.035958 3.987601 3.960099 3.869092\n\nhist(x = mymeans,\n     xlab = \"Mean of samples\",\n     main = \"100 cat weight samples (n = 30/sample)\")\nabline(v = mean(mymeans), col = \"red\", lty = 2, lwd = 2)\n\n\n\n\n\nNotice a few things (NB your results might look slightly different to mine - remember these are random samples):\n\nThe samples vary around the true mean of 4.0 Kg\nMost of the samples are pretty close to 4.0, fewer are farther away\nThe mean of the means is close to our true population mean\n\nTry our simulation a few more times, but vary the settings. How does adjusting the sample size (say up to 100 or down to 10)? How about the number of samples (say up to 1000 or down to 10)?"
  },
  {
    "objectID": "8-sampling-dist.html#harper-adams-data-science",
    "href": "8-sampling-dist.html#harper-adams-data-science",
    "title": "8 Sampling and distributions",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis website supports students and staff at Harper Adams University and the MSc in Data Science for Global Agriculture, Food, and Environment led by Ed Harris."
  },
  {
    "objectID": "1-setup.html#bootcamp-introduction-first-day-with-r",
    "href": "1-setup.html#bootcamp-introduction-first-day-with-r",
    "title": "1 Setup & intro",
    "section": "1 Bootcamp Introduction (first day with R)",
    "text": "1 Bootcamp Introduction (first day with R)\n\nThis page is intended to guide people during their first installation and use of R and RStudio.\n\n\n1.1 Objectives\nHere is what we will work on:\n\nHow the R Stats Bootcamp works\nR motivation\nInstall R and RStudio or set up RStudio Cloud\nRStudio components and setup\nWorkflow for scripts in R\nPractice exercises"
  },
  {
    "objectID": "1-setup.html#how-the-r-stats-bootcamp-works",
    "href": "1-setup.html#how-the-r-stats-bootcamp-works",
    "title": "1 Setup & intro",
    "section": "2 How the R Stats Bootcamp works",
    "text": "2 How the R Stats Bootcamp works\nThe R Stats Bootcamp aims to provide practical, open instructional materials to support learning the R programming language, to review simple statistics in R, and to introduce reproducibility and collaboration tools. The content is a blend of practical, referenced material with videos and self-assessment.\nWe also a have a friendly community Slack channel - go over and introduce yourself and say hi!"
  },
  {
    "objectID": "1-setup.html#r-motivation",
    "href": "1-setup.html#r-motivation",
    "title": "1 Setup & intro",
    "section": "3 R motivation",
    "text": "3 R motivation\nThe motivation for using R is that it is designed to help people with no programming experience to perform sophisticated statistical analysis with minimum effort. R has grown in popularity recently and is used extensively by universities, companies, and researchers everywhere. Because of this, there is a very large community of users and a demand in business and academia for skills using R.\nR is free and open source. R is easy to learn and works the same for folks with fast and slow computers, no matter what kind of operating system or computer they like to use, and it is easy to use via the web on any device."
  },
  {
    "objectID": "1-setup.html#install-r-and-rstudio-or-set-up-rstudio-cloud",
    "href": "1-setup.html#install-r-and-rstudio-or-set-up-rstudio-cloud",
    "title": "1 Setup & intro",
    "section": "4 Install R and RStudio or set up RStudio Cloud",
    "text": "4 Install R and RStudio or set up RStudio Cloud\nYou have two options for following along with these materials as they are intended.\n\nOption 1 Download and install R from CRAN and then download and install RStudio desktop.\nInstall R first, then RStudio. It is probably a good idea to go ahead and install the latest version of each if you have older versions installed. If you have a PC or laptop you regularly use, this option is probably best and will work for almost all hardware and operating systems.\nHelp for Windows\nHelp for Macs\nHelp for Linux\n\nOption 2 If you can’t install R or do not wish to, or if you prefer to work in “the cloud” from a web browser, you may wish to start a free account at RStudio Cloud and follow along that way."
  },
  {
    "objectID": "1-setup.html#rstudio-components-and-setup",
    "href": "1-setup.html#rstudio-components-and-setup",
    "title": "1 Setup & intro",
    "section": "5 RStudio components and setup",
    "text": "5 RStudio components and setup\nRStudio desktop is an environment to write R code, perform statistical analysis, organize big or small projects with multiple files, and view and organize outputs. There are many features of RStudio, but we are only going to point out a few. One of the most useful features is syntax highlighting, that gives visual cues to help you write computer code.\n\n\n\n\nRStudio layout\n\n\n\nBe aware (beware?) of:\nThe Script window\nThe script window is located in the upper right of the RStudio console by default. You may need to open a script or start a new one: File > New File > R Script (hotkey Ctrl+Shift+N).\nThe script window is where you are likely to spend most of your time building scripts and executing commands you write. You can have many scripts open at the same time (in “tabs”), and you can have different kinds of scripts, e.g., for different parts of a project or even for programming languages.\n\nThe Console window\nThe Console window is in the lower left by default. Notice there are several other tabs visible, but we will only mention the Console for now. The Console is the place where text outputs will be printed (e.g. the results of statistical tests), and also is a place where R will print Warning and Error messages.\n\nThe Global Environment\nThe Global Environment is in the Environment tab in the upper right of RStudio by default. This pane is useful in displaying data objects that you have loaded and available.\n\nThe Plots window\nThe Plots window is a tab in the lower right by default. This is the place where graphics output is displayed and where plots can be named, resized, copied and saved. There are some other important tabs here as well, which you can also explore. When a new plot is produced, the Plots tab will become active."
  },
  {
    "objectID": "1-setup.html#workflow-for-r-scripts",
    "href": "1-setup.html#workflow-for-r-scripts",
    "title": "1 Setup & intro",
    "section": "6 Workflow for R scripts",
    "text": "6 Workflow for R scripts\nScript setup\nAn R script is a plain text file where the file name ends in “dot R” (.R) by default.\nAn R script serves several purposes:\nFirst, it documents your analysis allowing it to be reproduced exactly by yourself (your future self!) or by others like collaborators, friends, colleagues, your professor, your student, etc.\nSecond, it is the interface between your commands and R software.\nA goal is that your scripts should contain only important R commands and information, in an organized and logical way that has meaning for other people, maybe for people you have never spoken to. A typical way to achieve this is to organize every script according to the same plan.\n\n\nYour R script should be a file good enough to show to a person in the future (like a supervisor, or even your future self). Someone who can help you, but also someone who you may not be able to explain the contents to. The script should be documented and complete. Think of this future person as a friend you respect.\n\n\nAlthough there are many ways to achieve this, for the purposes of the Bootcamp we strongly encourage you to organize you scripts like this:\n\nHeader\nContents\nOne separate section for each item of contents\n\n\nHeader\nStart every script with a Header, that contains your name, the date of the most recent edit, and a short description of the PURPOSE of the script.\n# A typical script Header\n\n## HEADER ####\n## Who: <your name>\n## What: My first script\n## Last edited: yyyy-mm-dd (ISO 8601 date format... Google it!)\n####\n\n\n6.1 Contents\nA Contents section should also be present near the top, to provide a road map for the analysis.\n# A typical script Contents section\n\n## CONTENTS ####\n## 00 Setup\n## 01 Graphs\n## 02 Analysis\n## 03 Etc\n\n\n\n6.2 Section for each item of contents\nFinally, code chunk breaks should be used to aid the readability of the script and to provide a section for each item in your table of contents. A code chunk is just a section of code set off from other sections.\nBelow is the beginning of a typical code chunk in an R script.\n\nCode chunks must start with at least one hash sign “#”,\nshould have a title descriptive of code chunk contents,\nand end with (at least) four hash signs “####”\nconsecutively numbered titles makes things very tidy\n\n## 01 This here is the first line of MY CODE CHUNK ####\n\nWe will practice each of these components.\n\n\n\n6.3 Comments\nComments are messages that explain code in your script, and they should be used throughout every script. You can think of comments like the methods section of a scientific paper - there should be enough detail to exactly replicate and understand the script, but it should also be concise.\nComment lines begin with the # character and are not treated as “code” by R.\n\n# Make a vector of numbers <--- a comment\nmy_variable <- c(2,5,3,6,3,4,7)\n\n# Calculate the mean of my_variable <--- another comment\nmean(my_variable)\n\n[1] 4.285714\n\n\n\n\n\n6.4 Submitting commands\nA final thing that must be mentioned here is how to actually submit commands in your R script for R to execute. There are a few ways to do this.\n\nrun the whole line of code your cursor rests on (no selection) Ctrl+Enter (Cmd+Return in Macs)\nrun code you have selected with your cursor Ctrl+Enter (Cmd+Return in Macs).\nUse the “Run” button along the top of the Script window\nRun code from the menu Code > Run Selected Line(s)."
  },
  {
    "objectID": "1-setup.html#practice-exercises",
    "href": "1-setup.html#practice-exercises",
    "title": "1 Setup & intro",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\n\n7.1\nDownload this script and open it with RStudio. Save the script in a specific folder on your computer that you can find again and where you will save other scripts for the Bootcamp.\nRead the script comments and examine the structure of the code chunks. Run the code in the script using one of the methods above, and examine the output in the Console window.\n\n\n\n7.2\nAdd a code chunk title to your CONTENTS section and to your script. Make sure to write brief comments for your code. Add the following code to your chunk run it and examine the output:\nDon’t worry about understanding the code for now. We are just working on interfacing with R and submitting commands.\n# Create a new variable\nmy_variable <- c(6.5, 1.35, 3.5)\n\n# Calculate the mean of my_variable\nmean(my_variable)\n\n# Calculate the standard deviation of my_variable\nsd(my_variable)"
  },
  {
    "objectID": "1-setup.html#harper-adams-data-science",
    "href": "1-setup.html#harper-adams-data-science",
    "title": "1 Setup & intro",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis website supports students and staff at Harper Adams University and the MSc in Data Science for Global Agriculture, Food, and Environment led by Ed Harris."
  },
  {
    "objectID": "2-r-lang.html",
    "href": "2-r-lang.html",
    "title": "2 R language",
    "section": "",
    "text": "Choose your language carefully"
  },
  {
    "objectID": "2-r-lang.html#r-syntax-basics-r-as-a-passive-aggressive-butler",
    "href": "2-r-lang.html#r-syntax-basics-r-as-a-passive-aggressive-butler",
    "title": "2 R language",
    "section": "1 R Syntax basics (R as a passive-aggressive butler)",
    "text": "1 R Syntax basics (R as a passive-aggressive butler)\n\n\nWe love the R language, but sometimes it is a little bit like talking to a passive aggressive butler - if you aren’t careful with your language, the interaction may have unexpected outcomes…\n\nR is a very popular statistical programming language and open source software design to help scientists and other non-programmers perform statistical analyses and to make great graphs. This page is intended to guide people through some of the basics of the R programming language, just enough to get started. We hope that these pages help make learning R simple (though it can difficult at times while you are learning)\n\n\n1.1 Objectives\nHere is what we will work on:\n\nExample script, comments, help, pseudocode\nMath operators\nLogical Boolean operators\nRegarding “base R” and the Tidyverse\nPractice exercises"
  },
  {
    "objectID": "2-r-lang.html#example-script-comments-help-pseudocode",
    "href": "2-r-lang.html#example-script-comments-help-pseudocode",
    "title": "2 R language",
    "section": "2 Example script, comments, help, pseudocode",
    "text": "2 Example script, comments, help, pseudocode\n\nThe most important thing to keep in mind right at the beginning of learning R is to view the script as something you are writing to document a piece of work (e.g. progress in a workshop like this, an analysis, a research project, etc.). Organizing every script you write is extremely important to build “good habits” for reproducibility of your work. A good guideline for best practice in writing scripts is to pretend you are writing the script, comments and contents, for a respected colleague - someone who you respect and want to impress, but might not be able to explain in person about the purpose of the script (even if this is your future self).\nTo get the most out of this page, we strongly recommend that you:\n\nWork through the instructions here while using R and RStudio as you go along.\nType you own code rather than using copy and paste\nDocument all the code in your own script and write clear, concise comments\n\n\n2.1 Example script\nDownload this example script.\nSave it in a logical place on your computer. Open it in RStudio.\n\nRight at the top you should see the HEADER.\n## HEADER ####\n## Who: <YOUR NAME>\n## What: 2 R language basics\n## Last edited: <DATE TODAY in yyyy-mm-dd format)\n####\n\nGo ahead and fill in the header with your own information.\nNext you should see the CONTENTS section. The idea here is for you to list the CONTENTS of a script that can act as a roadmap to the user, but that can also help organize a large project into manageable chunks. Each title under the CONTENTS section will become the title in a code chunk below. You should see the contents section in the script you downloaded like this:\n## CONTENTS ####\n## 2 Example script, help, pseudocode  \n## 3 Math operators  \n## 4 Logical Boolean operators  \n## 5 Regarding base R and the Tidyverse   \n## 6 Practice exercises  \n\nEach item in the CONTENTS section will become the title of individual code chunks. RStudio recognizes code chunks that have a particular syntax:\nCode chunks begin with 2 “##” signs (1 works too, we prefer 2)\nCode chunks end with at least 4 “####” signs\nThe first code chunk should already be placed in your script\n## 2 Example script, help, pseudocode  ####\n\n\n\n2.2 Help\nOne of the great things about using R is the community and the spirit of helping others. However, there are so many websites, books, blogs and other resources that it can be overwhelming. Best practice is to learn to use the R Help system first, then seek help elsewhere.\nThe basic way to access the built-in help in R, is to use the help() function, with the name of tool you need help using inside the brackets. For example, to calculate the mean of some numbers, we would use the function mean(), and to display the help for the mean() function we would run. Run the following code in your own script:\n# Display help page for the function mean\nhelp(mean)\n\nYou should see:\n\n\n\n\nMean help page (pun intended)\n\n\n\n\nDoes this help look mean enough?\nLet’s orient to the information that is here because the help pages are essential to understand and every help page on every subject is organised in exactly the same way (and we will practice a lot using them).\n\n1 Function name {Package name} This field let’s you know what “R package” the function belongs to. We can ignore this for now, but it can be very useful.\n2 Short description This tells you in a few words what the function does.\n3 (longer) description This gives a longer description of what the function does\n4 Usage This usually gives an example of the function in use and lists the “arguments” that you are required to supply to the function for it to work on. Of course, you need to know about the arguments…\n5 Argument definitions This field tells you what the argument are and do!\n\n\n\n2.3 Deeper help\nUsing the Usage and Argument fields, we can figure out how to make the function do the work we want.\n# Under Usage:\n# mean(x, ...)\n\n# The \"x\" is an argument that is required\n# The \"...\" means there are other optional arguments\n\n# Under Arguments:\n\n# x \n# An R object... for numeric/logical vectors ...\n\n# try this code in your own script\nmy_length <- c(101, 122, 97) # 3 numerical measures\nmean(x = my_length) \n\n\n\n2.4 Pseudocode\nThe idea of pseudocode is to break up a big task into a series of smaller tasks. An example of a task might be ANALYZE YOUR DATA (in shouty capitals because it is a big task). To accomplish this task, we might have to walk through a series of steps, e.g.,\nAnalyze your data:\n\nRead data into R\nTest assumption for statistical testing\nGraph the data\nPerform statistical test\nOrganize outputs to communicate in report\n\n\nIt is often a good idea to break down a task into pseudocode both to organize and document the methods in a logical way, but also to conceptually simplify a problem that is difficult to solve. Practically, the items in a typical table of contents in an R script might be similar to psuedocode. Note that this technique extends very well to any problem, not just R code and programming.\nE.g., what steps would be involved in a problem like: Send a rocket with people in it to Mars such that they survive and return to Earth."
  },
  {
    "objectID": "2-r-lang.html#math-operators",
    "href": "2-r-lang.html#math-operators",
    "title": "2 R language",
    "section": "3 Math operators",
    "text": "3 Math operators\nBasic manipulation of numbers in R is very easy to do and is so intuitive that you may be able to guess what they are and what they do. There are just a few specifics that we will practice. This list is not exhaustive; the goal is to get enough to begin practicing.\n\n\n3.1 Arithmetic\nTry these in your practice script:\n# There are a few others, but these are the basics\n\n# Add with \"+\"\n2 + 5\n\n# Subtract with \"-\"\n10 - 15\n\n# Multiply with \"*\"\n6 * 4.2\n\n# Divide by \"/\"\n\n10 / 4\n\n# raise to the power of x\n2^3 \n9^(1/2) # same as sqrt()!\n\nYour output should look similar to this:\n\n# There are a few others, but these are the basics\n\n# Add with \"+\"\n2 + 5\n\n[1] 7\n\n# Subtract with \"-\"\n10 - 15\n\n[1] -5\n\n# Multiply with \"*\"\n6 * 4.2\n\n[1] 25.2\n\n# Divide by \"/\"\n\n10 / 4\n\n[1] 2.5\n\n# raise to the power of x\n2^3 \n\n[1] 8\n\n9^(1/2) # same as sqrt()!\n\n[1] 3\n\n\n\nOrder of operation\nThe “order of operation” refers to the order in which mathematical calculations are carried out. A phrase like 2 + 2 is simple, but we need to consider order for more complicated phrases like 2 + 2 * 8 - 6. In general multiplication and division are carried out before addition and subtraction unless specific order is coded.\n# Try this\n\n4 + 2 * 3\n\n# Order control - same\n4 + (2 * 3)\n\n# Order control - different...\n(4 + 2) * 3\n\n\n\n3.2 Use of spaces\nIn some cases, the use of spaces does not matter in the R language. Which one of the following ways of writing math operation might be easier to document and read?\n# Try this\n\n6+10                                  # no spaces\n7     -5                              # uneven spaces\n1.6             /                2.3  # large spaces\n16 * 3                                # exactly 1 space\n\n# exactly 1 space is probably easiest to read...\nYour output should look like this:\n\n# Try this\n\n6+10                                  # no spaces\n\n[1] 16\n\n7     -5                              # uneven spaces\n\n[1] 2\n\n1.6             /                2.3  # large spaces\n\n[1] 0.6956522\n\n16 * 3                                # exactly 1 space\n\n[1] 48"
  },
  {
    "objectID": "2-r-lang.html#logical-boolean-operators",
    "href": "2-r-lang.html#logical-boolean-operators",
    "title": "2 R language",
    "section": "4 Logical “Boolean” operators",
    "text": "4 Logical “Boolean” operators\nBoolean operators are expressions that resolve TRUE (treated as 1 in most computing systems including R) versus FALSE (0). A typical expression might be something like asking if 5 > 3, which is TRUE. More sophisticated phrases are possible, and sometimes useful.\n\n4.1 Boolean example\n# Try this\n# simplest example\n3 > 5\n\n# 3 is compared to each element\n3 < c(1, 2, 3, 4, 5, 6) \n\n# Logic and math\n# & (ampersand) means \"and\"\n# | (pipe) means \"or\"\n\n# This asks if both phrases are true (true AND true)\n# notice \"TRUE\" has a special meaning in R\n\nTRUE & TRUE # both phrases are the same, TRUE\n\n3 > 1 & 1 < 5 # both phrases are true\n\n# Are both phrases true?\n\nTRUE & FALSE # are both true?\n\nFALSE & FALSE # are both true?\n\nBoolean expressions are often used to select groups of data, for example asking whether values in a column of variables are greater than some threshold.\n\n\n4.2 Selecting with Booleans\nWe often use Booleans to select particular parts of our data in a powerful way, as an alternative to creating different versions of a particular dataset.\n# Try this\n\n# Put some data into a variable and then print the variable\n# Note \"<-\" is the ASSIGNMENT syntax in R, which puts the value on the left \"into\" x\n\nx <- c(21, 3, 5, 6, 22)\nx\n\nx > 20\n\n# the square brackets act as the index for the data vector\nx[x > 20]\n\n\n\n4.3 The “not” operator, ! (Sorry !Sorry)**\nThe ! operator sets a Boolean value to the opposite. This is sometimes used when an expression can be made simpler by representing it as an opposite. For now we will just demonstrate how it works.\n# Try this\nTRUE # plain true\n\n!FALSE # not false is true!\n\n6 < 5 #definitely false\n\n!(6 < 5) #not false...\n\n!(c(23, 44, 16, 51, 12) > 50)"
  },
  {
    "objectID": "2-r-lang.html#base-r-versus-the-tidyverse",
    "href": "2-r-lang.html#base-r-versus-the-tidyverse",
    "title": "2 R language",
    "section": "5 “Base R” versus the ‘Tidyverse’",
    "text": "5 “Base R” versus the ‘Tidyverse’\n\n5.1 Base R\nThe R language as it was invented and continues to be developed is extremely popular, powerful and easy to use, especially for people without formal programming training or experience in different computing languages. In general, we refer to this pure form of R as Base R.\nSince the late 1990s, the R-user community and Base R resources on have exploded on the web and this form of the language continues to be extremely popular for experts and beginners alike. If interested, you can read more here.\n\n\n\n5.2 Tidyverse R\nRelatively recently, a second version of R has evolved that puts forward different conventions in the R lanuage. The differences are practical, but also philosophical. This form of R use is generally referred to as The Tidyverse. The Tidyverse is extremely powerful and we love it. However, we feel that it is far more efficient to first learn base R for non-programmers, before learning the Tidyverse, and we will exclusively use base R for this bootcamp.\nThere is some disagreement over which “version” of R is better or easier to learn and teach with. You will definitely encounter the Tidyverse at some point and eventually you can choose how much or how little you will use it.\n\nThe Argument for Base R\nThe Argument for the Tidyverse"
  },
  {
    "objectID": "2-r-lang.html#practice-exercises",
    "href": "2-r-lang.html#practice-exercises",
    "title": "2 R language",
    "section": "6 Practice exercises",
    "text": "6 Practice exercises\n\n\n6.1\nName and describe the purpose of the first 2 sections that should be present in every R script\n\n\n\n6.2\nWhat is the purpose of “subset” argument in the boxplot() function (hint: use help())\n\n\n\n6.3\nWrite an expression using good R spacing syntax that takes the sum of 3, 6, and 12 and divides it by 25\n\n\n\n6.4\nWrite pseudocode steps for calculating the volume of a cylinder (hint, if you do not know it by heart, you may need to research the equation for the volume of a cylinder!). For a cylinder of height = 3.2 cm and end radius of 5.5 cm, report the volume in cm to 2 decimal points of accuracy. Use at least 3 decimal points of accuracy for pi (hint, the quantity named pi is a standard variable in R!).\n\n\n\n6.5\nExecute the code and explain the outcome in comments.\nTRUE & 3 < 5 & 6 > 2 & !FALSE\n\n\n\n6.6\nWrite a plausible practice question involving the use of the not Boolean operator, !.\n\nNote: for this R Stats Bootcamp page, and only for this one, a sample of solutions is offered. For others, we avoid explicit solutions to encourage you to formulate your own unique solutions and to interact in Slack for help if you need support coming to your own solutions.\nSCRIPT 2 Example SOLUTIONS"
  },
  {
    "objectID": "2-r-lang.html#harper-adams-data-science",
    "href": "2-r-lang.html#harper-adams-data-science",
    "title": "2 R language",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis website supports students and staff at Harper Adams University and the MSc in Data Science for Global Agriculture, Food, and Environment led by Ed Harris."
  },
  {
    "objectID": "3-functions.html",
    "href": "3-functions.html",
    "title": "3 Functions & packages",
    "section": "",
    "text": "R as a garage full of tools"
  },
  {
    "objectID": "3-functions.html#functions-and-packages-toolboxes-r-as-a-garage-full-of-tools",
    "href": "3-functions.html#functions-and-packages-toolboxes-r-as-a-garage-full-of-tools",
    "title": "3 Functions & packages",
    "section": "1 Functions and Packages Toolboxes (R as a garage full of tools)",
    "text": "1 Functions and Packages Toolboxes (R as a garage full of tools)\n\nThe garage and toolbox metaphors\nOne of the best things about R is that it can be customized to accomplish a huge variety of kinds of tasks: perform all sorts of statistical analyses from simple to bleeding edge, produce professional graphs, format analyses into presentations, manuscripts and web pages, collaboration, GIS and mapping, and a lot more. The tools themselves are contained in toolboxes and in a given toolbox, the tools are related to each other, usually in a focus to the kind of tasks they are suited for.\n\nThe tools in R are functions, and the toolboxes are packages.\n\nWhile R comes with a lot of packages, there is an enormous amount available for instant download at any time you want or need them (over 18,000 different packages at the moment…). Making use of all these resources is usually a case of identifying a problem to solve, finding the package that can help you, and then learning how to use the functions in the package This page is all about introducing functions, packages and the R help system.\n\n\n1.1 Objectives\n\nFunction tour\nUsing functions and getting help\nR packages\nFinding, downloading and using packages\nPractice exercises"
  },
  {
    "objectID": "3-functions.html#function-tour",
    "href": "3-functions.html#function-tour",
    "title": "3 Functions & packages",
    "section": "2 Function tour",
    "text": "2 Function tour\n\nThink of functions in R as tools that do work for you.\n\nCode for functions is is simple once you get the idea, but you have to understand how they work to use them in the most powerful way. Also, to make the most of functions, you must get to know which ones perform common tasks, and how to use them. We will practice that in this section. We consider the USE of functions (for a given problem) as a separate issue from discovering a function, and here we focus on USE.\nA generic might look like this: function_name(). The function name is (obviously) the function_name part and all functions must have the bracket notation (). There are some rules for function names and for naming R objects in general, but for now the most important thing to keep in mind is that details like capitalization are important, that is, R is case sensitive.\nThus, function_name() is not the same as Function_name() or function_Name() (see what I did there?).\n\n2.1 Using functions\nFunctions are typically used by providing some information inside the brackets, usually data for the function to do work on or settings for the function. Function values and settings are assigned to function arguments and most functions have several arguments.\n\nfunction_name(argument_1 = value_1, argument_2 = value_2, ...)\nA general rule is that you INPUT information or data into function brackets that you want the function to do work and function OUTPUT is the work being done, sometimes including information output (like the results of a statistical test, or a plot).\n\nEach argument has a unique name\nArgument values are assigned using the equals sign =, the assignment operator\nEach argument is separated by a comma ,\nThe ... means there are additional arguments that can be used optionally (for now we can ignore those)\n\n\n\n\n2.2 Function names\nFinding functions by their names is often easy for very simple and common tasks. For example:\nmean() Calculates the arithmetic mean\nlog() Calculates the log\nsd() Calculates the standard deviation\nplot() Draws plots\nboxplot() Draws boxplots\nhelp() Used to access help pages\nYou get the idea…\n\nThe most important thing here is that you would generally get the help page up as a reference to what arguments are required and how to customize your function use. This is the key to learning R in the easiest way. That is, until you memorize the use and arguments for common functions."
  },
  {
    "objectID": "3-functions.html#using-functions-and-getting-help",
    "href": "3-functions.html#using-functions-and-getting-help",
    "title": "3 Functions & packages",
    "section": "3 Using functions and getting help",
    "text": "3 Using functions and getting help\nFor now, let’s assume that you know:\n\nWhat tasks you want to do (maybe outlined with pseudocode), and\nWhat function(s) can perform those tasks.\n\n\nTry this out in your own script:\n## A workflow for using functions ####\n\n## (make pseudocode of steps in comments)\n\n# Overall task: calculate the mean for a vector of numbers\n# Step 1: Code the vector of data - c() function\n# Step 2: Calculate the mean - mean() function\n# Step 3: Plot the data - boxplot()\n\n# Step 1: Code the vector of data - c() function\n\nhelp(c) # We use this a lot - it \"combines\" numbers\nc(2, 6, 7, 8.1, 5, 6) \n\n# Step 2: Calculate the mean - mean() function\n\nhelp(mean) \n# Notice under Usage, the \"x\" argument\n# Notice under Arguments, x is a numeric vector\n\nmean(x = c(2, 6, 7, 8.1, 5, 6)) # Easy\n\n# Step 3: Plot the data - boxplot()\n\nhelp(boxplot) # x again!\nboxplot(x = c(2, 6, 7, 8.1, 5, 6))\n\n# Challenge: Add an axis label to the y-axis - can you find the name of the argument?\n\n\n# without a y-axis label this is not a good graph\n\nboxplot(x = c(2, 6, 7, 8.1, 5, 6))"
  },
  {
    "objectID": "3-functions.html#r-packages",
    "href": "3-functions.html#r-packages",
    "title": "3 Functions & packages",
    "section": "4 R packages",
    "text": "4 R packages\nThere are a lot of R packages. These are “toolboxes” often built in the spirit of identifying a problem, literally making a tools that solves the problem, and then sharing the tool for other to use as well. In fact, all official R packages are “open source”, meaning that you may use them freely, but also you can improve them and add functionality. This section is about the basics of R packages that are additional to the base R installation.\nTypically, you only download a package once you identify you need to use functions in it. There are are several ways to accomplish this. We are going to practice 2 different ways, one with R code that is simple and will work no matter how you use R, and one that uses menus in RStudio.\n\n\n4.1 Finding, downloading and using packages\nFinding packages happens a variety of ways in practice. A package may be recommended to you, you might be told to use a particular package for a task or assignment, or you may discover it on the web.\nInstalling and loading packages with code\nThere are 2 steps here - installing, then loading. Installing is very easy to do using the install.packages() function. Loading a package making the functions in it available for use is done using the library() package. Basic usage of these functions is:\n# Step 1: install a package\n\nhelp(install.packages) # just have a look\ninstall.packages(pkgs = \"package_name\")\n\n# The package is downloaded from a remote repository, often\n# with additional packages that are required for use.\n\n# Step 2: load a package\n\nlibrary(\"package_name\")\n\n# Challenge: Install and load the \"ggplot2\" package, and then use help() to look at the help page for the function ggplot().  What kind of R object is required by the \"data\" argument?\n \n\n\n\n4.2 Installing and loading packages with the RStudio Packages tab\nYou can find the packages tab in RStudio in the lower left pane by default.\n\n\n\nPackages tab\n\n\n\nWhen you click on the Packages tab (A in the picture above), you can see a list of packages that are available to you (i.e., in RStudio desktop these have already been downloaded locally).\nIn order to load a package, you can find the package name in the list and click the radio button (B in the picture).\nTo install a package, you can click on the Install button (C in the image).\nYou should see the Install Packages window, where you can enter the name of a package for installation, searching the official Comprehensive R Archive Network (Repository (CRAN)) by default:\n\n\n\n\n\nInstall packages in RStudio"
  },
  {
    "objectID": "3-functions.html#practice-exercises",
    "href": "3-functions.html#practice-exercises",
    "title": "3 Functions & packages",
    "section": "5 Practice exercises",
    "text": "5 Practice exercises\n\n\n5.1\nExplain in your own words what the freq argument in the hist() function does. It often helps to practice trail and error to understand what is happening with data. Try experimenting with the data vector below with the hist() function to explore the freq argument:\nc(1,2,4,3,5,6,7,8,6,5,5,5,3,4,5,7)\n\n\n\n5.2\nTailor your code from the hist() example in problem 1 so that your histogram has a main title, axis labels, and set the col argument to “blue”. We are just scratching the surface with plot customization - try to incorporate other arguments to make an attractive graph.\n\n\n\n5.3\nUse the mean() function on the following data vector\nc(1,2,4,3,5,6,7,8,6,5,NA,5,3,4,5,7)\nYou will see an error message. The symbol “NA” has a special meaning in R, indicating a missing value. Use help() for the mean function and implement the na.rm argument to fix the problem. Show your code.\n\n\n\n5.4\nIn your own words, what value is required for the “d” argument in the pwr.t.test() function in the {pwr} package? Show the code involved including any appropriate comment code required to answer this question. (hint: you will probably need to install the package, load it, and use help() on the function name)\n\n\n\n5.5\nEvery official R package has a webpage on the Comprehensive R Archive Network (CRAN) and there are often tutorials called “vignettes”. Google the CRAN page for the package {ggplot2} and find the vignette called “Aesthetic specifications”. Read the section right near the top called “Colour and fill”.\nFollow the instructions to list all of the built-in colours in R and list them in the console. Ed’s personal favourite is “goldenrod”, index number [147]. Can you find the index number for “tomato2”?\n\n\n\n5.6\nWrite a plausible practice question involving the use of help() for an R function."
  },
  {
    "objectID": "3-functions.html#harper-adams-data-science",
    "href": "3-functions.html#harper-adams-data-science",
    "title": "3 Functions & packages",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis website supports students and staff at Harper Adams University and the MSc in Data Science for Global Agriculture, Food, and Environment led by Ed Harris."
  },
  {
    "objectID": "4-data.html",
    "href": "4-data.html",
    "title": "4 Data objects",
    "section": "",
    "text": "Data objects in ‘R’ space"
  },
  {
    "objectID": "4-data.html#data-objects-in-r",
    "href": "4-data.html#data-objects-in-r",
    "title": "4 Data objects",
    "section": "1 Data objects in R",
    "text": "1 Data objects in R\n\nImagine you are floating in space in the R Global Environment and any data object you can see, you can call on by name to manipulate with functions. Let’s call it R Space\n\nThe fundamental way to analyse data in R is to be able to manipulate it with code. In order to do that, we need a system of containers to store the data in. This page is about the rules used for storing data in data objects. We will examine basic data types at the same time as how R stores them. R is actually very flexible in the way it handles data, making it as easy as possible for non-programmers to get going.\n\n\n1.1 Objectives\n\nBasic data types in R, str()\nData with factors\nclass() and converting variables\nVector and Matrix fun\nPractice exercises"
  },
  {
    "objectID": "4-data.html#basic-data-types-str",
    "href": "4-data.html#basic-data-types-str",
    "title": "4 Data objects",
    "section": "2 Basic data types, str()",
    "text": "2 Basic data types, str()\n\n2.1 Storage containers types\nThe first things to examine are the way that R variables are named, and the organization system for storing the data. The organization part is particularly important, because it is used to actually access and use data.\nVariable names in R are simple and do not need to be “declared” like in some computing languages, and they can be almost anything, but there are a few rules. The R system has a built-in error message and warning message system (also known as the Passive-Aggressive Butler), which will usually give a hint when some of these rules are violated.\n\n\n\n2.2 The Global Environment\nOne of the things we notice when people begin using R, even if they are experienced in data analysis, is that they expect to “see” data and data sets, almost as they are physical things. This might be because of experience using Excel and seeing the visual representation of data in spreadsheets (indeed, a graphical representation of physical spreadsheets!).\nThe regular R system for interacting with data is a little more abstract which can be disconcerting to new users. Typical use is to create variables in code script files and, usually, the bring data into the Global Environment from external files on the local PC, or from the web. We will practice using the Global Environment is the main way to interact with data.\nYou can use the class() function to find out the variable type (using this this is a good idea, since R occasionally guesses the intended data type incorrectly).\n# Try this\n\nvariable_1 <- c(4,5,7,6,5,4,5,6,7,10,3,4,5,6) # a numeric vector\n\nvariable_2 <- c(TRUE, TRUE, TRUE, FALSE) # a logical vector\n\nvariable_3 <- c(\"Peter Parker\", \"Bruce Wayne\", \"Groo the Wanderer\") # a character vector\n\nclass(variable_1) # \"numeric\"\nclass(variable_2) # \"logical\"\nclass(variable_3) # \"character\"\n\nLook at the upper right pane of your RStudio and you should see something like:\n\n\n\n\nRStudio Global Environment\n\n\n\n\nThe Environment tab contains the Global Environment (labelled A in the picture above). There are some other tabs, but we will ignore these for now. The Global Environment itself contains information about the variables that are held in memory. If we think of this as “R Space”, a general rule is that if you can see a variable here in the Global Environment, you can manipulate it and work with it.\nNotice that there is quite a lot of information in the Global Environment about the actual variables (B in the picture). There is a column with the variable NAME (variable_1, variable_2, etc.), a column with the variable TYPES (num, logi, etc.), a column with the variable dimensions ([1:14] is an index like a unique “street address” for each of the 14 numeric values contained in “variable_1”)\n\n\n\n2.3 Naming conventions for variables\nVariable names:\n\nCan contain letters, numbers, some symbolic characters\nMust begin with a letter\nMust not contain spaces\nSome forbidden characters like math operators, “@”, and a few others\nShould be human-readable, consistent, and not too long\nCase sensitive\n\n# Try this\n\n## Variable name rules ####\n\n# Can contain letters, numbers, some symbolic characters\nx1 <- 5  # OK\n\nx2 <- \"It was a dark and stormy night\" # OK\n\nmy_variable_9283467 <- 1 # technically works, but hard to read\n\n# Must begin with a letter \n\nvarieties <- c(\"red delicious\", \"granny smith\") # OK\n\nx432 <- c(\"a\", \"b\") # OK\n\n22catch <- c(TRUE, TRUE, FALSE)  # nope\n\n# Must not contain spaces\nmy_variable <- 3 # OK\n\nmy.variable <- 4 # OK\n\nmyVariable <- 5 # OK\n\nmy variable <- 6 # nope\n\n\"my variable\" <- 7 # nope\n\n# Must not contain forbidden characters like \n# math operators, \"@\", and a few others\nmy@var <- 1 # nope\n\nmy-var <- 1 # nope\n\nmy=var <- 1 # nope\n\n# etc.\n\n# Should be human-readable, consistent, and not too long\n\nDiameter_Breast_Height_cm <- c(22, 24, 29, 55, 43) # legal but too long\n\nDBH_cm <- c(22, 24, 29, 55, 43) # much better\n\n#Case sensitive\nheight <- c(180, 164, 177) # OK\n\nHeight # Error: object 'Height' not found (notice capital H)\n\nheight # OK"
  },
  {
    "objectID": "4-data.html#data-with-factor",
    "href": "4-data.html#data-with-factor",
    "title": "4 Data objects",
    "section": "3 Data with factor()",
    "text": "3 Data with factor()\nSometimes you will need to analyze data that is a factor, where the different values are categories. Factors in R can be a cause for confusion, but there needn’t be problems if you understand them. The information here is a starting point, and we skip some complexities, but essentially\n\n3.1 Two types of factors, non-ordered and ordinal\nNon-ordered factors are simply categories and the levels are simply the names of the categories. By definition, non-ordered factor do not have a specific order! an example here might be plant varieties.\nOrdered factors have a specific order, which can be important for analysis or for graphing. You usually have to specify the order explicitly to get this right. An example here might be the days of the week, where the order of day is important."
  },
  {
    "objectID": "4-data.html#class-and-converting-variables",
    "href": "4-data.html#class-and-converting-variables",
    "title": "4 Data objects",
    "section": "4 Class() and converting variables",
    "text": "4 Class() and converting variables\nWe use the class() function to query what data category a variable is set too. R is pretty good at setting this correctly, but it is a good idea to check sometimes and occasionally you will have to manually set variable characteristics.\n\n# try this\n\n# non-ordered factor\nvariety <- c(\"short\", \"short\", \"short\",\n             \"early\", \"early\", \"early\",\n             \"hybrid\", \"hybrid\", \"hybrid\")\nclass(variety)  # \"character\", but this is really a factor...\nvariety # Notice the character strings are just printed out\n\nvariety <- factor(variety) # use factor() to convert the character vector to a factor\nclass(variety)  # now variety is a \"factor\"\nvariety # notice the output has changed\n\nNow, try some code manipulating ordered factors.\n# Ordered factors\nday <- c(\"Monday\", \"Monday\", \n         \"Tuesday\", \"Tuesday\", \n         \"Wednesday\", \"Wednesday\", \n         \"Thursday\")\nclass(day) # character\n\n#make day a factor\nday <- factor(day)\nclass(day)\nday # Notice the Levels: Monday Thursday Tuesday Wednesday\n\n# To set the order explicitly we need to set them explicitly\nhelp(factor) # notice the levels argument - it sets the order of the level names\n\nday <- factor(x = day, levels = c(\"Monday\", \"Tuesday\",\n                         \"Wednesday\", \"Thursday\"))\nday # Notice the level order now"
  },
  {
    "objectID": "4-data.html#vector-and-matrix-fun",
    "href": "4-data.html#vector-and-matrix-fun",
    "title": "4 Data objects",
    "section": "5 Vector and Matrix fun",
    "text": "5 Vector and Matrix fun\nVector and matrix data structures are two fundamental ways to arrange data in R. We have already looked at vectors, which store data in a single dimension.\nThere are actually a few different data organisation structures in R.\n\nVector - stores data in a single dimension from 1 to i, my_vec[i]\nMatrix - stores data in two dimensions 1 to i rows, 1 to j columns my_mat[i, j]\nArray - Three (or more) dimensions from 1 to to i, j, and k, my_array[i, j, k]\n\nVectors, Matrices and Arrays can only store the same TYPE of data.\n\n\n5.1 Vector practice\n# Try this\n\nmyvec1 <- c(1,2,3,4,5) # numeric vector\nmyvec1\nclass(myvec1) # see? I told ya!\n\nmyvec2 <- as.character(myvec1) #convert to character\nmyvec2 # notice the quotes\nclass(myvec2) # now character\n\nmyvec3 <- c(2, 3, \"male\")\nmyvec3 #notice the numbers now have quotes - forced to character...\n\nmyvec4 <- as.numeric(myvec3) #notice the warning\nmyvec4 # The vector element that could not be coerced to be a numeric was converted to NA\n\n\n\n5.2 Matrices\nMatrices can be quite useful - you can manipulate data into matrix form with the matrix() function. By default rows and columns are merely numbered, but they can be named as well.\n\n# Try this\n\nvec1 <- 1:16 # make a numeric vector with 16 elements\nvec1 \n\nhelp(matrix) #notice the ncol, nrow and byrow arguments\n\nmat1 <- matrix(data = vec1, ncol = 4, byrow = FALSE) #byrow = FALSE is the default\n\nmat1 # Notice the numbers filled in by columns\ncolnames(mat1) # The Columns and Rows have no names\n\ncolnames(mat1) <- c(\"A\", \"B\", \"C\", \"D\") # Set the column names for mat1\ncolnames(mat1)\n\nmat1 # Yep the columns shows names\n\n\n\n5.3 Matrix challenge\n# Challenge 1: Set the Row names for Mat1 using the rownames() function\n\n# Challenge 2: make a matrix with 3 rows with the following vector, \n# so the the first COLUMN contains the numbers 2, 5, and 9, in the order,\n# for rows 1, 2, and 3 respectively:\n\nvec2 <- c(2,3,5,4,5,6,7,8,9,5,3,1)"
  },
  {
    "objectID": "4-data.html#practice-exercises",
    "href": "4-data.html#practice-exercises",
    "title": "4 Data objects",
    "section": "6 Practice exercises",
    "text": "6 Practice exercises\n\n\n6.1\nCreate a vector named my_var1 that contains the following 6 integers: 3, 6, 12, 7, 5, 1. Create a Second vector called my_var2 that contains the following 2 integers: 2, 3. Evaluate the expression my_var1 + my_var 2. Explain the output in terms of R mechanics in your own words.\n\n\n\n6.2\nCreate a character vector with the names of the 12 months of the year. Convert the vector to a factor, with the month names in chronological order. Show your code.\n\n\n\n6.3\nWhat is wrong with the following code? Describe, show the code, and justify a fix for the problem in your own words.\nmymat <- matrix(data = c( 12,   23,   45,\n                         \"34\", \"22\", \"31\"))\n\n\n\n6.4\nUse the array() function to make a 2 x 2 x 3 array to produce the following output:\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n, , 3\n\n     [,1] [,2]\n[1,]    9   11\n[2,]   10   12\n\n\n\n6.5\nShow the code to make the following matrix:\n       cat dog\nmale    22  88\nfemale  71  29\n\n\n\n6.6\nWrite a plausible practice question involving the use of the matrix() and vector() functions."
  },
  {
    "objectID": "4-data.html#harper-adams-data-science",
    "href": "4-data.html#harper-adams-data-science",
    "title": "4 Data objects",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis website supports students and staff at Harper Adams University and the MSc in Data Science for Global Agriculture, Food, and Environment led by Ed Harris."
  },
  {
    "objectID": "5-data-frames.html",
    "href": "5-data-frames.html",
    "title": "5 Data frames",
    "section": "",
    "text": "Like your room, data should be tidy"
  },
  {
    "objectID": "5-data-frames.html#data-frames-in-r",
    "href": "5-data-frames.html#data-frames-in-r",
    "title": "5 Data frames",
    "section": "1 Data frames in R",
    "text": "1 Data frames in R\nNB for this page we assume you have access to Microsoft Excel. However, similar spreadsheet software (like Libre Office Calc) will work fine.\n\n\nThe first step in using R for data analysis is getting your data into R. The first step for getting your data into R is making your data tidy.\n\nThe commonest question we have experienced for new users of R who want to perform analysis on their data is how to get data into R. There is good news and bad news. The good news is that it is exceedingly easy to get data into R for analysis, in almost any format. The bad news is that a step most new users find challenging is taking responsibility for their own data.\nWhat we mean here is that best practice in data management involves active engagement with your dataset. This includes choosing appropriate variable names, error checking, and documenting information about variables and data collection. We also aim to avoid proliferation of excessive dataset versions, and, worst of all, embedding graphs and data summaries into Excel spreadsheets with data.\n\n\n1.1 Objectives\n\nCommon data file types\nExcel, data setup, and the Data Dictionary\nGetting data into R\nManipulating variables in the Data Frame\nPractice exercises"
  },
  {
    "objectID": "5-data-frames.html#tidy-data-concept",
    "href": "5-data-frames.html#tidy-data-concept",
    "title": "5 Data frames",
    "section": "2 Tidy data concept",
    "text": "2 Tidy data concept\n\n2.1 The Tidy Data concept\nA concept to streamline data preparation for analysis is Tidy Data. The basic idea is to format data for analysis in a way that\n\nArchives data for reproducibility of results, and\nMakes the data transparent to colleagues or researchers by documenting a data dictionary.\n\n\nThis page is all about the tidy data concept and a simple recipe for best practice to prepare data for analysis and to get data into R.\nThe definition of Tidy Data is generally attributed to Wickham (2014), and is based on the idea that with a few simple rules, data can be archived for complete reproducibility of results. This practice benefits any user because it facilitates collaboration at the same time as documenting both data and analysis methods for value to future use.\nThe essentials of Tidy Data are:\n\nEach variable should be in a column\nEach independent observation should be in a row\nA Data Dictionary should be associatied with the dataset, such that completely reproducible analysis is possible"
  },
  {
    "objectID": "5-data-frames.html#common-data-file-types",
    "href": "5-data-frames.html#common-data-file-types",
    "title": "5 Data frames",
    "section": "3 Common data file types",
    "text": "3 Common data file types\nThe best file type for the majority of people to archive data for analysis is in a plain text Comma Separated Values (CSV or .csv) file, or just an Excel Spreadsheet. Best practice in contemporary scientific data analysis dictates that proprietary data formats should be avoided, like those produced by SPSS, Genstat, Minitab or other programs.\nThe reason for this is that data stored in those formats is not necessarily useful to people who do not have access to the software, and that for archiving purposes, such software file formats tend to change over time. While Excel is a proprietary format, we find that it is is easy to use, (almost completely) ubiquitous, and relatively resilient to backwards compatibility issues. Thus, sticking to CSV or Excel is a rule you should have a very good reason if you choose to break from it.\nWe recommend using Excel to store data with the goal (for simple datasets and analyses) of having one table for the actual data, in Tidy Data format, and a second tab consisting of a Data Dictionary where each variable is described in enough detail to completely reproduce any analysis. Generally, no formatting or results should ever be embedded in an Excel spreadsheet that is used to store data."
  },
  {
    "objectID": "5-data-frames.html#excel-data-setup-and-the-data-dictionary",
    "href": "5-data-frames.html#excel-data-setup-and-the-data-dictionary",
    "title": "5 Data frames",
    "section": "4 Excel, data setup, and the Data Dictionary",
    "text": "4 Excel, data setup, and the Data Dictionary\n\n4.1 Tidy Data and Excel\nFor this section, you should download the following files in Excel (.xlsx) format:\n\nTidy Data example Excel file\nThe exact same information as un-Tidy Data\n\n\nThe aphid experiment\n\nYou are contacted by someone who wants help with data analysis and they give you some information about their experiment. They are interested in how diet affects the production of an important metabolite in pest aphids. They designed an experiment with a control treatment where aphids were allowed to feed on plain plants, another treatment where their diet was supplemented with one additive, “AD”, and a third treatment where their diet was supplemented with two additives, “AD” and “SA”. Another factor was aphid Genus, where individuals from the genera Brevicoryne and Myzus were tested. Three replicates of each treatment combination were performed:\n\n\naphid genus [2 levels] \\(\\times\\) food treatment [3 levels].\n\n\nThe metabolite of interest was measured with a spectrometer using three individual aphids from each replicate. The spectrometer peak area (an arbitrary scale) represents the total amount of the metabolite, which was converted to a real scale of metabolite total concentration. Finally, this total concentration was divided by 3 to estimate the concentration of the metabolite in each individual aphid.\n\n\n\n\n4.2 Untidy data\nHave a look at the file 5-untidy.xlsx in Excel.\n\nThe aphid dataset is fairly small and it is readable by humans, but in its current form it is not usable for analysis in R or other statistical software and there are a few ambiguous aspects which we will explore and try to improve.\n\nUntidy\n\n\n\nUntidy data\n\n\n\n\nThe file contains embedded figures and summary tables\nThere is empty white space in the file (Row 1 and Column A)\nThe variable names violate several naming conventions (spaces, special characters)\nMissing data is coded incorrectly (Row 13 was a failed data reading, but records zeros for the actual measurements)\nConversion information accessory to the data is present (Row 3)\nThere is no Data Dictionary (i.e. explanation of the variables)\nThe Aphid and Diet treatments are “confounded” in their coding\nWhat the heck is the “RT” column (most of the values are identical)\n\n\nNow, have a look at the Tidy Data version of the data file.\n\n\n\nTidy version of the data\n\n\n\n\nThe embedded figures have been removed\nThe white space rows and columns have been removed\nThe variable names have been edited but still are equally informative\nMissing data is coded correctly with “NA”\nThe conversion info has been removed and placed in the Data Dictionary\nA complete Data Dictionary on a new tab (“dictionary”) was added, explaining each variable\nThe aphid and food treatment variables were made separate\n\n\nTidy Data version Data Dictionary tab\n\n\n\nTidy data dictionary\n\n\n\nNotice in the Data Dictionary how there is a row for each variable with the name of the variable and an explanation for each variable.\n\n\n\n4.3 CSV files\nOnce your data is tidy, it is very easy to read in Excel data files, or they can be exported into a text file format like CSV (comma separated values) to read straight into R or other programs.\nHave a look at the Tidy Data dataset in .csv file format.\nOpen it with a plain text editor (e.g. Notepad in Windows, or similar). You will notice that each column entry is separated from others with a comma ,, hence the name Comma Separated Values!\n\nTidy csv\n\n\n\nTidy csv"
  },
  {
    "objectID": "5-data-frames.html#getting-data-into-r",
    "href": "5-data-frames.html#getting-data-into-r",
    "title": "5 Data frames",
    "section": "5 Getting data into R",
    "text": "5 Getting data into R\nWe still need to actually “read data into R” from external files. There are a very large number of ways to do this and most people eventually find their own workflow. We think it is best for most people to use Excel or CSV files in Tidy Data format.\nThe basics of reading external files from a script is to to use the read.xlsx() function in the {openxlsx} package (you will probably need to install this with the install.packages() function), or else to use read.csv() that comes standard in base R. We will briefly try both.\n\n\n5.1 Working directory\n\nBest practice when working with files is to formally set your “working directory”. Basically, this tells R where your input (i.e. data) and output (like scripts or figures) files should be.\n\n\nThere are several viable ways to set your working directory in R, e.g. via the Session menu:\n\n\n\nWorking directory\n\n\n\nHowever, the best way to do this this is to set your working directory using code with the setwd() function. Here we should a workflow for Windows, which is similar on other computer systems. We consider the step of setting a working directory essential for best practice.\nIf you are unfamiliar with how to obtain the path to your working directory, open windows explorer, navigate to the folder you wish to save your script, data files and other inputs and outputs. You can think of this folder as one that contains all related files for e.g. a data analysis project, or perhaps this bootcamp module!\n\n\n\n\nYour directory window might look similar to this\n\n\n\nNotice the folder “view” is set to “Details”, and also notice that the folder options are set to “Show file extensions”. We recommend setting your own settings like this (if using Windows Explorer).\nThe pointer is indicated in the circle marked A in the picture above.\nLeft click the area to the right of the folder text once (where the pointer is in the picture above) and you should see something similar to the figure below, where the folder path is displayed and the text is automatically selected.\n\n\n\n\nYour selected file path might look similar to this\n\n\n\nAssuming you have opened the File Explorer in your working directory or navigated there, the selected PATH is the working directory path which you can copy (Ctrl + c in Windows). In your script, you can now use getwd() to get and print your working directory path, and setwd(), which takes a single character string of the path for your working directory for the argument dir , to set it.\n\n\nR file paths use the forward slash symbol “/” to separate file names. A very important step for Windows users when setting the working directory in R is to change the Windows default “” for forward slashes…\n\n\n\n\n5.2 Read in your first file\nYou need this for the following code if you did not already download it above: .xlsx data file\n# Try this\n\ngetwd() # Prints working directory in Console\n\nsetwd(\"D:/Dropbox/git-rstats-bootcamp/website/data\")\n\n# NB the quotes\n# NB the use of \"/\"\n# NB this is MY directory - change the PATH to YOUR directory :)\n\ngetwd() # Check that change worked\n\n## Read in Excel data file\n\n\ninstall.packages(openxlsx, dep = T) # Run if needed\n\nlibrary(openxlsx) # Load package needed to read Excel files\n\n# Make sure the data file \"5-tidy.xlsx\" is in your working directory\nmy_data <- read.xlsx(\"5-tidy.xlsx\")\n\nAll being well, you should see the following data object in your Global Environement. Note the small blue button (A, circled below) you can press to espand the view of the variables in your data frame.\n\n\n\nData in your Global Environment\n\n\n\nNote that the same procedure works with Comma Separated Values data files, and other kinds of files that you want to read into R, except that the R function used will be specific to the file type. E.g., read.csv() for CSV files, read.delim for TAB delimited files, or read.table() as a generic function to tailor to many types of plain text data files (there are many others, but this is enough for now)."
  },
  {
    "objectID": "5-data-frames.html#manipulating-variables-in-the-data-frame",
    "href": "5-data-frames.html#manipulating-variables-in-the-data-frame",
    "title": "5 Data frames",
    "section": "6 Manipulating variables in the Data Frame",
    "text": "6 Manipulating variables in the Data Frame\n\nNow that there is a data frame in your working environment, we can start working with the variables. This is a good time to think about the “R Space” metaphor. You are floating in R Space and you can see a data frame called my_data. You cannot see inside the container, so we will look at methods of accessing the data inside by name…\n\n\n\n6.1 Data manipulation in R\n\nThe names() function\nThe use of the $ operator for data frames\nThe use of the str() function for data frames\nThe use of the index operator [ , ]\nThe use of the attach() function\n\n\nCarefully use the follow code and try some data manipulation on your own.\n\n\n6.2 class()\n# Try this\n\nclass(my_data) # data.frame, a generic class for holding data\n\n\n6.3 names()\nThe names() function returns the name of attributes in R objects. When used on a data frame it returns the names of the variables.\n# Try this\nnames(my_data)\n\n\n6.4 $ operator for data frames\nThe $ operator allows us to access variable names inside R objects. Use it like this:\ndata_object$variable_name\n# Try this\n\nconc.ind # Error because the variable conc.ind is INSIDE my_data\n\nmy_data$conc.ind\n\n\n6.5 str()\nThe str() function returns the STRUCTURE of a data frame. This includes variable names, classes, and the first few values\n# Try this\nstr(my_data) \nThe output similar to the graphical Global Environment view in RStudio. Note the conc.ind variable is classed numeric\nNote the treatment variable is classed as character (not a factor)\n\n\n\n6.6 [ , ] the index operator\nThe index operator allows us to access specified rows and columns in data frames (this works exactly the same in matrices and other indexed objects).\n# Try this\nmy_data$conc.tot # The conc.tot variable with $\nmy_data$conc.tot[1:6] # each variable is a vector - 1st to 6th values\n\nhelp(dim)\ndim(my_data) # my_data has 18 rows, 6 columns\n\nmy_data[ , ] # Leaving blanks means return all rows and columns\n\nnames(my_data) # Note conc.tot is the 6th variable\n\nnames(my_data)[6] # Returns the name of the 6th variable\n\nmy_data[ , 6] # Returns all rows of the 6th variable in my_data\n\n# We can explicitly specify all rows (there are 18 remember)\nmy_data[1:18 , 6] # ALSO returns all rows of the 6th variable in my_data\n\n# We can specify the variable names with a character\nmy_data[ , \"conc.tot\"]\nmy_data[ , \"conc.ind\"]\n\n# Specify more than 1 by name with c() in the column slot of [ , ]\nmy_data[ , c(\"conc.tot\", \"conc.ind\")] \n\n\n6.7 attach()\nThe attach() function makes variable names available for a data frame in R space\n# Try this\nconc.ind # Error; the Passive-Aggressive Butler doesn't understand...\n\nattach(my_data)\nconc.ind # Now that my_data is \"attached\", the Butler can find variables inside\n\nhelp(detach) # Undo attach()\ndetach(my_data)\nconc.ind # Is Sir feeling well, Sir?"
  },
  {
    "objectID": "5-data-frames.html#practice-exercises",
    "href": "5-data-frames.html#practice-exercises",
    "title": "5 Data frames",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\n\nButterfly data in xlsx format\nThe data are from a small experiment measuring antenna length in butterflies manipulating diet in both sexes.\n\n7.1\nDownload the data file above and place it in a working directory. Set your working directory. Read in the data file and place it in a data frame object named data1. After examining the data, use mean() to calculate the mean of the variable length and report the results in a comment to two decimal points accuracy. Show your R code.\n\n\n\n7.2\nShow the code to convert the diet variable to an ordinal factor with the order “control” > “enhanced”, and the sex variable to a plain categorical factor.\n\n\n\n7.3\nShow code for two different variations of using only the [ , ] operator with your data frame to show the following output:\n       diet length\n8   control      6\n9   control      7\n10  control      6\n11 enhanced      8\n12 enhanced      7\n13 enhanced      9\n\n\n\n7.4\nShow code to read in a comma separated values data file that does not have a header (first row containing variable names).\n\n\n\n7.5\nDescribe in your own words what the attach() function does.\n\n\n\n7.6\nWrite a plausible practice question involving any aspect of manipulation of a data frame."
  },
  {
    "objectID": "5-data-frames.html#harper-adams-data-science",
    "href": "5-data-frames.html#harper-adams-data-science",
    "title": "5 Data frames",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis website supports students and staff at Harper Adams University and the MSc in Data Science for Global Agriculture, Food, and Environment led by Ed Harris."
  },
  {
    "objectID": "6-data-manipulation.html",
    "href": "6-data-manipulation.html",
    "title": "6 Data subsetting and manipulation",
    "section": "",
    "text": "Command data and you are powerful"
  },
  {
    "objectID": "6-data-manipulation.html#subsetting-and-manipulation-data-sumo",
    "href": "6-data-manipulation.html#subsetting-and-manipulation-data-sumo",
    "title": "6 Data subsetting and manipulation",
    "section": "1 Subsetting and Manipulation (data sumo)",
    "text": "1 Subsetting and Manipulation (data sumo)\n\nWith a good basic set of moves for subsetting and manipulating data, you can overpower any dataset no matter how large and powerful they may be. Then, you will have strong data Sumo.\n\nSubsetting and manipulating data is probably the commonest activity for anyone who works with data. This is a core activity for exploratory data analysis, but is also extensively used in simple data acquisition, analysis and graphing, while also being related to more general data manipulating activities, for example database queries. This page is an introduction to the core syntax and some of the tools for manipulating and subsetting data in R.\n\n\n1.1 Objectives\n\nIndexing concept\nUsing which() and subsetting\nSelection on data.frame objects\nUsing aggregate()\nPractice exercises"
  },
  {
    "objectID": "6-data-manipulation.html#indexing-concept",
    "href": "6-data-manipulation.html#indexing-concept",
    "title": "6 Data subsetting and manipulation",
    "section": "2 Indexing concept",
    "text": "2 Indexing concept\n\nIf you would like to slice and dice your data, you will need to learn all about indexing!\n\nThe basics of the indexing concept in R syntax is very simple, where data storage objects like vectors (1 dimension), matrices (2 dimensions) and arrays (3 or more dimensions) store individual data values that can be accessed by the “address” of the dimension(s).\n\n\n2.1 How indexing works\nSay you have a numeric vector called my_vector that has 10 values. The index values will be 1 to 10, with each value corresponding consecutively to the data value at that position.\nmy_vector <- c(11.3, 11.2, 10.4, 10.4, 8.7, \n               10.8, 10.5, 10.3, 9.7, 11.2)\n  \nmy_vector\n[1] 11.3 11.2 10.4 10.4 8.7 10.8 10.5 10.3  9.7 11.2\n\nNotice the [1] in the R console output? This indicates the index of value right next to it and the R system will provide an index value for longer vectors as the wrap in the console. If we could see the actual index values it would look something like this:\n> my_vector\n [1] 11.3 11.2 10.4 10.4  8.7 10.8 10.5 10.3  9.7 11.2\n#     ^    ^    ^    ^    ^    ^    ^    ^    ^    ^\n#     1    2    3    4    5    6    7    8    9    10\n\n\n\n2.2 Vectors\nYou can create vector subsets by manipulating the index. Vector objects have indices in 1 dimension. For example, my_vector[1:i], where i is the length of the vector.\n## Vectors ####\n# Try this\n\nmy_vector <- c(11.3, 11.2, 10.4, 10.4, \n               8.7, 10.8, 10.5, 10.3, 9.7, 11.2)\n\n# Return all values\nmy_vector        # Typical way\nmy_vector[ ]     # Blank index implies all index values\nmy_vector[ 1:10] # Returns all index values explicitly\n\n# Return the first 3 values\n1:3 # Reminder of the function of the colon operator \":\"\nmy_vector[ 1:3] # Notice consecutive indices can use the \":\" operator\n\n# Return 5th and 9th values\nmy_vector[ c(5, 9)] # Notice we have to place non-consecutive index values in the c() function\n\n\n\n2.3 Matrices\nMatrix objects have 2 dimensions denoted as my_matrix[1:i, 1:j], where i is the number of rows and j is the number of columns.\n## Matrices ####\n# Try this\n\nmy_matrix <- matrix(data = c(2,3,4,5,6,6,6,6),\n                    nrow = 2, byrow = T)\n\nmy_matrix # notice how the arguments arranged the data\n\n# Flash challenge: make a matrix with the same data vector above to look like...\n#      [,1] [,2]\n# [1,]    2    6\n# [2,]    3    6\n# [3,]    4    6\n# [4,]    5    6\n\n# \"Slicing\" out a row or column\nmy_matrix[1,  ] # Slice out row 1\nmy_matric[ , 3] # Slice out column 3\n\n# Matrix columns and rows often have names\nnames(my_matrix) # No names yet\n\nnrow(my_matrix) # Returns number of rows (useful for large matrices)\nrownames(my_matrix) # No row names; 2 rows, need two names\n\nrownames(my_matrix) <- c(\"dogs\", \"cats\")\nmy_matrix # Now the rows have names!\nrownames(my_matrix) # Get them this way too!\n\n# Flash challenge: Name the columns of my_matrix \"a\", \"b\", \"c\", \"d\" with colnames()\n\nmy_matrix\n\n# Should look like this:\n#      a b c d\n# dogs 2 3 4 5\n# cats 6 6 6 6\n\n# You can also slice out matrix portions by name\nmy_matrix[\"dogs\", c(\"b\", \"d\")]\n\n# Finally, functions act on values, not the index\nmean(my_matrix[\"dogs\", c(\"b\", \"d\")])\n\n\n\n2.4 Arrays\nArrays are data objects with more than 2 dimensions (well, technically a matrix with 2 dimensions is also an array, but let’s ignore that for now). Array dimensions are denoted as my_array[1:i, 1:j, 1:k], where i is the number of rows and j the columns and k the “depth” of i * j.\n\n\n\nmy_array\n\n\n## Arrays ####\n# Try this\n\n# help(runif)\n# help(round)\n# Try it to see what it does... \nmy_vec <- round(runif(n = 27, min = 0, max = 100), 0)\nmy_vec # See what we did there?\n\nlength(my_vec) # Just checking\n\nmy_array <- array(data = my_vec,\n                  dim = c(3, 3, 3))\nmy_array\n\n# Flash challenge: \n# Specify and print the 1st and 3rd  slice of the k dimension of my_array\n# Assuming my_array has dimensions i, j, k like my_array[i,j,k]"
  },
  {
    "objectID": "6-data-manipulation.html#which-and-subsetting",
    "href": "6-data-manipulation.html#which-and-subsetting",
    "title": "6 Data subsetting and manipulation",
    "section": "3 which() and subsetting",
    "text": "3 which() and subsetting\nSubsetting data objects is done by exploiting the index system. We usually do this by either specifying index values explicitly (effective, but it requires that you know A LOT about the data object), or by constructing queries that choose subset of data based on particular values. The which() function is a powerful way to construct queries.\n# Try this\nhelp(which) # Notice how the x argument is required to be a LOGICAL vector?\n\n# Make a NUMERIC vector\nvector_a <- c(3, 4, 5, 4, 3, 4, 5, 6, 6, 7)\n\n# Use a boolean phrase to ask which elements of vector_a are greater than 5\nvector_a > 5 # Interesting... it is a LOGICAL vector!\n\n# which() will return the index values of TRUE values\n# In other words, WHICH values in vector_a are greater than 5?\nwhich(vector_a > 5)\n\nWhat is the point of all this? THE POINT is to be able to use expressions to obtain indices and values in data structures…\n# What VALUES in vector_a are > 5?\nvector_a[which(vector_a > 5)]\n\n# This also works on vectors of other types\n# Consider a character vector\nchar_vec <- c(\"wheat\", \"maize\", \"wheat\", \"maize\", \"wheat\", \"wheat\")\n\n# Which elements are equivalent to \"wheat\"?\nchar_vec == \"wheat\"\nwhich(char_vec == \"wheat\")\n\nchar_vec[ which(char_vec == \"wheat\")] # This works\nchar_vec[ char_vec == \"wheat\"]        # Same output\n\n# Flash challenge: Explain in your own words why \n# the previous 2 lines of code have identical output?\n\nWe are just beginning to scratch the surface of possibilities with the which() function. Keep this function in mind and practice it when you can."
  },
  {
    "objectID": "6-data-manipulation.html#selection-on-data.frame-objects",
    "href": "6-data-manipulation.html#selection-on-data.frame-objects",
    "title": "6 Data subsetting and manipulation",
    "section": "4 Selection on data.frame objects",
    "text": "4 Selection on data.frame objects\n\nData frames are the ultimate data object for getting, storing, organizing and analyzing data. A good scientist must learn to communicate the subtlety of data. A good statistician must learn not to underestimate the subtletly of data. A good student must learn that\n\nsubtlety may exist, even in simple data.\nThere are a couple of data object types that have a special characteristic in that they store data of different types, where vectors, matrices and arrays can only store one type of data (e.g., numeric, character, logical, etc.). The special data objects that can contain multiple data types are list objects, and data frames. Here we will focus on data frames.\nData frames can have different vector types arranged by column but there is a constraint that each vector must be the same length, that is, each ROW is considered an observation for each variable value (though there may be missing data coded by NA).\nThere are a few ways to think about selecting values in a data frame. The first is simply to access values through the variable names, which can either be done by using the data frame name with the $ operator and the variable name, or by using the [ , ] syntax with either the variable name or the column number of the variable of interest see here.\nA second powerful way to access variables in a data frame is by selecting particular rows of a data frame. This may be done by selecting the rows of a data based on values of one or more variables. We will practice doing this using which(), the [ , ] syntax, and boolean phrases is the following code block.\nFor the following section, we will use the OrchardSprays dataset that exists as a data frame in the in-built {datasets} package. You can use help(OrchardSprays) to see the help page (the help page is characteristically terse, so some description is given here).\n\n\n4.1 OrchardSprays data\nThis is a classic dataset based on an experiment looking at how a chemical additive could be used to deter honeybees from being attracted to crops and subsequently killed by pesticides.\nThe experiment involved having a treatment consisting of adding a “lime sulfur emulsion” (honeybee deterrent) in increasing concentrations to a sucrose solution. The treatment variable had 8 levels including a control (no deterrent) and 7 other levels with increasing concentration of the deterrent. The treatment levels were named A (the highest amount of deterrent), B (second highest deterrent) through to G (lowest deterrent) and H (control - no deterrent) The decrease variable was a measure of the quantity of sucrose solution that was taken by honeybees (the prediction here is that higher concentrations of the deterrent should result in a lower decrease in the sucrose solution).\nThe experiment involved a Latin Square design, with the order of the 8 treatments arranged randomly in an array of 8 columns (the purpose of this design is to randomize any effect of the treatment ORDER or POSITION on the response variable). This resulted in an 8 row by 8 column experiment. The response was measured after placing 100 honeybees into an experimental chamber with the 64 containers of sucrose solution.\n\n## OrchardSprays ####\n## Understand the data - an important step\n\n# Try this\n# Load the OrchardSpray data using the data() function\ndata(OrchardSprays) # Should see OrchardSprays <promise> in the Global Env.\n\n# Look at the data head()\nhead(OrchardSprays) # First 6 rows\n\n# Look at variable types with str()\nhelp(str) # Good function to see info about data object\nstr(OrchardSprays)\n\n# First let's just look at the data\n# Don't worry too much about the code for these graphs if you have not encountered it before\nboxplot(decrease ~ treatment, data = OrchardSprays, \n        main = \"The pattern fits the prediction\",\n        ylab = \"Amount of sucrose consumed\",\n        xlab = \"Lime sulpher treatment amount in decreasing order (H = control)\")\n\n# This is the experimental design\n# Latin Square is kind of like Sudoku\n# No treatment can be in row or column more than once\nplot(x = OrchardSprays$colpos,  # NB use of $ syntax to access data\n     y = OrchardSprays$rowpos, \n     pch = as.character(OrchardSprays$treatment),\n     xlim = c(0,9), ylim = c(0,9),\n     main = \"The Latin Square design of treatments\",\n     xlab = \"\\\"Column\\\" position\",\n     ylab = \"\\\"Row\\\" position\")\n\n\n\n4.2 Practice selecting parts a data frame\nSelecting particular parts of a data frame based on the values of one variable is a common and extremely useful task.\n## Practice selecting parts a data frame ####\n\n# Select the rows of the dataset for treatment \"D\"\n\n# (Pseudocode steps to solve) \n# Break it down to make small steps easy to read\n\n# 01 Boolean phrase to identify rows where treatment value is \"D\"\n# 02 which() to obtain index of TRUE in boolean vector\n# 03 Exploit [ , ] syntax with data frame object to slice out rows\n\n# 01 Boolean phrase\nOrchardSprays$treatment # Just print variable to compare visually to boolean\nOrchardSprays$treatment == \"D\" # logical vector - TRUE in \"D\" positions\n\n# 02 which()\nwhich(OrchardSprays$treatment == \"D\") # Index of TRUE values\nmy_selection <- which(OrchardSprays$treatment == \"D\") # Place index in a variable\nmy_selection # Just checking\n\n# 03 Exploit [ , ] syntax with data frame object to slice out rows\nOrchardSprays[my_selection, ]\n\n# Flash challenge: Select and print all rows at \"colpos\" values of 2\n\n\n\n4.3 Selection based on more than one variable value\nUsing the basic building blocks of boolean selection, more complex rules for selecting data can be made.\n## Compound boolean for selection ####\n\n# Select all rows of the data frame where \n# rowpos equals 4 OR 6 AND treatment equals \"A\" OR \"H\"\n# What we expect is exactly 2 values (A or H) for each powpos (4 or 6)\n\n# rowpos 4 and 6\nOrchardSprays$rowpos == 4 # The 4s\nOrchardSprays$rowpos == 6 # The 6s\n\nOrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6 # All together\n\n# now with which()\nwhich(OrchardSprays$rowpos == 4) # The 4s\nwhich(OrchardSprays$rowpos == 6) # The 6s\n\nwhich(OrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6) # All together\n\n# treatment A and H\nwhich(OrchardSprays$treatment == \"A\" | OrchardSprays$treatment == \"H\") # All together\n\n# Now we need the intersection of value that are in both our which() vectors\n\nwhich((OrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6) &  # It works\n        (OrchardSprays$treatment == \"A\" | OrchardSprays$treatment == \"H\") ) \n  \n# NB this is a long way of spelling out our selection, \n# but trying to be very explicit with what is going on\n\nmy_selec2 <- which((OrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6) &  \n                     (OrchardSprays$treatment == \"A\" | OrchardSprays$treatment == \"H\") ) \n\nOrchardSprays[my_selec2, ] # Double check it works and is similar to expectation...\n\n# Flash challenge: Calculate the mean of decrease for treatment \"A\" \n# and the mean of decrease for treatment \"H\""
  },
  {
    "objectID": "6-data-manipulation.html#aggregate-function",
    "href": "6-data-manipulation.html#aggregate-function",
    "title": "6 Data subsetting and manipulation",
    "section": "5 aggregate() function",
    "text": "5 aggregate() function\nWe often may wish to summarize parts of a data set according to some index of variable values. A very convenient tool for the is the aggregate() function, which we will practice here.\nhelp(aggregate)\n\n# A few important things to note about how this function works:\n# The \"x\" argument is a data object you input, but should only contain numeric values usually\n# If a data.frame object is input as x, a data.frame object is the output\n# The \"by\" argument must be a list() object and can be one or more indices\n# The FUN argument is the name of the function that will act on the \"x\" argument data\n\n# Let's try a few examples\n\n## 1 calculate the mean of decrease by treatment in OrchardSprays\n\naggregate(x = OrchardSprays$decrease,\n          # NB use of list() and naming it \"treatment\"\n          by = list(treatment = OrchardSprays$treatment), \n          FUN = mean)\n\n# we can \"recycle\" the code above to apply different functions\n# standard deviation with sd()\naggregate(x = OrchardSprays$decrease,\n          # NB use of list() and naming it \"treatment\"\n          by = list(treatment = OrchardSprays$treatment), \n          FUN = sd)\n\n# Range with range()\naggregate(x = OrchardSprays$decrease,\n          # NB use of list() and naming it \"treatment\"\n          by = list(treatment = OrchardSprays$treatment), \n          FUN = range)\n\n# What if we want several summary statistics?\n\naggregate(x = OrchardSprays$decrease,\n          # NB use of list() and naming it \"treatment\"\n          by = list(treatment = OrchardSprays$treatment), \n          # NB use of function() \n          FUN = function(x) c(mean = mean(x), # Add naming\n                              sd = sd(x), \n                              range = range(x)))\n\n## Example of use of aggregate object\n# Say you would like to graph a barplot of the MEAN of decrease by treatment\n# and you would like to show STANDARD DEVIATION error bars\n\n# Make data frame with summary values using aggregate()\nmy_mean <- aggregate(x = OrchardSprays$decrease,\n                        by = list(treatment = OrchardSprays$treatment), \n                        FUN = mean)\n\nmy_sd <- aggregate(x = OrchardSprays$decrease,\n                   by = list(treatment = OrchardSprays$treatment), \n                   FUN = sd)\n\nmy_mean\nmy_sd\n\n# Tidy things up in a new data frame using data.frame()\n# Take care of naming variables for clarity\nhelp(data.frame) # Continue using help() as a good habit\nnew_data <- data.frame(treatment = my_mean$treatment,\n                       mean = my_mean$x,\n                       sd = my_sd$x)\nnew_data # Looks good\n\n# There is a lot going on in the following code\n# The point is to show what is possible\n\n(bar_centers <- barplot(new_data$mean,#   use mean for barheight\n                        ylim = c(0, 115),\n                        ylab = \"Mean solution decrease (+- 1 SD)\",\n                        xlab = \"Treatment\"))\n# NB bar_centers holds the numerical position value of the bars...\n\nhelp(arrows)  # Use to draw error bars\narrows(x0 = bar_centers, \n       x1 = bar_centers,\n       y0 = new_data$mean , # start error bar at top of the bar!\n       y1 = new_data$mean + new_data$sd, # end error bar here!\n       angle = 90,\n       length = 0.1)\n\n# Last step: label the x axis\naxis(side = 1,\n     at = bar_centers,\n     labels = new_data$treatment)\n     \n# Flash challenge: Draw a new barplot by recycling the code above\n# This time, add error bars showing on both the top and the bottom of the mean values"
  },
  {
    "objectID": "6-data-manipulation.html#practice-exercises",
    "href": "6-data-manipulation.html#practice-exercises",
    "title": "6 Data subsetting and manipulation",
    "section": "6 Practice exercises",
    "text": "6 Practice exercises\nFor the following exercises, use the trees dataset built into R, which has Girth, Height and Volume variables for 31 Black Cherry trees.\n# Examine the data\nhelp(trees)\ndata(trees)\nstr(trees)\n\n\n6.1\nShow code to calculate the mean Girth of Black Cherry trees with Height less than 75 ft.\n\n\n\n6.2\nUse help(cut) and then use the cut() function to create a new factor variable based on the Height numeric variable in the trees dataset. Try setting the breaks argument to 2 or 3. Rename the levels of your new factor to something meaningful. Show the code.\n\n\n\n6.3\nUsing the new factor from question 2, use aggregate() to calculate the mean and standard deviation of all three variables in the trees data. Show your code and report the results to 2 decimal points of accuracy.\n\n\n\n6.4\nShow the code using which() and boolean phrases as appropriate to find the rows in the trees dataset where Girth is higher than 11 and Height is lower than 75.\n\n\n\n6.5\nRun the following code:\ndata_1 <- data.frame(volume = c(4,5,6,5,6,7,6,5,6,8,7,3,8,7,NA,10),\n           population = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n           \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\"))\nUse aggregate() to calculate the mean of volume for each population (hint: you may need to use help for the functions involved and pay close attention to your data frame…).\n\n\n\n6.6\nWrite a plausible practice question involving any aspect of using which(), boolean phrases and/or aggregate() involving the in-built R dataset iris."
  },
  {
    "objectID": "6-data-manipulation.html#harper-adams-data-science",
    "href": "6-data-manipulation.html#harper-adams-data-science",
    "title": "6 Data subsetting and manipulation",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis website supports students and staff at Harper Adams University and the MSc in Data Science for Global Agriculture, Food, and Environment led by Ed Harris."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "“Data science”"
  },
  {
    "objectID": "index.html#r-stats-bootcamp",
    "href": "index.html#r-stats-bootcamp",
    "title": "About",
    "section": "R Stats Bootcamp",
    "text": "R Stats Bootcamp\nThis is a self-guided tutorial designed for people new to data science, statistics and R, and for those who would like a review. The materials can help you get going with critical skills in R programming, traditional data analysis and open science tools. The aim is to provide open, foundational training you can build on in the most efficient format possible. This will help prepare you for further training and to get you working with data in R as quickly as possible.\nThe Bootcamp is self-paced and has built-in assessment materials. The material is meant to work through in order as an experience, but is not in any way intended to be a reference to the presented topics."
  },
  {
    "objectID": "index.html#bootcamp-modules",
    "href": "index.html#bootcamp-modules",
    "title": "About",
    "section": "Bootcamp modules",
    "text": "Bootcamp modules\nModule 1 Introduction to R scripting and the RStudio interface for beginners\nModule 2 Introduction to using R for simple data analysis\nModule 3 Introduction to reproducible code, R Markdown, and Github.\nThese pages will help orient you if you are a beginner, and we will also cover our design approach for the materials and the tools you will need to get started right away. If you are already an RStudio user and comfortable managing and storing script files and scripts in R, you may want to jump straight to the exercises on each page.\n\nWe make a few suggestions about this material:\n\nFollow along consecutively with the workshop pages\nActually type and run all of the example code yourself\nComplete each page before moving on to the next\nFormally attempt all problems at the end of each page\nJoin us in Slack if you need assistance\n\n\nThat’s it!"
  },
  {
    "objectID": "index.html#harper-adams-data-science",
    "href": "index.html#harper-adams-data-science",
    "title": "About",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis website supports students and staff at Harper Adams University and the MSc in Data Science for Global Agriculture, Food, and Environment led by Ed Harris."
  },
  {
    "objectID": "8-sampling-dist.html#gaussian-that-aint-necessarily-normal",
    "href": "8-sampling-dist.html#gaussian-that-aint-necessarily-normal",
    "title": "8 Sampling and distributions",
    "section": "3 Gaussian: That ain’t necessarily normal!",
    "text": "3 Gaussian: That ain’t necessarily normal!\n\nThe Gaussian distribution is sometimes referred to as the Normal distribution. This is not a good practice: do not refer to the Gaussian distribution as the Normal distribution. Referring to the Gaussian distribution as the normal distribution implies that Gaussian is “typical”, which is patently untrue.\n\n\n- The Comic Book Guy, The Simpsons\n\n\nThe Gaussian distribution is the classic “Bell curve” shaped distribution. It is probably the most important distribution to master, because of its importance in several ways:\n\nWe expect continuous numeric variables that “measure” things to be Gaussian (e.g., human height, lamb weight, songbird wing length, Nitrogen content in legumes, etc., etc.)\nExample data: length of chicken beaks in mm\n\nR output\n# chicken beak lengths\n\n [1] 19.1 19.5 16.5 20.9 18.7 20.9 21.4 22.1 18.8 21.0 16.6 18.4 18.3 15.2 20.1 20.4\n[17] 19.3 21.5 18.5 17.3\n\n\n3.1 The Gaussian assumption (important topic)\n\nFor linear models like regression and ANOVA (we will review these models), we assume the residuals (the difference between each observation and the mean) to be Gaussian distributed and we often must test and evaluate this assumption\nThe Gaussian is described by 2 quantities: the mean and the variance\n\n\n# Example data\n(myvar <- c(1,4,8,3,5,3,8,4,5,6))\n\n [1] 1 4 8 3 5 3 8 4 5 6\n\n# Mean the \"hard\" way\n(myvar.mean <- sum(myvar)/length(myvar))\n\n[1] 4.7\n\n# Mean the easy way\nmean(myvar)\n\n[1] 4.7\n\n# Variance the \"hard\" way \n# (NB this is the sample variance with [n-1])\n(sum((myvar-myvar.mean)^2 / (length(myvar)-1)))\n\n[1] 4.9\n\n# Variance the easy way \nvar(myvar)\n\n[1] 4.9\n\n# Std dev the easy way\nsqrt(var(myvar))\n\n[1] 2.213594\n\n\n\n\n\n3.2 Gaussian histograms\nWe can describe the expected perfect (i.e., theoretical) Gaussian distribution based just on the mean and variance. The value of this mean and variance control the shape of the distribution.\n\n\n\n\n\n3.3 More Gaussian fun\n\n## Gaussian variations ####\n# Try this:\n\n# 4 means\n(meanvec <- c(10, 7, 10, 10))\n\n[1] 10  7 10 10\n\n# 4 standard deviations\n(sdvec <- c(2, 2, 1, 3))\n\n[1] 2 2 1 3\n\n# Make a baseline plot\nx <- seq(0,20, by = .1)\n\n# Probabilities for our first mean and sd\ny1 <- dnorm(x = x, \n            mean = meanvec[1],\n            sd = sdvec[1])\n\n# Baseline plot of 1st mean and sd\nplot(x = x, y = y1, ylim = c(0, .4),\n     col = \"goldenrod\",\n     lwd = 2, type = \"l\",\n     main = \"Gaussian fun \n     \\n mean -> curve position; sd -> shape\",\n     ylab = \"Density\",\n     xlab = \"(Arbitrary) Measure\")\n\n# Make distribution lines\nmycol <- c(\"red\", \"blue\", \"green\")\nfor(i in 1:3){\n  y <- dnorm(x = x, \n                mean = meanvec[i+1],\n                sd = sdvec[i+1])\n  lines(x = x, y = y, \n        col = mycol[i],\n        lwd = 2, type = \"l\")\n}\n\n# Add a legend\nlegend(title = \"mean (sd)\",\n       legend = c(\"10 (2)\", \"  7 (2)\", \n                  \"10 (1)\", \"10 (3)\"),\n       lty = c(1,1,1,1), lwd = 2,\n       col = c(\"goldenrod\", \"red\", \"blue\", \"green\"),\n       x = 15, y = .35)\n\n\n\n\n\n\n\n3.4 Quartile-Quartile (Q-Q) plots\nIt is very often that you might want a peek or even more formally test whether data are Gaussian. This might be in a situation when looking at, for example, the residuals for a linear model to test whether they adhere to the assumption of a Gaussian distribution. In that case, a common diagnostic graph to construct is the quantile-quantile, or “q-q”” Gaussian plot.\nThe q-q Gaussian plot your data again the theoretical expectation of the “quantile”, or percentile, were your data perfectly Gaussian (a straight, diagonal line). Remember, samples are not necessarily expected to perfectly conform to Gaussian (due to sampling error), even if the population from which the sample was taken were to be perfectly Gaussian. Thus, this is a way to confront your data with a model, to help be completely informed. The degree to which your data deviates from the line (especially systematic deviation at the ends of the line of expectation), is the degree to which is deviates from Gaussian.\n\n\n## q-q- Gaussian ####\n\n# Try This:\n\nlibrary(car) # Might need to install {car}\n\nWarning: package 'car' was built under R version 4.1.1\n\n\nLoading required package: carData\n\n# Set graph output to 2 x 2 grid\n# (we will set it back to 1 x 1 later)\npar(mfrow = c(2,2))  \n\n# Small Gaussian sample\nset.seed(42)\nsm.samp <- rnorm(n = 10, \n                 mean = 10, sd = 2)\n\nqqPlot(x = sm.samp, \n       dist = \"norm\", # C'mon guys, Gaussian ain't normal!\n       main = \"Small sample Gaussian\")\n\n[1] 9 2\n\n# Large Gaussian sample\nset.seed(42)\nlg.samp <- rnorm(n = 1000, \n                 mean = 10, sd = 2)\n\nqqPlot(x = lg.samp, \n       dist = \"norm\", \n       main = \"Large sample Gaussian\")\n\n[1] 988 980\n\n# Non- Gaussian sample\nset.seed(42)\nuni <- runif(n = 50, \n                 min = 3, max = 17)\n\nqqPlot(x = uni, \n       dist = \"norm\", \n       main = \"Big deviation at top\")\n\n[1] 35 37\n\npar(mfrow = c(1,1)) # set graph grid back"
  },
  {
    "objectID": "8-sampling-dist.html#poisson-distribution",
    "href": "8-sampling-dist.html#poisson-distribution",
    "title": "8 Sampling and distributions",
    "section": "4 Poisson distribution",
    "text": "4 Poisson distribution\n\nLife is good for only two things, discovering mathematics and teaching mathematics.\n\n\n- Simeon-Denis Poisson\n\nThe description of the Poisson distribution was credited to Simeon-Denis Poisson, a (very, very) passionate mathematician. The classic example for use is for count data, where famously it was exemplified by the number of Prussian soldiers who were killed by being kicked by a horse in a particular year.\n\n\n4.1 The Poisson distribution\n\nCount data of discrete events, objects, etc.\nIntegers, for example the number of beetles caught each day in a pitfall trap:\n\n\nrpois(20, 4)\n\n [1] 3 3 3 5 1 5 5 2 3 4 5 9 5 4 6 2 3 6 5 3\n\n\n\n\nPoisson data are typically skewed to the right\nDescribed by a single parameter, \\(\\lambda\\) (lambda), which describes the mean and the variance\n\n\nThe Poisson parameter:\n\n\n\n\n4.2 Example Poisson data\n\n# Try this:\n\n# E.g. (simulated) Number of ewes giving birth to triplets\n# The counts were made in one year 1n 100 similar flocks (<20 ewes each)\n\nset.seed(42)\nmypois <- rpois(n = 30, lambda = 3)\n\nhist(mypois,\n     main = \"Ewes with triplets\",\n     xlab = \"Count of Triplets\")\n\n\n\n\n\n\n\n4.3 Density plot for different Poisson lambda values\n\n# Try this:\n# 3 lambdas\n(lambda <- c(1, 3, 5))\n\n[1] 1 3 5\n\n# Make a baseline plot\nx <- seq(0, 15, by = 1)\n\n# Probabilities for our first lambda\ny1 <- dpois(x = x, \n            lambda = lambda[1])\n\n# Baseline plot Pois\nplot(x = x, y = y1, ylim = c(0, .4),\n     col = \"goldenrod\",\n     lwd = 2, type = \"b\",\n     main = \"Poisson fun\",\n     ylab = \"Density\",\n     xlab = \"(Arbitrary) Count\")\n\n# Make distribution lines\nmycol <- c(\"red\", \"blue\")\nfor(i in 1:2){\n  y <- dpois(x = x, \n             lambda = lambda[i+1])\n  lines(x = x, y = y, \n        col = mycol[i],\n        lwd = 2, type = \"b\")\n}\n\n# Add a legend\nlegend(title = \"lambda\",\n       legend = c(\"1\", \"3\", \"5\"),\n       lty = c(1,1,1,1), lwd = 2,\n       col = c(\"goldenrod\", \"red\", \"blue\"),\n       x = 8, y = .35)"
  },
  {
    "objectID": "8-sampling-dist.html#binomial",
    "href": "8-sampling-dist.html#binomial",
    "title": "8 Sampling and distributions",
    "section": "5 Binomial",
    "text": "5 Binomial\n\nWhen faced with 2 choices, simply toss a coin. It works not because it settles the question for you, but because in that brief moment when the coin is in the air you suddenly know what you are hoping for.\n\nThe Binomial distribution describes data that has exactly 2 outcomes: 0 and 1, Yes and No, True and False, etc. (you get the idea).\nExamples of this kind of data include things like flipping a coin (heads or tails), successful germination of seeds (success or failure), or binary behavioral decisions (remain or disperse)\n\n\n5.1 The Binomial distribution:\n\nData are the count of “successes”” in (binary) outcomes of a series of independent events\nData coding can be variable, but an example would be success for failure while surveying for wildlife: check this nestbox; is there at least one dormouse (Muscardinus avellanarius) in it?.\n\n\n\n5.2 Ex 1 nest boxes\nLet’s say you check 50 nest boxes, there is exactly 1 result per nest box (occupied or not), and the probability of occupancy is 30%.\n\n# Try this:\n\n# dormouse presence:\nset.seed(42)\n(my_occ <- rbinom(n = 50, # Number of \"experiments\", here nestboxes checked\n       size = 1, # Number of checks, one check per nestbox\n       prob = .3)) # Probability of presence\n\n [1] 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0\n[39] 1 0 0 0 0 1 0 1 1 0 1 0\n\nmosaicplot(table(my_occ), col = c(2,'goldenrod'))\n\n\n\n\n\n\n\n5.3 Ex 2 Flipping a coin: 20 people 10 times each\n\n# Try this:\n# Flip a fair coin:\nset.seed(42)\n(coin <- rbinom(n = 20, # Number of \"experiments\", 20 people flipping a coin\n       size = 10, # Number of coin flips landing on \"heads\" out of 10 flips per person\n       prob = .5)) # Probability of \"heads\"\n\n [1] 7 7 4 7 6 5 6 3 6 6 5 6 7 4 5 7 8 3 5 5\n\nmosaicplot(table(coin), col = 1:unique(coin))\n\nWarning in 1:unique(coin): numerical expression has 6 elements: only the first\nused\n\n\n\n\n\n\nDescribed by 2 parameters, The number of trials with a binary outcome in a single “experiment” (\\(n\\)), and the probability of success for each binary outcome (\\(p\\)).\n\nThe Binomial parameters:\n\n\n\n\n5.4 Density plot for different Binomial parameters\n\n# Try this:\n\n# Binomial parameters\n# 3 n of trial values\n(n <- c(10, 10, 20))\n\n[1] 10 10 20\n\n# 3 probability values\n(p <- c(.5, .8, .5))\n\n[1] 0.5 0.8 0.5\n\n# Make a baseline plot\nx <- seq(0, 20, by = 1)\n\n# Probabilities for our first set of parameters\ny1 <- dbinom(x = x, \n            size = n[1],\n            prob = p[1])\n\n# Baseline plot Binom\nplot(x = x, y = y1, ylim = c(0, .4),\n     col = \"goldenrod\",\n     lwd = 2, type = \"b\",\n     main = \"Binomial fun\",\n     ylab = \"Density\",\n     xlab = \"(Arbitrary) # \\\"Successes\\\"\")\n\n# Make distribution lines\nmycol <- c(\"red\", \"blue\")\nfor(i in 1:2){\n  y <- dbinom(x = x, \n             size = n[i+1],\n             prob = p[i+1])\n  lines(x = x, y = y, \n        col = mycol[i],\n        lwd = 2, type = \"b\")\n}\n\n# Add a legend\nlegend(title = \"Parameters\",\n       legend = c(\"n = 10, p = 0.50\", \n                  \"n = 10, p = 0.80\",\n                  \"n = 20, p = 0.50\"),\n       lty = c(1,1,1,1), lwd = 2, bty='n',\n       col = c(\"goldenrod\", \"red\", \"blue\"),\n       x = 11, y = .35)"
  },
  {
    "objectID": "8-sampling-dist.html#diagnosing-the-distribution",
    "href": "8-sampling-dist.html#diagnosing-the-distribution",
    "title": "8 Sampling and distributions",
    "section": "6 Diagnosing the distribution",
    "text": "6 Diagnosing the distribution\n\nA very common task faced when handling data is “diagnosing the distribution”. Just like a human doctor diagnosing an ailment, you examine the evidence, consider the alternatives, judge the context, and take an educated guess.\n\nThere are statistical tests to compare data to a theoretical model, and they can be useful, but diagnosing a statistical distribution is principally a subjective endeavor. A common situation would be to examine the residual distribution for a regression model compared to the expected Gaussian distribution. Good practice is to have a set of steps to adhere to when diagnosing a distribution.\n\nFirst, develop an expectation of the distribution, based on the type of data\nSecond graph the data, almost always with a histogram, and a q-q plot with a theoretical quartile line for comparison\nThird, compare q-q plots with different distributions for comparison if in doubt, and if it makes sense to do so!\nIf the assumption of a particular distribution is important (like Gaussian residuals), try transformation and compare, e.g., log(your-data), cuberoot(your-data), or others, to the Gaussian q-q expectation.\n\n\nIt is beyond the intention of this page to examine all the possibilities of examining and diagnosing data distributions, but instead the intention is to alert readers that this topic is wide and deep. Here are a few good resources that can take you farther:\nVitto Ricci, Fitting distributions with R\nBill Huber, Fitting distributions to data Quick-R, Probability plots"
  },
  {
    "objectID": "8-sampling-dist.html#practice-exercises",
    "href": "8-sampling-dist.html#practice-exercises",
    "title": "8 Sampling and distributions",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\nFor the following exercises, run the code below to create the data oibject dat.\n# The code below loads and prints the data frame \"dat\"\n\n# Data dictionary for \"dat\", a dataset with different measures of 20 sheep\n# weight - weight in Kg\n# ked - count of wingless flies\n# trough - 2 feed troughs, proportion of times \"Trough A\" fed from\n# shear - minutes taken to \"hand shear\" each sheep\n\n(dat <- data.frame(\n  weight = c(44.1, 38.3, 41.1, 41.9, 41.2, 39.7, 44.5, 39.7, 46.1, 39.8, \n    43.9, 46.9, 35.8, 39.2, 39.6, 41.9, 39.1, 32.0, 32.7, 44.0),\n  \n  ked = c(9, 4, 15, 11, 10, 8, 12, 12, 6, 11,\n             12, 13, 8, 11, 19, 19, 12, 7, 8, 14),\n  \n  trough = c(0.52, 0.74, 0.62, 0.63, 0.22, 0.22, 0.39, 0.94, 0.96, 0.74,\n              0.73, 0.54, 0.00, 0.61, 0.84, 0.75, 0.45, 0.54, 0.54, 0.00),\n  \n  shear = c(14.0, 8.0, 14.0, 11.0, 14.0, 5.0, 9.5, 11.0, 6.5, 11.0, \n            18.5, 11.0, 18.5, 8.0, 8.0, 6.5, 18.5, 15.5, 14.0, 8.0)\n  ))\n\n\n7.1\nDo you expect weight to be Gaussian distributed? How about ked? Explain your answer for each. \n\n\n7.2\nShow the code to graphically diagnose and decide whether weight is Gaussian and explain your conclusion.\n\n\n\n7.3\nShow the code to graphically diagnose and decide whether ked is Gaussian and explain your conclusion. If you choose another likely distribution, test it as well and similarly diagnose.\n\n\n\n7.4\nExplore whether trough is Gaussian, and explain whether you expect it to be so. If not, does transforming the data “persuade it” to conform to Gaussian? Discuss.\n\n\n\n7.5\nWrite a plausible practice question involving any aspect of graphical diagnosis of a data distribution using the iris data."
  }
]