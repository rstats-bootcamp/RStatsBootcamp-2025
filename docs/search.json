[
  {
    "objectID": "7-question-explore.html",
    "href": "7-question-explore.html",
    "title": "7 Question, explore, analyze",
    "section": "",
    "text": "We call sizing up the data “weighing the pig”"
  },
  {
    "objectID": "7-question-explore.html#question-explore-analyze-a-workflow-for-data-science",
    "href": "7-question-explore.html#question-explore-analyze-a-workflow-for-data-science",
    "title": "7 Question, explore, analyze",
    "section": "1 Question, explore, analyze (a workflow for data science)",
    "text": "1 Question, explore, analyze (a workflow for data science)\n\n\nA dataset often comes to the Data Scientist in an imperfect state, possibly incomplete, containing errors, and with minimal description. Likewise, it may contain wonderful knowledge, there to discover. Either way, your first task is to weigh the pig.\n\n\nThe very first task for any data analysis is to gain an understanding of the data itself. This typically involves examining the variables. Are they as we expect? Do we need to adjust the variable types?\nEDA Exploratory Data Analysis\nThis almost always involves graphing the data, and possibly examining numerical summaries and statistical assumptions. Further, it is necessary to look for errors in the data both trivial (e.g. misspelling factor level names like “control” with an extra space “control”), and more serious errors such as numerical typographical errors (e.g. misplacing a decimal point is a classic: height of 5 men in feet: c(5.5, 5.8, 6.1, 5.9, 52.).\nIn total, this part of data analysis is sometimes referred to as Exploratory Data Analysis.\nEDA is part practical and part philosophical in that is requires skill and experience, but is also subjective. Think of it as a step that might take a long while, where the data scientists decides what the analysis is that will be applied to the data, that the analysis is correct and appropriate. Ironically, while EDA is considered very important and can take a large proportion of the total time spent analyzing data, it is usually only reported on very briefly if at all.\nThe order of operation for most analyses should be\n\n1 question\n2 explore\n3 analyse\n\n\n\nYou choose your data analysis prior to collecting the first data point.\n\nFocus on the question and make sure it is clear in formulation, and choose an analysis approach that can resolve the question , given the data… But the data collection should be DESIGNED to fit the question and chosen analysis prior to collection. Explore the data to examine any assumptions required for the analysis, including the use of graphing and any diagnostic or summary statistics. Finally, perform and summarize the analysis. We will practice this workflow for different basic questions, with an emphasis on simple quantitative data.\n\n\n1.1 Objectives\n\nQuestion formulation and hypothesis testing\nSummarize: Weighing the Pig\nVariables and graphing\n“Analysis” versus “EDA”\nStatistical Analysis Plan: the concept\nPractice exercises"
  },
  {
    "objectID": "7-question-explore.html#question-formulation-and-hypothesis-testing",
    "href": "7-question-explore.html#question-formulation-and-hypothesis-testing",
    "title": "7 Question, explore, analyze",
    "section": "2 Question formulation and hypothesis testing",
    "text": "2 Question formulation and hypothesis testing\n\nIt is the primary responsibility of the scientist to agree on the specific details of generating evidence from data to answer questions (i.e., statistical analysis). When these roles are occupied by the same person, this matter should be settled before collecting any data.\n\nThe general topic of formulating statistical questions is vast; many books have been written on the subject. The tradition and practice of statistical analysis has evolved through time. Here we will focus on the traditional starting point for a “first statistics course”, within the context of Null Hypothesis Significance testing (NHST).\n\n2.1 Sampling concept and NHST\nThe gambit of NHST is that there is a population of interest but that the population cannot be directly measured because it is too big or otherwise inconvenient or impossible to measure. Thus, experimental samples are drawn randomly from the population, possibly subjected to experimental conditions, and the magnitude of observed differences or measured associations are summarized by various test statistics and compared to how likely such an observed difference or association would be to observe in the absence of the hypothesized effect.\nThe null hypothesis is the one consistent with no effect or difference. We evaluate whether to reject the null hypothesis using the P-value, the (conditional) probability that the observed effect is unlikely to arise duie to sampling or experimental error.\nTraditionally, the P-value is compared to the alpha value, almost always set to 0.05. This alpha value can be interpreted as the maximum probability that is acceptable of making a mistake and concluding there IS a difference, when in fact a difference does not exist. When the P-value is less than 0.05, we conclude there is a difference, rejecting the null hypothesis and “accepting” the hypothesis we predicted was true, usually referred to as the alternative hypothesis.\n\n\n\n2.2 NHST notes\nBenefits of NHST\n\nFamiliar and acceptable to majority of researchers\nTypically robust to assumptions when applied correctly\nStrong framework for evidence, especially for experiments\nThe basic idea is objective and simple\n\n\nCriticism of HNST\n\nOften conceived, applied and interpreted under error\nValidation of analysis (e.g. assumptions testing) is often neglected\nEducation for applied researchers often deficient\nThough simple, practitioners may be ignorant of subtle concepts\n\n\n\n\n2.3 Further reading\nIf the idea is new to you that NHST in statistics is not perfect and you want to get serious about understanding why, like most subjects, you will need to pursue further sources.\nAnderson, D.R., Burnham, K.P. and Thompson, W.L., 2000. Null hypothesis testing: problems, prevalence, and an alternative. The journal of wildlife management, pp.912-923.\nNickerson, R.S., 2000. Null hypothesis significance testing: a review of an old and continuing controversy. Psychological methods, 5(2), p.241.\nNix, T.W. and Barnette, J.J., 1998. The data analysis dilemma: Ban or abandon. A review of null hypothesis significance testing. Research in the Schools, 5(2), pp.3-14.\nStephens, P.A., Buskirk, S.W., Hayward, G.D. and Martinez Del Rio, C., 2005. Information theory and hypothesis testing: a call for pluralism. Journal of applied ecology, 42(1), pp.4-12."
  },
  {
    "objectID": "7-question-explore.html#summarize-weighing-the-pig",
    "href": "7-question-explore.html#summarize-weighing-the-pig",
    "title": "7 Question, explore, analyze",
    "section": "3 Summarize: Weighing the Pig",
    "text": "3 Summarize: Weighing the Pig\n\n\nThe best way gain skill in handling data is to practice.\n\nWeighing the pig is the term we use to describe creating a summary-at-a-glance of a dataset. Usually this includes graphics and statistical summary, as well a description of how much data we have. A key consideration is, also, the specification of the variables.\nWe will practice data handling with the data file chickwts.xlsx.\nDownload the file, read it into a data object in R called chicks, and convert the feed variable to a factor if necessary.\n# Try this:\n\n# Download the 7-chickwts.xlsx file, read it into a data \n# object in R called \"chicks\", \n# and convert the \"feed\" variable to a factor if necessary.\n\n# Do not neglect looking inside the \"raw\" data file\n# Is it as you expect?  Is the data dictionary present and clear?\n\n# Load necessary libraries\nlibrary(openxlsx)\n\n# Read file\nsetwd(\"D:/Dropbox/git/DSgarage/public/data\") # NB change to YOUR file path...\nchicks <- read.xlsx(\"7-chickwts.xlsx\")\n\n# Convert feed to factor if needed\nclass(chicks$feed) # Character\nchicks$feed <- factor(chicks$feed)\nclass(chicks$feed) # Factor\n\n\n3.1 Chick data\n\nThe hypothesis voices “how you think the world works” or what you predict to be true”\n\nThe hypothesis we believe is true for the chicks dataset might be phrased in different ways.\n\nChick weight differs after 6 weeks according to feed additive type\nMean chick weight varies according to feed additive type\nThe variance between chick weight for different feed additives is bigger than the variance within chick weight as a whole\n\n\n\n\n3.2 Hypothesis\nThe minimum amount of information we are usually interested in when sizing up a dataset is How much data is there?, What is the central tendency (e.g. the mean, variance, etc.)?, and possibly Are there rare values?.\nWe would typically start graphing the data right away. If we have a notion of what our questions or hypotheses are, they should inform the initial peek at the data. For example, in the chickwts data, we know our question will be related not to the overall central tendency of chick weight, but to chick weight for each individual feed type.\nWe do not approach this sizing up of the data in a workhorse fashion, merely to check a tick box. We are looking quickly for details in the data that give us insight into what the data is like. For example, we peek at whether the mean and median are close to each other (indicator our data may be Gaussian), we compare the standard deviation, variance or standard error of a numeric variable relative to different levels of a factor, to see if they are similar.\n# Try this:\n\n# Summarize the whole dataset\n# summary() provides summary statistics for numeric variables and counts\nsummary(chicks)\n\n# we might want to look at summary for different levels of feed\n?summary\nsummary(object = chicks$weight[which(chicks$feed == \"casein\")])\nsummary(object = chicks$weight[which(chicks$feed == \"horsebean\")])\n# etc. - this method is easy but inelegant?\n\n# aggregate()\n?aggregate\n\n# mean\naggregate(x = chicks$weight, by = list(chicks$feed), FUN = mean)\n\n# standard deviation\naggregate(x = chicks$weight, by = list(chicks$feed), FUN = sd)\n\n# You can make your own function for the FUN argument\n# stadard error of mean, SEM = standard deviation / square root of sample size\naggregate(x = chicks$weight, by = list(chicks$feed), \n          FUN = function(x){ sd(x)/sqrt(length(x)) })\n\n# You can apply several functions and name them!\naggregate(x = chicks$weight, by = list(feed = chicks$feed), \n          FUN = function(x){ c(mean = mean(x), \n                               sd = sd(x),  \n                               SEM = sd(x)/sqrt(length(x)))})"
  },
  {
    "objectID": "7-question-explore.html#variables-and-graphing",
    "href": "7-question-explore.html#variables-and-graphing",
    "title": "7 Question, explore, analyze",
    "section": "4 Variables and graphing",
    "text": "4 Variables and graphing\n\nA good graph usually tells the whole story, but a bad graph is worse than no graph at all.\n\n\n\n\n\n\nXKCD Convinced by data\n\n\n\n\nThere are a few topics in graphing data that are important to consider here, but the topic is wide and deep, analytical, creative, and even artistic. We make a distinction between graphs used to explore data during EDA (meant to be “consumed” only by the data scientist who made them and are of no use to document a pattern to others) and graphs intended to constitute evidence.\n\n\n4.1 Scientific graphs\nA few graphing principles:\n\nMust convey the relevant information\nShould be consistent in aesthetics\nMust be self-contained (meaning is contained 100% within the figure and legend)\nShould reflect a hypothesis or statistical concept (if not purely descriptive)\nShould be appropriate to the data\n\n\nYou can think of R graphics as a way to “build up information in layers” onto a graph. There are many aesthetic features of graph that can be controlled, like adding colors, text, lines, legends, etc. The R graphics system is very simple to use, but can also be very powerful (mastering this takes practice). We make a distinction here between R base graphics and packages that can be used to make specialized and varied graphs (like the powerful and popular package {ggplot})\n\n\n4.2 Layering information\nWe can look at graphing the chicks data in a few different ways. We will try a few different graphs in this way, building up features. We might build up features on a graph using arguments in a particular graph function.\n\nLike, adding\n\na main title with the argument main\nthe x axis title with the argument xlab\nadding lines with the functions abline() or lines()\n\n\n\n\n4.3 Types of graphs\nTypically you would choose the type of graph that both fits the type of data you have and that conveys the information you wish to examine or showcase. E.g., for a single numeric variable, you might wish to show:\n\nThe distribution of data with a histogram: hist()\nThe central tendency relative to a factor with a boxplot: boxplot()\n\n\nHistogram of the chicks data\n# The least you can do\nhelp(hist)\nhist(x = chicks$weight)\n\n\nAdd a title with main\n# Argument main\nhist(x = chicks$weight,\n     main = \"Distribution of chick weights (all feeds)\")\n\n\nAdd an x axis title with xlab\n# x axis title\nhist(x = chicks$weight,\n     main = \"Distribution of chick weights (all feeds)\",\n     xlab = \"Chick weight (grams)\")\n\n\nAdd a vertical line for the weight mean with abline()\n# Add vertical line for mean weight\nhist(x = chicks$weight,\n     main = \"Distribution of chick weights (all feeds)\",\n     xlab = \"Chick weight (grams)\")\n\nhelp(abline)\nabline(v = mean(chicks$weight), col = \"red\", lty = 2, lwd = 3)\n\n\n# Try a boxplot\n\nhelp(boxplot)\nboxplot(x = chicks$weight)\n\n# I have seen worse graphs, but I can't remember when.\n# Flash challenge: Improve the graph\n\n\n# weight as a function of feed\nboxplot(formula = weight ~ feed,\n        data = chicks)\n# This is probably a good representation of our hypothesis\n# Flash challenge: Improve the graph..."
  },
  {
    "objectID": "7-question-explore.html#analysis-versus-eda",
    "href": "7-question-explore.html#analysis-versus-eda",
    "title": "7 Question, explore, analyze",
    "section": "5 “Analysis” versus “EDA”",
    "text": "5 “Analysis” versus “EDA”\nAlthough you could consider Exploratory Data Analysis, EDA, an important part of the complete process of data analysis, we might make a distinction between “Analysis” the part of analysis that generates Evidence, and that of EDA which is used to explore data and test assumptions.\n\n\n5.1 Analysis\nA data analysis is\n\nDesigned to fit a specific question or hypothesis\nPart of a workflow: Informal hypothesis statement (in plain language) > Statistical hypothesis (specifies a or implies a statistical test) > Evidence (the specific results)\nDesigned and usually formatted to present to others, such as in a report or a scientific manuscript\nContains only bare essentials as relates to the initial hypothesis (e.g. a good graph, the summary of a statistical analysis)\nShould strictly be reproducible via a script and archived data\nDone in conjunction with EDA\n\n\n\n\n5.2 EDA\nExploratory data analysis is\n\nInformal and may be haphazard\nDesigned to explore or gain understanding of data\nAssumptions testing\nUsually not designed to document or show to others\nOccurs primarily before (every) analysis\nMay or may not be documented to be reproducible\nDone before the final, evidence-generating Analysis\n\n\nWe can keep this concept of EDA versus Analysis in our mind while we discuss the Statistical Analysis Plan."
  },
  {
    "objectID": "7-question-explore.html#statistical-analysis-plan-the-concept",
    "href": "7-question-explore.html#statistical-analysis-plan-the-concept",
    "title": "7 Question, explore, analyze",
    "section": "6 Statistical Analysis Plan: the concept",
    "text": "6 Statistical Analysis Plan: the concept\n\nI have a cunning (statistical analysis) plan -Baldrick\n\nA Statistical Analysis Plan (SAP) is a formal document that should be used to design data analysis. One of the most important functions of the SAP is to make a formal connection between the hypothesis, the data collected and and the method of analysis that will be used to generate evidence to support or refute the hypothesis. This is conducted before any data are collected.\nThe components of a basic SAP are:\n\nThe hypotheses stated in plain language\nEach hypothesis translated into a specific statistical model\nSpecification of data and and data collection methods\n\n-) Specification of effect size\n\nJustification of sample size through power analysis or other means\n\n\nDefinition of all of these components is beyond the boundaries of this Bootcamp, however the explicit connection of hypotheses with a statistical model is one of the very basic elements of best practice in science.\n\n\n6.1 The scientific method, Classic version\nWe usually learn the scientific method as a cycle where we conceive a problem, form a hypothesis, conduct an experiment, evaluate the result and so on. We learn and teach this as a literal cycle.\n\n\n\n\n\nThe classic view of the scientific process\n\n\n\n\nThis classic view of the scientific process implies that we plan the analysis only after we conduct the experiment and collect data. While many data scientists or statisticians would agree that this model is widely used in science, it is considered very poor practice for several reasons.\n\nThe expected difference or relationship (i.e., the effect size) should explicitly be part of the hypothesis and quantified BEFORE collecting data\nThe statistical test must be chosen prior to collect the data to insure the evidence matches the expectation\nThe sample size should be justified, using power analysis or a less formal means. Collecting too little data will likely result in failing to detect a difference (even if your hypothesis is correct!); Collecting too much data is simply a waste of resources.\n\n\n\n\n\n\nScientific Process - what we teach children in school is not quite right\n\n\n\n\n\n\n6.2 Best practice scientific method\nThe traditional view of the scientific method should probably be adjusted to explicitly accommodate planning the analysis at the same time as the hypothesis formulation stage. Likewise, the analysis plan should specifically influence the design of the data collection for the experiment.\n\n\n\n\n\nModern scientific process\n\n\n\n\nA modern view of best practice of scientific endeavor includes an experimental design phase, with consideration of effect size and power analysis, and the production of a statistical analysis plan that contains a formal statistical hypothesis. All off this happens prior to any data collection."
  },
  {
    "objectID": "7-question-explore.html#practice-exercises",
    "href": "7-question-explore.html#practice-exercises",
    "title": "7 Question, explore, analyze",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\nFor the following questions, use the field-trial.xlsx dataset.\nThis is real data in Tidy Data format, but our information for these exercises is limited precisely to the contents of the file, including the data dictionary. In this experiment, seeds were raised under field trial conditions for two weeks to look at the effect of different treatment conditions on mass of gain during germination. There are several measured variables, with the calculated pct variable probably intended to be the dependent variable, with the factor treatment being the main explanatory variable for variation in pct.\n\n\n7.1\nShow code to set up an R analysis file with a header, table of contents, and a setup section that sets your working directory, loads any required libraries and reads in the data. Call the data.frame object you create seed.\n\n\n\n7.2\n\npct, wet and dry should be numeric\nblock and trial should be factors\ntreatment should be a factor with the level “Control” set as the reference.\n\nShow the code to do this.\n\n\n\n7.3\nUse aggregate() to calculate the mean, standard deviation, standard error, and the count (e.g. length()) of pct for each level of treatment. Show the code.\n\n\n\n7.4\nMake a fully labelled boxplot of the pct variable as a function of treatment. Add a horizontal line (red and dashed) for the overall mean of pct, and two horizontal lines (gray, dotted) for the overall mean of pct +/- 1 standard deviation.\n\n\n\n7.5 (hard: may require tinkering and problem solving)\nExperiment making a boxplot showing pct ~ treatment separated for each trial\n\n\n\n7.6\nWrite a plausible practice question involving aggregate() and boxplot() in-built R dataset iris."
  },
  {
    "objectID": "7-question-explore.html#harper-adams-data-science",
    "href": "7-question-explore.html#harper-adams-data-science",
    "title": "7 Question, explore, analyze",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module supports students and staff at Harper Adams University and the MSc in Data Science for Global Agriculture, Food, and Environment led by Ed Harris."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "I call this “Robot farmer with calendar”"
  },
  {
    "objectID": "schedule.html#bootcamp-resources",
    "href": "schedule.html#bootcamp-resources",
    "title": "Schedule",
    "section": "Bootcamp resources",
    "text": "Bootcamp resources\nThis material is intended to require a significant time commitment to complete (most students report about 20 hours at minimum). The schedule below is intended to be a concise overview of the different resources for the Bootcamp. Some pages have associated slides and videos linked here.\nFor all of the lab pages it is intended that you type and run 100% of the code in examples and then explicitly answer the exercise questions, which are based on examples for given code on the pages. Thus, to answer the questions it is recommended to adapt code you have already run when completing the pages.\n\n\n\n\n\n\n\nTopics\n\n\n\n\n  Bootcamp overview\n\n\nModule 1: R scripting\n1 R and RStudio Setup\n2 R language\n3 Functions and packages\n4 Data objects\n5 Data frames\n6 Data sub-setting and manipulation\n\n\nModule 2: Statistics review\n7 Question, explore, analyze\n8 Sampling distributions\n9 Correlation\n10 Simple linear regression\n11 T-test\n12 1-way ANOVA\n\n\nModule 3: reproducibility, collaboration\n13 Reproducibility\n14 Automate reports\n15 Git and version control"
  },
  {
    "objectID": "schedule.html#harper-adams-data-science",
    "href": "schedule.html#harper-adams-data-science",
    "title": "Schedule",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module supports students and staff at Harper Adams University and the MSc in Data Science for Global Agriculture, Food, and Environment led by Ed Harris."
  },
  {
    "objectID": "slides-setup.html",
    "href": "slides-setup.html",
    "title": "R Bootcamp RStudio & R Scripts",
    "section": "",
    "text": "https://dsgarage.netlify.app/bootcamp/0.1-bootcamp-intro/"
  },
  {
    "objectID": "slides-setup.html#what-will-you-learn",
    "href": "slides-setup.html#what-will-you-learn",
    "title": "R Bootcamp RStudio & R Scripts",
    "section": "What will you learn?",
    "text": "What will you learn?\n \n\nInstall R and RStudio\nPractice of good, reproducible scripting\nDesign vision of learning materials: Read the pages and type and run all code"
  },
  {
    "objectID": "slides-setup.html#tour-of-rstudio-interface",
    "href": "slides-setup.html#tour-of-rstudio-interface",
    "title": "R Bootcamp RStudio & R Scripts",
    "section": "Tour of RStudio interface",
    "text": "Tour of RStudio interface"
  },
  {
    "objectID": "slides-setup.html#reproducible-script",
    "href": "slides-setup.html#reproducible-script",
    "title": "R Bootcamp RStudio & R Scripts",
    "section": "Reproducible script",
    "text": "Reproducible script"
  }
]