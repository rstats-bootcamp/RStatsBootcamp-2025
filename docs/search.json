[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "Data science learning pathway\n\n\n\n\nThis self-paced bootcamp requires approximately 20-30 hours to complete. The schedule below provides an overview of all course modules and resources. Work through the materials sequentially for the best learning experience.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule"
    ]
  },
  {
    "objectID": "schedule.html#r-stats-bootcamp-learning-path",
    "href": "schedule.html#r-stats-bootcamp-learning-path",
    "title": "Course Schedule",
    "section": "",
    "text": "This self-paced bootcamp requires approximately 20-30 hours to complete. The schedule below provides an overview of all course modules and resources. Work through the materials sequentially for the best learning experience.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule"
    ]
  },
  {
    "objectID": "schedule.html#module-1-r-foundations",
    "href": "schedule.html#module-1-r-foundations",
    "title": "Course Schedule",
    "section": "Module 1: R Foundations",
    "text": "Module 1: R Foundations\nLearn the fundamentals of R programming and the RStudio environment.\n\n\n1. R and RStudio Setup\nGetting started with the R programming environment\n\nInstalling R and RStudio\nNavigating the RStudio interface\nCreating your first R script\nUnderstanding R packages\n\nBegin Lesson → | Slides\n\n\n2. R Language Basics\nCore concepts of the R programming language\n\nR syntax and data types\nVariables and assignment\nBasic operations and calculations\nControl structures (if/else, loops)\n\nBegin Lesson → | Slides\n\n\n3. Functions and Packages\nWorking with functions and extending R’s capabilities\n\nUsing built-in functions\nCreating your own functions\nInstalling and loading packages\nFunction documentation and help\n\nBegin Lesson →\n\n\n4. Data Objects\nUnderstanding R’s data structures\n\nVectors, matrices, and arrays\nLists and factors\nWorking with dates and times\nType conversion and coercion\n\nBegin Lesson →\n\n\n5. Data Frames\nWorking with tabular data in R\n\nCreating and manipulating data frames\nAccessing data frame elements\nAdding and modifying columns\nMerging data frames\n\nBegin Lesson →\n\n\n6. Data Manipulation\nTechniques for cleaning and transforming data\n\nFiltering and subsetting data\nSorting and arranging data\nSummarizing data\nReshaping data (wide vs. long format)\n\nBegin Lesson →",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule"
    ]
  },
  {
    "objectID": "schedule.html#module-2-statistical-analysis",
    "href": "schedule.html#module-2-statistical-analysis",
    "title": "Course Schedule",
    "section": "Module 2: Statistical Analysis",
    "text": "Module 2: Statistical Analysis\nApply R programming to statistical analysis and data visualization.\n\n\n7. Question, Explore, Analyze\nThe data analysis workflow\n\nFormulating research questions\nExploratory data analysis\nData visualization principles\nCommunicating findings\n\nBegin Lesson →\n\n\n8. Sampling Distributions\nUnderstanding probability and sampling\n\nRandom sampling\nProbability distributions\nThe Central Limit Theorem\nConfidence intervals\n\nBegin Lesson →\n\n\n9. Correlation\nMeasuring relationships between variables\n\nCorrelation coefficients\nVisualizing correlations\nTesting correlation significance\nCorrelation vs. causation\n\nBegin Lesson →\n\n\n10. Simple Linear Regression\nModeling relationships between variables\n\nLinear regression concepts\nFitting regression models in R\nInterpreting regression output\nAssessing model fit\n\nBegin Lesson →\n\n\n11. T-tests\nComparing group means\n\nOne-sample t-tests\nIndependent samples t-tests\nPaired samples t-tests\nEffect sizes and power\n\nBegin Lesson →\n\n\n12. One-way ANOVA\nAnalyzing variance between groups\n\nANOVA concepts and assumptions\nConducting ANOVA in R\nPost-hoc tests\nReporting ANOVA results\n\nBegin Lesson →",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule"
    ]
  },
  {
    "objectID": "schedule.html#module-3-reproducible-research",
    "href": "schedule.html#module-3-reproducible-research",
    "title": "Course Schedule",
    "section": "Module 3: Reproducible Research",
    "text": "Module 3: Reproducible Research\nLearn essential tools and practices for reproducible data science.\n\n\n13. Reproducibility Principles\nIntroduction to reproducible research\n\nWhy reproducibility matters\nComponents of reproducible workflows\nDocumentation best practices\nFile organization strategies\n\nBegin Lesson →\n\n\n14. R Markdown\nCreating dynamic documents\n\nR Markdown basics\nCombining code, results, and narrative\nDocument formatting options\nGenerating reports in multiple formats\n\nBegin Lesson →\n\n\n15. Git and GitHub Basics\nVersion control for data science\n\nUnderstanding version control\nSetting up Git and GitHub\nBasic Git workflow\nTracking changes to your code\n\nBegin Lesson →\n\n\n16. Collaborative Workflows\nWorking effectively with others\n\nProject organization\nSharing code and data\nCollaboration best practices\nMaintaining reproducibility in teams\n\nBegin Lesson →",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Stats Bootcamp",
    "section": "",
    "text": "Master R programming, statistical analysis, and reproducible research in one comprehensive bootcamp\n\n\n&lt;a href=\"schedule.qmd\" class=\"btn btn-primary\"&gt;Start Learning&lt;/a&gt;\n&lt;a href=\"about.qmd\" class=\"btn btn-outline\"&gt;About the Course&lt;/a&gt;",
    "crumbs": [
      "Home",
      "Getting Started",
      "R Stats Bootcamp"
    ]
  },
  {
    "objectID": "index.html#why-learn-with-r-stats-bootcamp",
    "href": "index.html#why-learn-with-r-stats-bootcamp",
    "title": "R Stats Bootcamp",
    "section": "Why Learn with R Stats Bootcamp?",
    "text": "Why Learn with R Stats Bootcamp?\n\n\nLearn by Doing\nMaster R programming through hands-on exercises and real-world data analysis projects. Build skills by writing actual code from day one.\n\n\nDiscover Insights\nTransform raw data into meaningful insights. Learn how to ask the right questions and find answers through statistical analysis.\n\n\nShare Your Findings\nCreate reproducible research that others can understand, verify, and build upon using modern tools like R Markdown and GitHub.",
    "crumbs": [
      "Home",
      "Getting Started",
      "R Stats Bootcamp"
    ]
  },
  {
    "objectID": "index.html#your-learning-pathway",
    "href": "index.html#your-learning-pathway",
    "title": "R Stats Bootcamp",
    "section": "Your Learning Pathway",
    "text": "Your Learning Pathway\n\n\n ### Module 1: R Foundations Get started with R programming and RStudio. Learn the fundamentals of data structures, functions, and data manipulation. Begin Module 1 →\n\n\n ### Module 2: Statistical Analysis Explore your data, understand sampling distributions, and master essential statistical tests for research. Begin Module 2 →\n\n\n ### Module 3: Reproducible Research Learn tools and practices for reproducible science including R Markdown, version control with Git, and collaborative workflows. Begin Module 3 →",
    "crumbs": [
      "Home",
      "Getting Started",
      "R Stats Bootcamp"
    ]
  },
  {
    "objectID": "index.html#designed-for-research-students",
    "href": "index.html#designed-for-research-students",
    "title": "R Stats Bootcamp",
    "section": "Designed for Research Students",
    "text": "Designed for Research Students\n\n“This bootcamp transformed how I approach data analysis in my research. The step-by-step approach made learning R accessible and enjoyable, even as a complete beginner.”\n\n\n— Research Student, Harper Adams University",
    "crumbs": [
      "Home",
      "Getting Started",
      "R Stats Bootcamp"
    ]
  },
  {
    "objectID": "index.html#ready-to-start-your-data-science-journey",
    "href": "index.html#ready-to-start-your-data-science-journey",
    "title": "R Stats Bootcamp",
    "section": "Ready to Start Your Data Science Journey?",
    "text": "Ready to Start Your Data Science Journey?\nFollow along with the course materials at your own pace. Complete each module sequentially for the best learning experience.\nView Full Course Schedule",
    "crumbs": [
      "Home",
      "Getting Started",
      "R Stats Bootcamp"
    ]
  },
  {
    "objectID": "9-correlation.html",
    "href": "9-correlation.html",
    "title": "9 Correlation",
    "section": "",
    "text": "The data were formless like a cloud of tiny birds in the sky",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "9 Correlation"
    ]
  },
  {
    "objectID": "9-correlation.html#statistical-relationships",
    "href": "9-correlation.html#statistical-relationships",
    "title": "9 Correlation",
    "section": "1 Statistical relationships",
    "text": "1 Statistical relationships\n\nMost of you will have heard the maxim “correlation does not imply causation”. Just because two variables have a statistical relationship with each other does not mean that one is responsible for the other. For instance, ice cream sales and forest fires are correlated because both occur more often in the summer heat. But there is no causation; you don’t light a patch of the Montana brush on fire when you buy a pint of Haagan-Dazs. - Nate Silver\n\nCorrelation is used in many applications and is a basic tool in the data science toolbox. We use it to to describe, and sometimes to test, the relationship between two numeric variables, principally to ask whether there is or is not a relationship between the variables (and if there is, whether they tend to vary positively (both tend to increase in value) or negatively (one tends to decrease as the other increases).\n\n1.1 Objectives\n\nThe question of correlation\nData and assumptions\nGraphing\nTest and alternatives\nPractice exercises",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "9 Correlation"
    ]
  },
  {
    "objectID": "9-correlation.html#the-question-of-correlation",
    "href": "9-correlation.html#the-question-of-correlation",
    "title": "9 Correlation",
    "section": "2 The question of correlation",
    "text": "2 The question of correlation\nThe question of correlation is simply whether there is a demonstrable association between two numeric variables. For example, we might wonder this for two numeric variables, such as a measure of vegetation biomass and a measure of insect abundance. Digging a little deeper, we are interested in seeing and quantifying whether and how the variables may “co-vary” (i.e, exhibit significant covariance). This covariance may be quantified as being either positive or negative and may vary in strength from zero, to perfect positive or negative correlation (+1, or -1). We traditionally visualise correlation with a scatterplot (plot() in R). If there is a relationship, the degree of “scatter” is related to strength of the correlation (more scatter = a weaker correlation).\nE.g., we can see in the figure below that the two variables tend to increase with each other positively, suggesting a positive correlation.\n\n# Try this:\n\n# Flash challenge\n# Take the data input below and try to exactly recreate the figure above!\n\nveg &lt;- c(101.7, 101.2, 97.1, 92.4, 91, 99.4, 104.2, 115.9, 91.9, 101.4, \n93.5, 87.2, 89.2, 92.8, 103.1, 116.4, 95.2, 80.9, 94.9, 88.8, \n108.2, 86.1, 104.1, 101.5, 116.9, 109.6, 103.7, 83.9, 85.9, 88.5, \n98.9, 98.8, 107.8, 86.5, 92.6, 76, 95.2, 105.3, 103.1, 89.3, \n100.1, 103.1, 87.7, 92.4, 91.5, 105.4, 105.7, 90.5, 105.6, 101.6, \n97.4, 93.4, 88.7, 81.1, 100.9, 91.6, 102.4, 92.8, 92, 97.1, 91.1, \n97.3, 104, 99, 101.5, 112.8, 82.4, 84.9, 116.3, 92.2, 106.2, \n94.2, 89.6, 108.8, 106.2, 91, 95.5, 99.1, 111.6, 124.1, 100.8, \n117.6, 118.6, 115.8, 102.2, 107.7, 105, 86.7, 99, 101.8, 106.3, \n100.3, 86.6, 106.4, 92.6, 108.2, 100.5, 100.9, 116.4)\n\narth &lt;- c(1002, 1006, 930, 893, 963, 998, 1071, 1052, 997, 1044, 923, \n988, 1022, 975, 1022, 1050, 929, 928, 1019, 957, 1054, 850, 1084, \n995, 1065, 1039, 1009, 945, 995, 967, 916, 998, 988, 956, 975, \n910, 954, 1044, 1063, 948, 966, 1037, 976, 979, 969, 1009, 1076, \n943, 1024, 1071, 969, 963, 1020, 936, 1004, 961, 1089, 953, 1037, \n962, 977, 958, 944, 933, 970, 1036, 960, 912, 978, 967, 1035, \n959, 831, 1016, 901, 1010, 1072, 1019, 996, 1122, 1029, 1047, \n1132, 996, 979, 994, 970, 976, 997, 950, 1002, 1003, 982, 1071, \n959, 976, 1011, 1032, 1024)",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "9 Correlation"
    ]
  },
  {
    "objectID": "9-correlation.html#data-and-assumptions",
    "href": "9-correlation.html#data-and-assumptions",
    "title": "9 Correlation",
    "section": "3 Data and assumptions",
    "text": "3 Data and assumptions\n\n3.1 Pearson correlation\n“Traditional correlation” is sometimes referred to as the Pearson correlation. The data and assumptions are important for the Pearson correlation - we use this when there is a linear relationship between our variables of interest, and the numeric values are Gaussian distributed.\nMore technically, the Pearson correlation coefficient is the covariance of two variables, divided by the product of their standard deviations:\n\n{width = “500px”}\n\nThe correlation coefficient can be calculated in R using the cor() function.\n\n# Try this:\n# use veg and arth from above\n\n# r the \"hard way\"\n\n# r &lt;- ((covariance of x,y) / (std dev x * std dev y) )\n\n\n# sample covariance (hard way)\n(cov_veg_arth &lt;- sum( (veg-mean(veg))*(arth-mean(arth))) / (length(veg) - 1 ))\n\ncov(veg,arth) # easy way\n\n# r the \"hard way\"\n(r_arth_veg &lt;- cov_veg_arth / (sd(veg) * sd(arth)))\n\n# r the easy way\nhelp(cor)\ncor(x = veg, y = arth,\n    method = \"pearson\") # NB \"pearson\" is the default method if unspecified",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "9 Correlation"
    ]
  },
  {
    "objectID": "9-correlation.html#graphing",
    "href": "9-correlation.html#graphing",
    "title": "9 Correlation",
    "section": "4 Graphing",
    "text": "4 Graphing\nWe can look at a range of different correlation magnitudes, to think about diagnosing correlation.\n\n\nIf we really care and value making a correlation between two specific variables, traditionally we would use the scatterplot like above with the veg and arth data, and the calculation of the correlation coefficient (using plot() and cor() respectively).\nOn the other hand, we might have loads of variables and just want to quickly assess the degree of correlation and intercorrelation. To do this we might just make and print a matric of the correlation plot (using pairs()) and the correlation matrix (again using cor())\n\n## Correlation matrices ####\n# Try this:\n\n# Use the iris data to look at correlation matrix \n# of flower measures\ndata(iris)\nnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\ncor(iris[ , 1:4]) # all rows, just the numeric columns\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n# fix the decimal output\nround(cor(iris[ , 1:4]), 2) # nicer\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length         1.00       -0.12         0.87        0.82\nSepal.Width         -0.12        1.00        -0.43       -0.37\nPetal.Length         0.87       -0.43         1.00        0.96\nPetal.Width          0.82       -0.37         0.96        1.00\n\n# pairs plot\npairs(iris[ , 1:4], pch = 16, \n      col = iris$Species) # Set color to species...\n\n\n\n\n\n\n\n\n\n# Correlation table\n\nround(cor(iris[ , 1:4]), 2) # nicer output\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length         1.00       -0.12         0.87        0.82\nSepal.Width         -0.12        1.00        -0.43       -0.37\nPetal.Length         0.87       -0.43         1.00        0.96\nPetal.Width          0.82       -0.37         0.96        1.00\n\n\n\nIn this case, we can see that the correlation of plant parts is very strongly influenced by species! For further exploration we would definitely want to explore and address that, thus here we have shown how powerful the combination of statistical summary and graphing can be.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "9 Correlation"
    ]
  },
  {
    "objectID": "9-correlation.html#test-and-alternatives",
    "href": "9-correlation.html#test-and-alternatives",
    "title": "9 Correlation",
    "section": "5 Test and alternatives",
    "text": "5 Test and alternatives\nWe may want to perform a statistical test to determine whether a correlation coefficient is “significantly” different to zero, using Null Hypothesis Significance Testing. There are a lot of options, but a simple solution is to use the cor.test() function.\nAn alternative to the Pearson correlation (the traditional correlation, assuming the variables in question are Gaussian), is the Spearman rank correlation, which can be used when the assumptions for the Pearson correlation are not met. We will briefly perform both tests.\nSee here for further information on Pearson correlation\nSee here for further information on Spearman rank correlation\n\nBriefly, the principle assumptions of the Pearson correlation are:\n\nThe relationship between the variables is linear (i.e., not a curved relationship)\nThe variables exhibit a bivariate Gaussian distribution (in practice, it is often accepted that each variable is Gaussian)\nHomoscedasticity (i.e., the variance is similar across the range of the variables)\nObservations are independent\nAbsence of outliers\n\n\nDiscussion of assumptions\nFurther practical information about correlation\n\nLet’s try a statistical test of correlation. We need to keep in mind an order of operation for statistical analysis, e.g.:\n1 Question\n2 Graph\n3 Test\n4 Validate (e.g. test assumptions)\n\n## Correlation test ####\n# Try this:\n\n# 1 Question: whether Petal length and width are correlated?\n# (i.e. is there evidence the correlation coefficient different to zero)\n\n\n# 2 Graph\nplot(iris$Petal.Width, iris$Petal.Length,\n     xlab = \"Petal width\",\n     ylab = \"Petal length\",\n     col = iris$Species, pch = 16)\n\n\n\n\n\n\n\n\n\n# 3 Test\ncor.test(iris$Petal.Width, iris$Petal.Length)\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Petal.Width and iris$Petal.Length\nt = 43.387, df = 148, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9490525 0.9729853\nsample estimates:\n      cor \n0.9628654",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "9 Correlation"
    ]
  },
  {
    "objectID": "9-correlation.html#note-about-results-and-reporting",
    "href": "9-correlation.html#note-about-results-and-reporting",
    "title": "9 Correlation",
    "section": "6 Note about results and reporting",
    "text": "6 Note about results and reporting\nWhen we do report or document the results of an analysis we may do it in different ways depending on the intended audience.\n\n\n6.1 Documenting results only for ourselves (almost never)\nIt would be typical to just comment the R script and have your script and data file in fully reproducible format. This format would also work for close colleague (who also use R), or collaborators, to use to share or develop the analysis. Even in this format, care should be taken to\n\nclean up redundant or unnecessary code\nto organize the script as much as possible in a logical way, e.g. by pairing graphs with relevant analysis outputs\nremoving analyses that were tried but obsolete (e.g. because a better analysis was found)\n\n\n\n\n6.2 Documenting results to summarize to others (most common)\nHere it would be typical to summarize and format output results and figures in a way that is most easy to consume for the intended audience.\n\nYou should NEVER PRESENT RAW COPIED AND PASTED STATISTICAL RESULTS (O.M.G.!).\n\n\n- Ed Harris\n\nA good way to do this would be to use R Markdown to format your script to produce an attractive summary, in html, MS Word, or pdf formats (amongst others).\nSecond best (primarily because it is more work and harder to edit or update) would be to format results in word processed document (like pdf, Word, etc.). This is the way most scientists tend to work.\n\n\n\n6.3 Statistical summary\nMost statistical tests under NHST will have exactly three quantities reported for each test:\n\nthe test statistic (different for each test, r (“little ‘r’”) for the Pearson correlation)\nthe sample size or degrees of freedom\nthe p-value.\n\n\nFor our results above, it might be something like:\n\nWe found a significant correlation between petal width and length (Pearson’s r = 0.96, df = 148, P &lt; 0.0001)\n\n\nNB:\n\nthe rounding of decimal accuracy (usually 2 decimals accuracy, if not be consistent)\nthe formatting of the p-value (if smaller than 0.0001 ALWAYS use P &lt; 0.0001; also never report the P value in scientific notation).\n\n\nThe 95% confidence interval of the estimate of r is also produced (remember, we are making an inference on the greater population from which our sample was drawn), and we might also report that in our descriptive summary of results, if it is deemed important.\n\n\n# 4 Validate\nhist(iris$Petal.Length) # Ummm..\n\n\n\n\n\n\n\nhist(iris$Petal.Width) # Yuck\n\n\n\n\n\n\n\n# We violate the assumption of Gaussian\n\n# ... Relatedly, we also violate the assumption of independence \n# due to similarities within and differences between species!\n\n\n\n\n6.4 Flash challenge\nWrite a script following the steps to question, graph, test, and validate for each iris species separately. Do these test validate? How does the estimate of correlation compare amongst the species?\n\n\n\n6.5 Correlation alternatives to Pearson’s\nThere are several alternative correlation estimate frameworks to the Pearson’s; we will briefly look at the application of the Spearman correlation. The main difference is a relaxation of assumptions. Here the main assumptions are that:\n\nThe data are ranked or otherwise ordered\nThe data rows are independent\n\n\nHence, the Spearman rank correlation descriptive coefficient and test are appropriate even if the data are not “bivariate” Gaussian (this just means both variables are Gaussian) or if the data are not strictly linear in their relationship.\n\n\n# Spearman rank correlation ####\n\n# Try this:\n\nheight &lt;- c(18.4, 3.2, 6.4, 3.6, 13.2, 3.6, 15, 5.9, 13.5, 10.8, 9.7, 10.9, \n18.8, 11.6, 9.8, 14.4, 19.6, 14.7, 10, 16.8, 18.2, 16.9, 19.8, \n18.2, 2.6, 18.8, 8.4, 18.9, 9.5, 19)\n\nvol &lt;- c(16.7, 17.8, 17.9, 21.1, 21.2, 21.4, 23.2, 25.7, 26.1, 26.4, \n26.8, 27, 27.8, 28.2, 28.5, 28.6, 28.7, 29.1, 30.2, 31, 32.3, \n32.3, 32.5, 33.2, 33.5, 33.8, 35.2, 36.1, 36.6, 39.5)\n\nplot(height, vol,\n     xlab = \"Height\",\n     ylab = \"Volume\",\n     pch = 16, col = \"blue\", cex = .7)\n\n\n\n\n\n\n\n# Spearman coefficient\ncor(height, vol, method = \"spearman\")\n\n[1] 0.3945026\n\n# Spearman test\ncor.test(height, vol, method = \"spearman\")\n\nWarning in cor.test.default(height, vol, method = \"spearman\"): Cannot compute\nexact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  height and vol\nS = 2721.7, p-value = 0.03098\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.3945026 \n\n\n\nNB the correlation coefficient for the Spearman rank correlation is notated as “rho” (the Greek letter \\(\\rho\\) - sometimes avoided by non-math folks because it looks like the letter “p”), reported and interpreted exactly like the Pearson correlation coefficient, r.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "9 Correlation"
    ]
  },
  {
    "objectID": "9-correlation.html#practice-exercises",
    "href": "9-correlation.html#practice-exercises",
    "title": "9 Correlation",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\nFor the following exercises, use 2 datasets: the waders data from the package {MASS}, and the cape furs dataset cfseals you can download here\n\n\n7.1\nLoad the waders data and read the help page. Use the pairs function on the data and make a statement about the overall degree of intercorrelation between variables based on the graphical output.\n\n\n\n7.2\nThink about the variables and data themselves in waders. Do you expect the data to be Gaussian? Formulate hypothesis statements for correlations amongst the first 3 columns of bird species in the dataset. Show the code to make three good graphs (i.e., one for each pairwise comparison for the first three columns), and perform the three correlation tests.\n\n\n\n7.3\nValidate the test performed in question 2. Which form of correlation was performed, and why. Show the code for any diagnostic tests performed, and any adjustment to the analysis required. Formally report the results of your validated results.\n\n\n\n7.4\nLoad the 2.3-cfseal.xlsx data and examine the information in the data dictionary. Analyse the correlations amongst the weight, heart, and lung variables, utilizing the 1 question, 2 graph, 3 test and 4 validate workflow. Show your code and briefly report the results.\n\n\n\n7.5\nComment on the expectation of Gaussian for the age variable in the cfseal data. Would expect this variable to be Gaussian? Briefly explain you answer and analyse the correlation between weight and age, using our four-step workflow and briefly report your results.\n\n\n\n7.6\nWrite a plausible practice question involving any aspect of the use of correlation, and our workflow. Make use of the data from either the waders data, or else the cfseal data.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "9 Correlation"
    ]
  },
  {
    "objectID": "7-question-explore.html",
    "href": "7-question-explore.html",
    "title": "7 Explore data",
    "section": "",
    "text": "We call sizing up the data “weighing the pig”",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "7 Explore data"
    ]
  },
  {
    "objectID": "7-question-explore.html#question-explore-analyze-a-workflow-for-data-science",
    "href": "7-question-explore.html#question-explore-analyze-a-workflow-for-data-science",
    "title": "7 Explore data",
    "section": "1 Question, explore, analyze (a workflow for data science)",
    "text": "1 Question, explore, analyze (a workflow for data science)\n\n\nA dataset often comes to the Data Scientist in an imperfect state, possibly incomplete, containing errors, and with minimal description. Likewise, it may contain wonderful knowledge, there to discover. Either way, your first task is to weigh the pig.\n\n\nThe very first task for any data analysis is to gain an understanding of the data itself. This typically involves examining the variables. Are they as we expect? Do we need to adjust the variable types?\n\nEDA Exploratory Data Analysis\nThis almost always involves graphing the data, and possibly examining numerical summaries and statistical assumptions. Further, it is necessary to look for errors in the data both trivial (e.g. misspelling factor level names like “control” with an extra space “control”), and more serious errors such as numerical typographical errors (e.g. misplacing a decimal point is a classic: height of 5 men in feet: c(5.5, 5.8, 6.1, 5.9, 52.).\nIn total, this part of data analysis is sometimes referred to as Exploratory Data Analysis.\nEDA is part practical and part philosophical in that is requires skill and experience, but is also subjective. Think of it as a step that might take a long while, where the data scientists decides what the analysis is that will be applied to the data, that the analysis is correct and appropriate. Ironically, while EDA is considered very important and can take a large proportion of the total time spent analyzing data, it is usually only reported on very briefly if at all.\nThe order of operation for most analyses should be\n\n1 question\n2 explore\n3 analyse\n\n\n\nYou choose your data analysis prior to collecting the first data point.\n\nFocus on the question and make sure it is clear in formulation, and choose an analysis approach that can resolve the question , given the data… But the data collection should be DESIGNED to fit the question and chosen analysis prior to collection. Explore the data to examine any assumptions required for the analysis, including the use of graphing and any diagnostic or summary statistics. Finally, perform and summarize the analysis. We will practice this workflow for different basic questions, with an emphasis on simple quantitative data.\n\n\n1.1 Objectives\n\nQuestion formulation and hypothesis testing\nSummarize: Weighing the Pig\nVariables and graphing\n“Analysis” versus “EDA”\nStatistical Analysis Plan: the concept\nPractice exercises",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "7 Explore data"
    ]
  },
  {
    "objectID": "7-question-explore.html#question-formulation-and-hypothesis-testing",
    "href": "7-question-explore.html#question-formulation-and-hypothesis-testing",
    "title": "7 Explore data",
    "section": "2 Question formulation and hypothesis testing",
    "text": "2 Question formulation and hypothesis testing\n\nIt is the primary responsibility of the scientist to agree on the specific details of generating evidence from data to answer questions (i.e., statistical analysis). When these roles are occupied by the same person, this matter should be settled before collecting any data.\n\nThe general topic of formulating statistical questions is vast; many books have been written on the subject. The tradition and practice of statistical analysis has evolved through time. Here we will focus on the traditional starting point for a “first statistics course”, within the context of Null Hypothesis Significance testing (NHST).\n\n\n2.1 Sampling concept and NHST\nThe gambit of NHST is that there is a population of interest but that the population cannot be directly measured because it is too big or otherwise inconvenient or impossible to measure. Thus, experimental samples are drawn randomly from the population, possibly subjected to experimental conditions, and the magnitude of observed differences or measured associations are summarized by various test statistics and compared to how likely such an observed difference or association would be to observe in the absence of the hypothesized effect.\nThe null hypothesis is the one consistent with no effect or difference. We evaluate whether to reject the null hypothesis using the P-value, the (conditional) probability that the observed effect is unlikely to arise duie to sampling or experimental error.\nTraditionally, the P-value is compared to the alpha value, almost always set to 0.05. This alpha value can be interpreted as the maximum probability that is acceptable of making a mistake and concluding there IS a difference, when in fact a difference does not exist. When the P-value is less than 0.05, we conclude there is a difference, rejecting the null hypothesis and “accepting” the hypothesis we predicted was true, usually referred to as the alternative hypothesis.\n\n\n\n2.2 NHST notes\nBenefits of NHST\n\nFamiliar and acceptable to majority of researchers\nTypically robust to assumptions when applied correctly\nStrong framework for evidence, especially for experiments\nThe basic idea is objective and simple\n\n\nCriticism of HNST\n\nOften conceived, applied and interpreted under error\nValidation of analysis (e.g. assumptions testing) is often neglected\nEducation for applied researchers often deficient\nThough simple, practitioners may be ignorant of subtle concepts\n\n\n\n\n2.3 Further reading\nIf the idea is new to you that NHST in statistics is not perfect and you want to get serious about understanding why, like most subjects, you will need to pursue further sources.\nAnderson, D.R., Burnham, K.P. and Thompson, W.L., 2000. Null hypothesis testing: problems, prevalence, and an alternative. The journal of wildlife management, pp.912-923.\nNickerson, R.S., 2000. Null hypothesis significance testing: a review of an old and continuing controversy. Psychological methods, 5(2), p.241.\nNix, T.W. and Barnette, J.J., 1998. The data analysis dilemma: Ban or abandon. A review of null hypothesis significance testing. Research in the Schools, 5(2), pp.3-14.\nStephens, P.A., Buskirk, S.W., Hayward, G.D. and Martinez Del Rio, C., 2005. Information theory and hypothesis testing: a call for pluralism. Journal of applied ecology, 42(1), pp.4-12.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "7 Explore data"
    ]
  },
  {
    "objectID": "7-question-explore.html#summarize-weighing-the-pig",
    "href": "7-question-explore.html#summarize-weighing-the-pig",
    "title": "7 Explore data",
    "section": "3 Summarize: Weighing the Pig",
    "text": "3 Summarize: Weighing the Pig\n\n\nThe best way gain skill in handling data is to practice.\n\nWeighing the pig is the term we use to describe creating a summary-at-a-glance of a dataset. Usually this includes graphics and statistical summary, as well a description of how much data we have. A key consideration is, also, the specification of the variables.\nWe will practice data handling with the data file chickwts.xlsx.\nDownload the file, read it into a data object in R called chicks, and convert the feed variable to a factor if necessary.\n# Try this:\n\n# Download the 7-chickwts.xlsx file, read it into a data \n# object in R called \"chicks\", \n# and convert the \"feed\" variable to a factor if necessary.\n\n# Do not neglect looking inside the \"raw\" data file\n# Is it as you expect?  Is the data dictionary present and clear?\n\n# Load necessary libraries\nlibrary(openxlsx)\n\n# Read file\nsetwd(\"D:/Dropbox/git/DSgarage/public/data\") # NB change to YOUR file path...\nchicks &lt;- read.xlsx(\"7-chickwts.xlsx\")\n\n# Convert feed to factor if needed\nclass(chicks$feed) # Character\nchicks$feed &lt;- factor(chicks$feed)\nclass(chicks$feed) # Factor\n\n\n3.1 Chick data\n\nThe hypothesis voices “how you think the world works” or what you predict to be true”\n\nThe hypothesis we believe is true for the chicks dataset might be phrased in different ways.\n\nChick weight differs after 6 weeks according to feed additive type\nMean chick weight varies according to feed additive type\nThe variance between chick weight for different feed additives is bigger than the variance within chick weight as a whole\n\n\n\n\n3.2 Hypothesis\nThe minimum amount of information we are usually interested in when sizing up a dataset is How much data is there?, What is the central tendency (e.g. the mean, variance, etc.)?, and possibly Are there rare values?.\nWe would typically start graphing the data right away. If we have a notion of what our questions or hypotheses are, they should inform the initial peek at the data. For example, in the chickwts data, we know our question will be related not to the overall central tendency of chick weight, but to chick weight for each individual feed type.\nWe do not approach this sizing up of the data in a workhorse fashion, merely to check a tick box. We are looking quickly for details in the data that give us insight into what the data is like. For example, we peek at whether the mean and median are close to each other (indicator our data may be Gaussian), we compare the standard deviation, variance or standard error of a numeric variable relative to different levels of a factor, to see if they are similar.\n# Try this:\n\n# Summarize the whole dataset\n# summary() provides summary statistics for numeric variables and counts\nsummary(chicks)\n\n# we might want to look at summary for different levels of feed\n?summary\nsummary(object = chicks$weight[which(chicks$feed == \"casein\")])\nsummary(object = chicks$weight[which(chicks$feed == \"horsebean\")])\n# etc. - this method is easy but inelegant?\n\n# aggregate()\n?aggregate\n\n# mean\naggregate(x = chicks$weight, by = list(chicks$feed), FUN = mean)\n\n# standard deviation\naggregate(x = chicks$weight, by = list(chicks$feed), FUN = sd)\n\n# You can make your own function for the FUN argument\n# stadard error of mean, SEM = standard deviation / square root of sample size\naggregate(x = chicks$weight, by = list(chicks$feed), \n          FUN = function(x){ sd(x)/sqrt(length(x)) })\n\n# You can apply several functions and name them!\naggregate(x = chicks$weight, by = list(feed = chicks$feed), \n          FUN = function(x){ c(mean = mean(x), \n                               sd = sd(x),  \n                               SEM = sd(x)/sqrt(length(x)))})",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "7 Explore data"
    ]
  },
  {
    "objectID": "7-question-explore.html#variables-and-graphing",
    "href": "7-question-explore.html#variables-and-graphing",
    "title": "7 Explore data",
    "section": "4 Variables and graphing",
    "text": "4 Variables and graphing\n\nA good graph usually tells the whole story, but a bad graph is worse than no graph at all.\n\n\n\n\n\n\nXKCD Convinced by data\n\n\n\n\nThere are a few topics in graphing data that are important to consider here, but the topic is wide and deep, analytical, creative, and even artistic. We make a distinction between graphs used to explore data during EDA (meant to be “consumed” only by the data scientist who made them and are of no use to document a pattern to others) and graphs intended to constitute evidence.\n\n\n4.1 Scientific graphs\nA few graphing principles:\n\nMust convey the relevant information\nShould be consistent in aesthetics\nMust be self-contained (meaning is contained 100% within the figure and legend)\nShould reflect a hypothesis or statistical concept (if not purely descriptive)\nShould be appropriate to the data\n\n\nYou can think of R graphics as a way to “build up information in layers” onto a graph. There are many aesthetic features of graph that can be controlled, like adding colors, text, lines, legends, etc. The R graphics system is very simple to use, but can also be very powerful (mastering this takes practice). We make a distinction here between R base graphics and packages that can be used to make specialized and varied graphs (like the powerful and popular package {ggplot})\n\n\n\n4.2 Layering information\nWe can look at graphing the chicks data in a few different ways. We will try a few different graphs in this way, building up features. We might build up features on a graph using arguments in a particular graph function.\n\nLike, adding\n\na main title with the argument main\nthe x axis title with the argument xlab\nadding lines with the functions abline() or lines()\n\n\n\n\n4.3 Types of graphs\nTypically you would choose the type of graph that both fits the type of data you have and that conveys the information you wish to examine or showcase. E.g., for a single numeric variable, you might wish to show:\n\nThe distribution of data with a histogram: hist()\nThe central tendency relative to a factor with a boxplot: boxplot()\n\n\nHistogram of the chicks data\n# The least you can do\nhelp(hist)\nhist(x = chicks$weight)\n\n\nAdd a title with main\n# Argument main\nhist(x = chicks$weight,\n     main = \"Distribution of chick weights (all feeds)\")\n\n\nAdd an x axis title with xlab\n# x axis title\nhist(x = chicks$weight,\n     main = \"Distribution of chick weights (all feeds)\",\n     xlab = \"Chick weight (grams)\")\n\n\nAdd a vertical line for the weight mean with abline()\n# Add vertical line for mean weight\nhist(x = chicks$weight,\n     main = \"Distribution of chick weights (all feeds)\",\n     xlab = \"Chick weight (grams)\")\n\nhelp(abline)\nabline(v = mean(chicks$weight), col = \"red\", lty = 2, lwd = 3)\n\n\n# Try a boxplot\n\nhelp(boxplot)\nboxplot(x = chicks$weight)\n\n# I have seen worse graphs, but I can't remember when.\n# Flash challenge: Improve the graph\n\n\n# weight as a function of feed\nboxplot(formula = weight ~ feed,\n        data = chicks)\n# This is probably a good representation of our hypothesis\n# Flash challenge: Improve the graph...",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "7 Explore data"
    ]
  },
  {
    "objectID": "7-question-explore.html#analysis-versus-eda",
    "href": "7-question-explore.html#analysis-versus-eda",
    "title": "7 Explore data",
    "section": "5 “Analysis” versus “EDA”",
    "text": "5 “Analysis” versus “EDA”\nAlthough you could consider Exploratory Data Analysis, EDA, an important part of the complete process of data analysis, we might make a distinction between “Analysis” the part of analysis that generates Evidence, and that of EDA which is used to explore data and test assumptions.\n\n\n5.1 Analysis\nA data analysis is\n\nDesigned to fit a specific question or hypothesis\nPart of a workflow: Informal hypothesis statement (in plain language) &gt; Statistical hypothesis (specifies a or implies a statistical test) &gt; Evidence (the specific results)\nDesigned and usually formatted to present to others, such as in a report or a scientific manuscript\nContains only bare essentials as relates to the initial hypothesis (e.g. a good graph, the summary of a statistical analysis)\nShould strictly be reproducible via a script and archived data\nDone in conjunction with EDA\n\n\n\n\n5.2 EDA\nExploratory data analysis is\n\nInformal and may be haphazard\nDesigned to explore or gain understanding of data\nAssumptions testing\nUsually not designed to document or show to others\nOccurs primarily before (every) analysis\nMay or may not be documented to be reproducible\nDone before the final, evidence-generating Analysis\n\n\nWe can keep this concept of EDA versus Analysis in our mind while we discuss the Statistical Analysis Plan.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "7 Explore data"
    ]
  },
  {
    "objectID": "7-question-explore.html#statistical-analysis-plan-the-concept",
    "href": "7-question-explore.html#statistical-analysis-plan-the-concept",
    "title": "7 Explore data",
    "section": "6 Statistical Analysis Plan: the concept",
    "text": "6 Statistical Analysis Plan: the concept\n\nI have a cunning (statistical analysis) plan -Baldrick\n\nA Statistical Analysis Plan (SAP) is a formal document that should be used to design data analysis. One of the most important functions of the SAP is to make a formal connection between the hypothesis, the data collected and and the method of analysis that will be used to generate evidence to support or refute the hypothesis. This is conducted before any data are collected.\nThe components of a basic SAP are:\n\nThe hypotheses stated in plain language\nEach hypothesis translated into a specific statistical model\nSpecification of data and and data collection methods\n\n-) Specification of effect size\n\nJustification of sample size through power analysis or other means\n\n\nDefinition of all of these components is beyond the boundaries of this Bootcamp, however the explicit connection of hypotheses with a statistical model is one of the very basic elements of best practice in science.\n\n\n6.1 The scientific method, Classic version\nWe usually learn the scientific method as a cycle where we conceive a problem, form a hypothesis, conduct an experiment, evaluate the result and so on. We learn and teach this as a literal cycle.\n\n\n\n\n\nThe classic view of the scientific process\n\n\n\n\nThis classic view of the scientific process implies that we plan the analysis only after we conduct the experiment and collect data. While many data scientists or statisticians would agree that this model is widely used in science, it is considered very poor practice for several reasons.\n\nThe expected difference or relationship (i.e., the effect size) should explicitly be part of the hypothesis and quantified BEFORE collecting data\nThe statistical test must be chosen prior to collect the data to insure the evidence matches the expectation\nThe sample size should be justified, using power analysis or a less formal means. Collecting too little data will likely result in failing to detect a difference (even if your hypothesis is correct!); Collecting too much data is simply a waste of resources.\n\n\n\n\n\n\nScientific Process - what we teach children in school is not quite right\n\n\n\n\n\n\n6.2 Best practice scientific method\nThe traditional view of the scientific method should probably be adjusted to explicitly accommodate planning the analysis at the same time as the hypothesis formulation stage. Likewise, the analysis plan should specifically influence the design of the data collection for the experiment.\n\n\n\n\n\nModern scientific process\n\n\n\n\nA modern view of best practice of scientific endeavor includes an experimental design phase, with consideration of effect size and power analysis, and the production of a statistical analysis plan that contains a formal statistical hypothesis. All off this happens prior to any data collection.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "7 Explore data"
    ]
  },
  {
    "objectID": "7-question-explore.html#practice-exercises",
    "href": "7-question-explore.html#practice-exercises",
    "title": "7 Explore data",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\nFor the following questions, use the field-trial.xlsx dataset.\nThis is real data in Tidy Data format, but our information for these exercises is limited precisely to the contents of the file, including the data dictionary. In this experiment, seeds were raised under field trial conditions for two weeks to look at the effect of different treatment conditions on mass of gain during germination. There are several measured variables, with the calculated pct variable probably intended to be the dependent variable, with the factor treatment being the main explanatory variable for variation in pct.\n\n\n7.1\nShow code to set up an R analysis file with a header, table of contents, and a setup section that sets your working directory, loads any required libraries and reads in the data. Call the data.frame object you create seed.\n\n\n\n7.2\n\npct, wet and dry should be numeric\nblock and trial should be factors\ntreatment should be a factor with the level “Control” set as the reference.\n\nShow the code to do this.\n\n\n\n7.3\nUse aggregate() to calculate the mean, standard deviation, standard error, and the count (e.g. length()) of pct for each level of treatment. Show the code.\n\n\n\n7.4\nMake a fully labelled boxplot of the pct variable as a function of treatment. Add a horizontal line (red and dashed) for the overall mean of pct, and two horizontal lines (gray, dotted) for the overall mean of pct +/- 1 standard deviation.\n\n\n\n7.5\n(hard: may require tinkering and problem solving)\nExperiment making a boxplot showing pct ~ treatment separated for each trial\n\n\n\n7.6\nWrite a plausible practice question involving aggregate() and boxplot() in-built R dataset iris.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "7 Explore data"
    ]
  },
  {
    "objectID": "5-data-frames.html",
    "href": "5-data-frames.html",
    "title": "5 Data frames",
    "section": "",
    "text": "Like your room, data should be tidy",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "5 Data frames"
    ]
  },
  {
    "objectID": "5-data-frames.html#data-frames-in-r",
    "href": "5-data-frames.html#data-frames-in-r",
    "title": "5 Data frames",
    "section": "1 Data frames in R",
    "text": "1 Data frames in R\nNB for this page we assume you have access to Microsoft Excel. However, similar spreadsheet software (like Libre Office Calc) will work fine.\n\n\nThe first step in using R for data analysis is getting your data into R. The first step for getting your data into R is making your data tidy.\n\nThe commonest question we have experienced for new users of R who want to perform analysis on their data is how to get data into R. There is good news and bad news. The good news is that it is exceedingly easy to get data into R for analysis, in almost any format. The bad news is that a step most new users find challenging is taking responsibility for their own data.\nWhat we mean here is that best practice in data management involves active engagement with your dataset. This includes choosing appropriate variable names, error checking, and documenting information about variables and data collection. We also aim to avoid proliferation of excessive dataset versions, and, worst of all, embedding graphs and data summaries into Excel spreadsheets with data.\n\n\n1.1 Objectives\n\nCommon data file types\nExcel, data setup, and the Data Dictionary\nGetting data into R\nManipulating variables in the Data Frame\nPractice exercises",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "5 Data frames"
    ]
  },
  {
    "objectID": "5-data-frames.html#tidy-data-concept",
    "href": "5-data-frames.html#tidy-data-concept",
    "title": "5 Data frames",
    "section": "2 Tidy data concept",
    "text": "2 Tidy data concept\n\n2.1 The Tidy Data concept\nA concept to streamline data preparation for analysis is Tidy Data. The basic idea is to format data for analysis in a way that\n\nArchives data for reproducibility of results, and\nMakes the data transparent to colleagues or researchers by documenting a data dictionary.\n\n\nThis page is all about the tidy data concept and a simple recipe for best practice to prepare data for analysis and to get data into R.\nThe definition of Tidy Data is generally attributed to Wickham (2014), and is based on the idea that with a few simple rules, data can be archived for complete reproducibility of results. This practice benefits any user because it facilitates collaboration at the same time as documenting both data and analysis methods for value to future use.\nThe essentials of Tidy Data are:\n\nEach variable should be in a column\nEach independent observation should be in a row\nA Data Dictionary should be associatied with the dataset, such that completely reproducible analysis is possible",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "5 Data frames"
    ]
  },
  {
    "objectID": "5-data-frames.html#common-data-file-types",
    "href": "5-data-frames.html#common-data-file-types",
    "title": "5 Data frames",
    "section": "3 Common data file types",
    "text": "3 Common data file types\nThe best file type for the majority of people to archive data for analysis is in a plain text Comma Separated Values (CSV or .csv) file, or just an Excel Spreadsheet. Best practice in contemporary scientific data analysis dictates that proprietary data formats should be avoided, like those produced by SPSS, Genstat, Minitab or other programs.\nThe reason for this is that data stored in those formats is not necessarily useful to people who do not have access to the software, and that for archiving purposes, such software file formats tend to change over time. While Excel is a proprietary format, we find that it is is easy to use, (almost completely) ubiquitous, and relatively resilient to backwards compatibility issues. Thus, sticking to CSV or Excel is a rule you should have a very good reason if you choose to break from it.\nWe recommend using Excel to store data with the goal (for simple datasets and analyses) of having one table for the actual data, in Tidy Data format, and a second tab consisting of a Data Dictionary where each variable is described in enough detail to completely reproduce any analysis. Generally, no formatting or results should ever be embedded in an Excel spreadsheet that is used to store data.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "5 Data frames"
    ]
  },
  {
    "objectID": "5-data-frames.html#excel-data-setup-and-the-data-dictionary",
    "href": "5-data-frames.html#excel-data-setup-and-the-data-dictionary",
    "title": "5 Data frames",
    "section": "4 Excel, data setup, and the Data Dictionary",
    "text": "4 Excel, data setup, and the Data Dictionary\n\n4.1 Tidy Data and Excel\nFor this section, you should download the following files in Excel (.xlsx) format:\n\nTidy Data example Excel file\nThe exact same information as un-Tidy Data\n\n\nThe aphid experiment\n\nYou are contacted by someone who wants help with data analysis and they give you some information about their experiment. They are interested in how diet affects the production of an important metabolite in pest aphids. They designed an experiment with a control treatment where aphids were allowed to feed on plain plants, another treatment where their diet was supplemented with one additive, “AD”, and a third treatment where their diet was supplemented with two additives, “AD” and “SA”. Another factor was aphid Genus, where individuals from the genera Brevicoryne and Myzus were tested. Three replicates of each treatment combination were performed:\n\n\naphid genus [2 levels] \\(\\times\\) food treatment [3 levels].\n\n\nThe metabolite of interest was measured with a spectrometer using three individual aphids from each replicate. The spectrometer peak area (an arbitrary scale) represents the total amount of the metabolite, which was converted to a real scale of metabolite total concentration. Finally, this total concentration was divided by 3 to estimate the concentration of the metabolite in each individual aphid.\n\n\n\n\n4.2 Untidy data\nHave a look at the file 5-untidy.xlsx in Excel.\n\nThe aphid dataset is fairly small and it is readable by humans, but in its current form it is not usable for analysis in R or other statistical software and there are a few ambiguous aspects which we will explore and try to improve.\n\nUntidy\n\n\n\nUntidy data\n\n\n\n\nThe file contains embedded figures and summary tables\nThere is empty white space in the file (Row 1 and Column A)\nThe variable names violate several naming conventions (spaces, special characters)\nMissing data is coded incorrectly (Row 13 was a failed data reading, but records zeros for the actual measurements)\nConversion information accessory to the data is present (Row 3)\nThere is no Data Dictionary (i.e. explanation of the variables)\nThe Aphid and Diet treatments are “confounded” in their coding\nWhat the heck is the “RT” column (most of the values are identical)\n\n\nNow, have a look at the Tidy Data version of the data file.\n\n\n\nTidy version of the data\n\n\n\n\nThe embedded figures have been removed\nThe white space rows and columns have been removed\nThe variable names have been edited but still are equally informative\nMissing data is coded correctly with “NA”\nThe conversion info has been removed and placed in the Data Dictionary\nA complete Data Dictionary on a new tab (“dictionary”) was added, explaining each variable\nThe aphid and food treatment variables were made separate\n\n\nTidy Data version Data Dictionary tab\n\n\n\nTidy data dictionary\n\n\n\nNotice in the Data Dictionary how there is a row for each variable with the name of the variable and an explanation for each variable.\n\n\n\n4.3 CSV files\nOnce your data is tidy, it is very easy to read in Excel data files, or they can be exported into a text file format like CSV (comma separated values) to read straight into R or other programs.\nHave a look at the Tidy Data dataset in .csv file format.\nOpen it with a plain text editor (e.g. Notepad in Windows, or similar). You will notice that each column entry is separated from others with a comma ,, hence the name Comma Separated Values!\n\nTidy csv\n\n\n\nTidy csv",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "5 Data frames"
    ]
  },
  {
    "objectID": "5-data-frames.html#getting-data-into-r",
    "href": "5-data-frames.html#getting-data-into-r",
    "title": "5 Data frames",
    "section": "5 Getting data into R",
    "text": "5 Getting data into R\nWe still need to actually “read data into R” from external files. There are a very large number of ways to do this and most people eventually find their own workflow. We think it is best for most people to use Excel or CSV files in Tidy Data format.\nThe basics of reading external files from a script is to to use the read.xlsx() function in the {openxlsx} package (you will probably need to install this with the install.packages() function), or else to use read.csv() that comes standard in base R. We will briefly try both.\n\n\n5.1 Working directory\n\nBest practice when working with files is to formally set your “working directory”. Basically, this tells R where your input (i.e. data) and output (like scripts or figures) files should be.\n\n\nThere are several viable ways to set your working directory in R, e.g. via the Session menu:\n\n\n\nWorking directory\n\n\n\nHowever, the best way to do this this is to set your working directory using code with the setwd() function. Here we should a workflow for Windows, which is similar on other computer systems. We consider the step of setting a working directory essential for best practice.\nIf you are unfamiliar with how to obtain the path to your working directory, open windows explorer, navigate to the folder you wish to save your script, data files and other inputs and outputs. You can think of this folder as one that contains all related files for e.g. a data analysis project, or perhaps this bootcamp module!\n\n\n\n\nYour directory window might look similar to this\n\n\n\nNotice the folder “view” is set to “Details”, and also notice that the folder options are set to “Show file extensions”. We recommend setting your own settings like this (if using Windows Explorer).\nThe pointer is indicated in the circle marked A in the picture above.\nLeft click the area to the right of the folder text once (where the pointer is in the picture above) and you should see something similar to the figure below, where the folder path is displayed and the text is automatically selected.\n\n\n\n\nYour selected file path might look similar to this\n\n\n\nAssuming you have opened the File Explorer in your working directory or navigated there, the selected PATH is the working directory path which you can copy (Ctrl + c in Windows). In your script, you can now use getwd() to get and print your working directory path, and setwd(), which takes a single character string of the path for your working directory for the argument dir , to set it.\n\n\nR file paths use the forward slash symbol “/” to separate file names. A very important step for Windows users when setting the working directory in R is to change the Windows default “” for forward slashes…\n\n\n\n\n5.2 Read in your first file\nYou need this for the following code if you did not already download it above: .xlsx data file\n# Try this\n\ngetwd() # Prints working directory in Console\n\nsetwd(\"D:/Dropbox/git-rstats-bootcamp/website/data\")\n\n# NB the quotes\n# NB the use of \"/\"\n# NB this is MY directory - change the PATH to YOUR directory :)\n\ngetwd() # Check that change worked\n\n## Read in Excel data file\n\n\ninstall.packages(openxlsx, dep = T) # Run if needed\n\nlibrary(openxlsx) # Load package needed to read Excel files\n\n# Make sure the data file \"5-tidy.xlsx\" is in your working directory\nmy_data &lt;- read.xlsx(\"5-tidy.xlsx\")\n\nAll being well, you should see the following data object in your Global Environement. Note the small blue button (A, circled below) you can press to espand the view of the variables in your data frame.\n\n\n\nData in your Global Environment\n\n\n\nNote that the same procedure works with Comma Separated Values data files, and other kinds of files that you want to read into R, except that the R function used will be specific to the file type. E.g., read.csv() for CSV files, read.delim for TAB delimited files, or read.table() as a generic function to tailor to many types of plain text data files (there are many others, but this is enough for now).",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "5 Data frames"
    ]
  },
  {
    "objectID": "5-data-frames.html#manipulating-variables-in-the-data-frame",
    "href": "5-data-frames.html#manipulating-variables-in-the-data-frame",
    "title": "5 Data frames",
    "section": "6 Manipulating variables in the Data Frame",
    "text": "6 Manipulating variables in the Data Frame\n\nNow that there is a data frame in your working environment, we can start working with the variables. This is a good time to think about the “R Space” metaphor. You are floating in R Space and you can see a data frame called my_data. You cannot see inside the container, so we will look at methods of accessing the data inside by name…\n\n\n\n6.1 Data manipulation in R\n\nThe names() function\nThe use of the $ operator for data frames\nThe use of the str() function for data frames\nThe use of the index operator [ , ]\nThe use of the attach() function\n\n\nCarefully use the follow code and try some data manipulation on your own.\n\n\n6.2 class()\n# Try this\n\nclass(my_data) # data.frame, a generic class for holding data\n\n\n6.3 names()\nThe names() function returns the name of attributes in R objects. When used on a data frame it returns the names of the variables.\n# Try this\nnames(my_data)\n\n\n6.4 $ operator for data frames\nThe $ operator allows us to access variable names inside R objects. Use it like this:\ndata_object$variable_name\n# Try this\n\nconc.ind # Error because the variable conc.ind is INSIDE my_data\n\nmy_data$conc.ind\n\n\n6.5 str()\nThe str() function returns the STRUCTURE of a data frame. This includes variable names, classes, and the first few values\n# Try this\nstr(my_data) \nThe output similar to the graphical Global Environment view in RStudio. Note the conc.ind variable is classed numeric\nNote the treatment variable is classed as character (not a factor)\n\n\n\n6.6 [ , ] the index operator\nThe index operator allows us to access specified rows and columns in data frames (this works exactly the same in matrices and other indexed objects).\n# Try this\nmy_data$conc.tot # The conc.tot variable with $\nmy_data$conc.tot[1:6] # each variable is a vector - 1st to 6th values\n\nhelp(dim)\ndim(my_data) # my_data has 18 rows, 6 columns\n\nmy_data[ , ] # Leaving blanks means return all rows and columns\n\nnames(my_data) # Note conc.tot is the 6th variable\n\nnames(my_data)[6] # Returns the name of the 6th variable\n\nmy_data[ , 6] # Returns all rows of the 6th variable in my_data\n\n# We can explicitly specify all rows (there are 18 remember)\nmy_data[1:18 , 6] # ALSO returns all rows of the 6th variable in my_data\n\n# We can specify the variable names with a character\nmy_data[ , \"conc.tot\"]\nmy_data[ , \"conc.ind\"]\n\n# Specify more than 1 by name with c() in the column slot of [ , ]\nmy_data[ , c(\"conc.tot\", \"conc.ind\")] \n\n\n6.7 attach()\nThe attach() function makes variable names available for a data frame in R space\n# Try this\nconc.ind # Error; the Passive-Aggressive Butler doesn't understand...\n\nattach(my_data)\nconc.ind # Now that my_data is \"attached\", the Butler can find variables inside\n\nhelp(detach) # Undo attach()\ndetach(my_data)\nconc.ind # Is Sir feeling well, Sir?",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "5 Data frames"
    ]
  },
  {
    "objectID": "5-data-frames.html#practice-exercises",
    "href": "5-data-frames.html#practice-exercises",
    "title": "5 Data frames",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\n\nButterfly data in xlsx format\nThe data are from a small experiment measuring antenna length in butterflies manipulating diet in both sexes.\n\n7.1\nDownload the data file above and place it in a working directory. Set your working directory. Read in the data file and place it in a data frame object named data1. After examining the data, use mean() to calculate the mean of the variable length and report the results in a comment to two decimal points accuracy. Show your R code.\n\n\n\n7.2\nShow the code to convert the diet variable to an ordinal factor with the order “control” &gt; “enhanced”, and the sex variable to a plain categorical factor.\n\n\n\n7.3\nShow code for two different variations of using only the [ , ] operator with your data frame to show the following output:\n       diet length\n8   control      6\n9   control      7\n10  control      6\n11 enhanced      8\n12 enhanced      7\n13 enhanced      9\n\n\n\n7.4\nShow code to read in a comma separated values data file that does not have a header (first row containing variable names).\n\n\n\n7.5\nDescribe in your own words what the attach() function does.\n\n\n\n7.6\nWrite a plausible practice question involving any aspect of manipulation of a data frame.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "5 Data frames"
    ]
  },
  {
    "objectID": "3-functions.html",
    "href": "3-functions.html",
    "title": "3 R Functions",
    "section": "",
    "text": "R as a garage full of tools",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "3 R Functions"
    ]
  },
  {
    "objectID": "3-functions.html#functions-and-packages-toolboxes-r-as-a-garage-full-of-tools",
    "href": "3-functions.html#functions-and-packages-toolboxes-r-as-a-garage-full-of-tools",
    "title": "3 R Functions",
    "section": "1 Functions and Packages Toolboxes (R as a garage full of tools)",
    "text": "1 Functions and Packages Toolboxes (R as a garage full of tools)\n\nThe garage and toolbox metaphors\nOne of the best things about R is that it can be customized to accomplish a huge variety of kinds of tasks: perform all sorts of statistical analyses from simple to bleeding edge, produce professional graphs, format analyses into presentations, manuscripts and web pages, collaboration, GIS and mapping, and a lot more. The tools themselves are contained in toolboxes and in a given toolbox, the tools are related to each other, usually in a focus to the kind of tasks they are suited for.\n\nThe tools in R are functions, and the toolboxes are packages.\n\nWhile R comes with a lot of packages, there is an enormous amount available for instant download at any time you want or need them (over 18,000 different packages at the moment…). Making use of all these resources is usually a case of identifying a problem to solve, finding the package that can help you, and then learning how to use the functions in the package This page is all about introducing functions, packages and the R help system.\n\n\n1.1 Objectives\n\nFunction tour\nUsing functions and getting help\nR packages\nFinding, downloading and using packages\nPractice exercises",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "3 R Functions"
    ]
  },
  {
    "objectID": "3-functions.html#function-tour",
    "href": "3-functions.html#function-tour",
    "title": "3 R Functions",
    "section": "2 Function tour",
    "text": "2 Function tour\n\nThink of functions in R as tools that do work for you.\n\nCode for functions is is simple once you get the idea, but you have to understand how they work to use them in the most powerful way. Also, to make the most of functions, you must get to know which ones perform common tasks, and how to use them. We will practice that in this section. We consider the USE of functions (for a given problem) as a separate issue from discovering a function, and here we focus on USE.\nA generic might look like this: function_name(). The function name is (obviously) the function_name part and all functions must have the bracket notation (). There are some rules for function names and for naming R objects in general, but for now the most important thing to keep in mind is that details like capitalization are important, that is, R is case sensitive.\nThus, function_name() is not the same as Function_name() or function_Name() (see what I did there?).\n\n2.1 Using functions\nFunctions are typically used by providing some information inside the brackets, usually data for the function to do work on or settings for the function. Function values and settings are assigned to function arguments and most functions have several arguments.\n\nfunction_name(argument_1 = value_1, argument_2 = value_2, ...)\nA general rule is that you INPUT information or data into function brackets that you want the function to do work and function OUTPUT is the work being done, sometimes including information output (like the results of a statistical test, or a plot).\n\nEach argument has a unique name\nArgument values are assigned using the equals sign =, the assignment operator\nEach argument is separated by a comma ,\nThe ... means there are additional arguments that can be used optionally (for now we can ignore those)\n\n\n\n\n2.2 Function names\nFinding functions by their names is often easy for very simple and common tasks. For example:\nmean() Calculates the arithmetic mean\nlog() Calculates the log\nsd() Calculates the standard deviation\nplot() Draws plots\nboxplot() Draws boxplots\nhelp() Used to access help pages\nYou get the idea…\n\nThe most important thing here is that you would generally get the help page up as a reference to what arguments are required and how to customize your function use. This is the key to learning R in the easiest way. That is, until you memorize the use and arguments for common functions.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "3 R Functions"
    ]
  },
  {
    "objectID": "3-functions.html#using-functions-and-getting-help",
    "href": "3-functions.html#using-functions-and-getting-help",
    "title": "3 R Functions",
    "section": "3 Using functions and getting help",
    "text": "3 Using functions and getting help\nFor now, let’s assume that you know:\n\nWhat tasks you want to do (maybe outlined with pseudocode), and\nWhat function(s) can perform those tasks.\n\n\nTry this out in your own script:\n## A workflow for using functions ####\n\n## (make pseudocode of steps in comments)\n\n# Overall task: calculate the mean for a vector of numbers\n# Step 1: Code the vector of data - c() function\n# Step 2: Calculate the mean - mean() function\n# Step 3: Plot the data - boxplot()\n\n# Step 1: Code the vector of data - c() function\n\nhelp(c) # We use this a lot - it \"combines\" numbers\nc(2, 6, 7, 8.1, 5, 6) \n\n# Step 2: Calculate the mean - mean() function\n\nhelp(mean) \n# Notice under Usage, the \"x\" argument\n# Notice under Arguments, x is a numeric vector\n\nmean(x = c(2, 6, 7, 8.1, 5, 6)) # Easy\n\n# Step 3: Plot the data - boxplot()\n\nhelp(boxplot) # x again!\nboxplot(x = c(2, 6, 7, 8.1, 5, 6))\n\n# Challenge: Add an axis label to the y-axis - can you find the name of the argument?\n\n\n# without a y-axis label this is not a good graph\n\nboxplot(x = c(2, 6, 7, 8.1, 5, 6))",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "3 R Functions"
    ]
  },
  {
    "objectID": "3-functions.html#r-packages",
    "href": "3-functions.html#r-packages",
    "title": "3 R Functions",
    "section": "4 R packages",
    "text": "4 R packages\nThere are a lot of R packages. These are “toolboxes” often built in the spirit of identifying a problem, literally making a tools that solves the problem, and then sharing the tool for other to use as well. In fact, all official R packages are “open source”, meaning that you may use them freely, but also you can improve them and add functionality. This section is about the basics of R packages that are additional to the base R installation.\nTypically, you only download a package once you identify you need to use functions in it. There are are several ways to accomplish this. We are going to practice 2 different ways, one with R code that is simple and will work no matter how you use R, and one that uses menus in RStudio.\n\n\n4.1 Finding, downloading and using packages\nFinding packages happens a variety of ways in practice. A package may be recommended to you, you might be told to use a particular package for a task or assignment, or you may discover it on the web.\nInstalling and loading packages with code\nThere are 2 steps here - installing, then loading. Installing is very easy to do using the install.packages() function. Loading a package making the functions in it available for use is done using the library() package. Basic usage of these functions is:\n# Step 1: install a package\n\nhelp(install.packages) # just have a look\ninstall.packages(pkgs = \"package_name\")\n\n# The package is downloaded from a remote repository, often\n# with additional packages that are required for use.\n\n# Step 2: load a package\n\nlibrary(\"package_name\")\n\n# Challenge: Install and load the \"ggplot2\" package, and then use help() to look at the help page for the function ggplot().  What kind of R object is required by the \"data\" argument?\n \n\n\n\n4.2 Installing and loading packages with the RStudio Packages tab\nYou can find the packages tab in RStudio in the lower left pane by default.\n\n\n\nPackages tab\n\n\n\nWhen you click on the Packages tab (A in the picture above), you can see a list of packages that are available to you (i.e., in RStudio desktop these have already been downloaded locally).\nIn order to load a package, you can find the package name in the list and click the radio button (B in the picture).\nTo install a package, you can click on the Install button (C in the image).\nYou should see the Install Packages window, where you can enter the name of a package for installation, searching the official Comprehensive R Archive Network (Repository (CRAN)) by default:\n\n\n\n\n\nInstall packages in RStudio",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "3 R Functions"
    ]
  },
  {
    "objectID": "3-functions.html#practice-exercises",
    "href": "3-functions.html#practice-exercises",
    "title": "3 R Functions",
    "section": "5 Practice exercises",
    "text": "5 Practice exercises\n\n\n5.1\nExplain in your own words what the freq argument in the hist() function does. It often helps to practice trail and error to understand what is happening with data. Try experimenting with the data vector below with the hist() function to explore the freq argument:\nc(1,2,4,3,5,6,7,8,6,5,5,5,3,4,5,7)\n\n\n\n5.2\nTailor your code from the hist() example in problem 1 so that your histogram has a main title, axis labels, and set the col argument to “blue”. We are just scratching the surface with plot customization - try to incorporate other arguments to make an attractive graph.\n\n\n\n5.3\nUse the mean() function on the following data vector\nc(1,2,4,3,5,6,7,8,6,5,NA,5,3,4,5,7)\nYou will see an error message. The symbol “NA” has a special meaning in R, indicating a missing value. Use help() for the mean function and implement the na.rm argument to fix the problem. Show your code.\n\n\n\n5.4\nIn your own words, what value is required for the “d” argument in the pwr.t.test() function in the {pwr} package? Show the code involved including any appropriate comment code required to answer this question. (hint: you will probably need to install the package, load it, and use help() on the function name)\n\n\n\n5.5\nEvery official R package has a webpage on the Comprehensive R Archive Network (CRAN) and there are often tutorials called “vignettes”. Google the CRAN page for the package {ggplot2} and find the vignette called “Aesthetic specifications”. Read the section right near the top called “Colour and fill”.\nFollow the instructions to list all of the built-in colours in R and list them in the console. Ed’s personal favourite is “goldenrod”, index number [147]. Can you find the index number for “tomato2”?\n\n\n\n5.6\nWrite a plausible practice question involving the use of help() for an R function.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "3 R Functions"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html",
    "href": "16-collaborative-workflows.html",
    "title": "16 Collaborative Workflows",
    "section": "",
    "text": "Learning Objectives\n\n\n\nBy the end of this lesson, you will be able to:\n\nOrganize data science projects for effective collaboration\nImplement best practices for code sharing and documentation\nUse Git and GitHub for team-based workflows\nMaintain reproducibility in collaborative environments\nResolve common collaboration challenges",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaborative Workflows"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#why-collaborative-workflows-matter",
    "href": "16-collaborative-workflows.html#why-collaborative-workflows-matter",
    "title": "16 Collaborative Workflows",
    "section": "Why Collaborative Workflows Matter",
    "text": "Why Collaborative Workflows Matter\nData science is increasingly a team effort. Effective collaboration requires more than just technical skills—it demands thoughtful project organization, clear communication, and established workflows. When done right, collaboration can:\n\nIncrease productivity through division of labor\nImprove quality through peer review\nEnhance creativity through diverse perspectives\nEnsure continuity when team members change\n\n\n\n\n\n\n\nKey Concept\n\n\n\nA collaborative workflow is a systematic approach to working together on data science projects that maximizes productivity while maintaining reproducibility and quality.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaborative Workflows"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#project-organization-for-teams",
    "href": "16-collaborative-workflows.html#project-organization-for-teams",
    "title": "16 Collaborative Workflows",
    "section": "Project Organization for Teams",
    "text": "Project Organization for Teams\n\nDirectory Structure\nA well-organized project structure helps team members navigate the codebase:\nproject/\n├── README.md           # Overview, setup instructions\n├── CONTRIBUTING.md     # Guidelines for contributors\n├── data/\n│   ├── raw/            # Original, immutable data\n│   └── processed/      # Cleaned, transformed data\n├── code/\n│   ├── data_prep/      # Data preparation scripts\n│   ├── analysis/       # Analysis scripts\n│   └── visualization/  # Visualization scripts\n├── results/\n│   ├── figures/        # Generated plots\n│   └── tables/         # Generated tables\n├── docs/\n│   ├── data_dict.md    # Data dictionary\n│   └── methods.md      # Methodological details\n└── reports/            # Final reports and presentations\n\n\nDocumentation\nComprehensive documentation is crucial for collaboration:\n\nREADME.md: Project overview, setup instructions, and usage examples\nCONTRIBUTING.md: Guidelines for how to contribute to the project\nCode comments: Explain why, not just what, the code does\nFunction documentation: Purpose, parameters, return values, examples\nData dictionary: Describe variables, units, and data sources\nAnalysis log: Document key decisions and their rationale",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaborative Workflows"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#code-sharing-best-practices",
    "href": "16-collaborative-workflows.html#code-sharing-best-practices",
    "title": "16 Collaborative Workflows",
    "section": "Code Sharing Best Practices",
    "text": "Code Sharing Best Practices\n\nStyle Guides\nConsistent coding style makes collaboration easier:\n\nFollow a style guide (e.g., tidyverse style guide for R)\nUse consistent naming conventions\nFormat code for readability\nConsider using linters and formatters\n\n\n\nModular Code\nWrite modular code that others can understand and reuse:\n# Instead of one long script, break into functions\nclean_data &lt;- function(raw_data) {\n  # Data cleaning steps\n  return(cleaned_data)\n}\n\nanalyze_data &lt;- function(clean_data) {\n  # Analysis steps\n  return(results)\n}\n\nvisualize_results &lt;- function(results) {\n  # Visualization steps\n  return(plots)\n}\n\n# Main workflow\nraw_data &lt;- read_csv(\"data/raw/dataset.csv\")\nclean_data &lt;- clean_data(raw_data)\nresults &lt;- analyze_data(clean_data)\nplots &lt;- visualize_results(results)\n\n\nPackage Management\nEnsure consistent package versions across team members:\n# Use renv for project-specific package management\ninstall.packages(\"renv\")\nrenv::init()\nrenv::snapshot()",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaborative Workflows"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#git-workflows-for-teams",
    "href": "16-collaborative-workflows.html#git-workflows-for-teams",
    "title": "16 Collaborative Workflows",
    "section": "Git Workflows for Teams",
    "text": "Git Workflows for Teams\n\nCentralized Workflow\nThe simplest approach for small teams:\n\nEveryone clones the central repository\nTeam members pull before starting work\nMake changes and commit locally\nPull again to merge any new changes\nPush to the central repository\n\n\n\nFeature Branch Workflow\nBetter for larger teams or complex projects:\n\nCreate a branch for each feature or task\nWork on the branch until the feature is complete\nPull the latest main branch and merge it into your feature branch\nCreate a pull request for code review\nMerge into the main branch after approval\n\n\n\nForking Workflow\nCommon for open-source projects:\n\nFork the main repository to your account\nClone your fork locally\nCreate a branch for your changes\nPush to your fork\nCreate a pull request to the main repository",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaborative Workflows"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#code-review-process",
    "href": "16-collaborative-workflows.html#code-review-process",
    "title": "16 Collaborative Workflows",
    "section": "Code Review Process",
    "text": "Code Review Process\nCode reviews improve quality and share knowledge:\n\nGuidelines for Reviewers\n\nBe respectful and constructive\nFocus on the code, not the person\nConsider both functionality and style\nAsk questions rather than making demands\nAcknowledge good practices\n\n\n\nGuidelines for Authors\n\nExplain the purpose of your changes\nKeep pull requests focused and manageable\nRespond to feedback positively\nBe open to suggestions\nThank reviewers for their time",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaborative Workflows"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#maintaining-reproducibility",
    "href": "16-collaborative-workflows.html#maintaining-reproducibility",
    "title": "16 Collaborative Workflows",
    "section": "Maintaining Reproducibility",
    "text": "Maintaining Reproducibility\n\nEnvironment Management\nEnsure everyone works in the same environment:\n\nUse renv (R) or conda (Python) for package management\nDocument system requirements\nConsider containerization with Docker\n\n\n\nData Access\nEstablish protocols for data access and sharing:\n\nUse version-controlled metadata\nDocument data sources and access methods\nConsider data access APIs for large datasets\nImplement appropriate security measures\n\n\n\nContinuous Integration\nAutomate testing to catch issues early:\n\nSet up GitHub Actions or other CI tools\nRun tests automatically on pull requests\nCheck code style and documentation",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaborative Workflows"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#common-collaboration-challenges",
    "href": "16-collaborative-workflows.html#common-collaboration-challenges",
    "title": "16 Collaborative Workflows",
    "section": "Common Collaboration Challenges",
    "text": "Common Collaboration Challenges\n\nChallenge: Merge Conflicts\nWhen two people edit the same part of a file:\n\nPull the latest changes\nIdentify the conflicting files\nOpen the files and resolve conflicts\nCommit the resolved files\nPush the changes\n\n\n\nChallenge: Large Files\nGit struggles with large files:\n\nUse Git LFS (Large File Storage) for binary files\nStore large datasets externally and document access\nConsider data subsets for testing\n\n\n\nChallenge: Onboarding New Team Members\nHelp new team members get up to speed:\n\nMaintain clear setup instructions\nDocument project structure and conventions\nAssign mentors for new members\nCreate starter tasks for learning the codebase",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaborative Workflows"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#exercises",
    "href": "16-collaborative-workflows.html#exercises",
    "title": "16 Collaborative Workflows",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 1\n\n\n\nCreate a CONTRIBUTING.md file for a data science project, outlining guidelines for code style, pull requests, and code review.\n\nExercise 2\nPractice resolving a merge conflict by having two team members edit the same file and then merge their changes.\n\n\nExercise 3\nSet up a feature branch workflow for a small project and practice the complete process from branch creation to pull request and merge.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaborative Workflows"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#next-steps",
    "href": "16-collaborative-workflows.html#next-steps",
    "title": "16 Collaborative Workflows",
    "section": "Next Steps",
    "text": "Next Steps\nCongratulations on completing the R Stats Bootcamp! You now have a solid foundation in:\n\nR programming fundamentals\nStatistical analysis techniques\nReproducible research practices\nCollaborative workflows\n\nContinue building on these skills by:\n\nWorking on your own data analysis projects\nContributing to open-source R packages\nJoining R user groups and communities\nExploring advanced topics in data science",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaborative Workflows"
    ]
  },
  {
    "objectID": "16-collaborative-workflows.html#additional-resources",
    "href": "16-collaborative-workflows.html#additional-resources",
    "title": "16 Collaborative Workflows",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nGitHub Flow Guide\nThe Turing Way: Guide to Collaboration\nrOpenSci Packages: Development, Maintenance, and Peer Review\nTeam Data Science Process",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "16 Collaborative Workflows"
    ]
  },
  {
    "objectID": "14-rmarkdown.html",
    "href": "14-rmarkdown.html",
    "title": "14 R Markdown",
    "section": "",
    "text": "Learning Objectives\n\n\n\nBy the end of this lesson, you will be able to:\n\nUnderstand the basics of R Markdown\nCreate documents that combine code, output, and narrative text\nFormat text using Markdown syntax\nGenerate reports in multiple formats (HTML, PDF, Word)\nUse R Markdown for reproducible data analysis",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#what-is-r-markdown",
    "href": "14-rmarkdown.html#what-is-r-markdown",
    "title": "14 R Markdown",
    "section": "What is R Markdown?",
    "text": "What is R Markdown?\nR Markdown is a file format that allows you to combine R code, its output, and narrative text in a single document. It’s a powerful tool for creating reproducible reports, presentations, dashboards, and even websites.\n\n\n\n\n\n\nKey Concept\n\n\n\nR Markdown documents are plain text files that contain three important elements: 1. YAML header - Document metadata and formatting options 2. Markdown text - For narrative content 3. Code chunks - R code that can be executed",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#getting-started-with-r-markdown",
    "href": "14-rmarkdown.html#getting-started-with-r-markdown",
    "title": "14 R Markdown",
    "section": "Getting Started with R Markdown",
    "text": "Getting Started with R Markdown\nTo create an R Markdown document in RStudio:\n\nClick File → New File → R Markdown\nChoose a document type (HTML, PDF, or Word)\nGive your document a title and author name\nClick OK\n\nThis will generate a template R Markdown file that you can modify.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#yaml-header",
    "href": "14-rmarkdown.html#yaml-header",
    "title": "14 R Markdown",
    "section": "YAML Header",
    "text": "YAML Header\nThe YAML header appears at the top of the document between triple dashes (---):\n---\ntitle: \"My Analysis Report\"\nauthor: \"Your Name\"\ndate: \"2023-06-15\"\noutput: html_document\n---\nYou can customize various aspects of your document by adding options to the YAML header:\n---\ntitle: \"My Analysis Report\"\nauthor: \"Your Name\"\ndate: \"2025-06-16\"\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n    theme: flatly\n    highlight: tango\n    code_folding: show\n---",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#markdown-text",
    "href": "14-rmarkdown.html#markdown-text",
    "title": "14 R Markdown",
    "section": "Markdown Text",
    "text": "Markdown Text\nMarkdown is a lightweight markup language that allows you to format text using simple syntax:\n\nBasic Formatting\n# Heading 1\n## Heading 2\n### Heading 3\n\n**Bold text**\n*Italic text*\n~~Strikethrough~~\n\n[Link text](https://example.com)\n![Image alt text](path/to/image.png)\n\n\nLists\nUnordered list:\n- Item 1\n- Item 2\n  - Subitem 2.1\n  - Subitem 2.2\n\nOrdered list:\n1. First item\n2. Second item\n   a. Subitem a\n   b. Subitem b\n\n\nTables\n| Column 1 | Column 2 | Column 3 |\n|----------|----------|----------|\n| Row 1    | Data     | Data     |\n| Row 2    | Data     | Data     |",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#code-chunks",
    "href": "14-rmarkdown.html#code-chunks",
    "title": "14 R Markdown",
    "section": "Code Chunks",
    "text": "Code Chunks\nCode chunks in R Markdown allow you to execute R code and display its results:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your R code here\n```\n:::\nFor example:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data\ndata(mtcars)\n\n# Calculate summary statistics\nsummary(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n```\n\n\n:::\n:::\n\nChunk Options\nYou can control how code chunks behave using options:\n\n\n\nOption\nDescription\n\n\n\n\neval=TRUE/FALSE\nWhether to evaluate the code\n\n\necho=TRUE/FALSE\nWhether to show the code\n\n\ninclude=TRUE/FALSE\nWhether to include the chunk in output\n\n\nmessage=TRUE/FALSE\nWhether to display messages\n\n\nwarning=TRUE/FALSE\nWhether to display warnings\n\n\nfig.width=7\nFigure width in inches\n\n\nfig.height=5\nFigure height in inches\n\n\n\nExample:\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-rmarkdown_files/figure-html/plot-1.png){width=768}\n:::\n:::",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#inline-code",
    "href": "14-rmarkdown.html#inline-code",
    "title": "14 R Markdown",
    "section": "Inline Code",
    "text": "Inline Code\nYou can also include R code directly within text using backticks and r:\nThe average miles per gallon is 20.090625.\nThis will calculate the mean and insert the result directly into your text.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#generating-reports",
    "href": "14-rmarkdown.html#generating-reports",
    "title": "14 R Markdown",
    "section": "Generating Reports",
    "text": "Generating Reports\nTo generate your report:\n\nClick the Knit button in RStudio\nChoose your desired output format\nView the generated document\n\nR Markdown will: - Run all the code chunks - Generate all outputs (tables, plots, etc.) - Format the text according to Markdown syntax - Combine everything into a single document",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#example-data-analysis-report",
    "href": "14-rmarkdown.html#example-data-analysis-report",
    "title": "14 R Markdown",
    "section": "Example: Data Analysis Report",
    "text": "Example: Data Analysis Report\nHere’s a simple example of an R Markdown document for data analysis:\n---\ntitle: \"Car Performance Analysis\"\nauthor: \"Data Scientist\"\ndate: \"2025-06-16\"\noutput: html_document\n---\n\n## Introduction\n\nThis report analyzes the relationship between car characteristics and fuel efficiency.\n\n## Data Overview\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the mtcars dataset\ndata(mtcars)\n\n# Display the first few rows\nhead(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Summary statistics\nsummary(mtcars[, c(\"mpg\", \"wt\", \"hp\")])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      mpg              wt              hp       \n Min.   :10.40   Min.   :1.513   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:2.581   1st Qu.: 96.5  \n Median :19.20   Median :3.325   Median :123.0  \n Mean   :20.09   Mean   :3.217   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:3.610   3rd Qu.:180.0  \n Max.   :33.90   Max.   :5.424   Max.   :335.0  \n```\n\n\n:::\n:::\n\n\n## Visualization\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a scatterplot\nplot(mtcars$wt, mtcars$mpg, \n     main=\"Car Weight vs. Mileage\",\n     xlab=\"Weight (1000 lbs)\",\n     ylab=\"Miles Per Gallon\",\n     pch=19, col=\"blue\")\n\n# Add a regression line\nabline(lm(mpg ~ wt, data = mtcars), col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](14-rmarkdown_files/figure-html/visualization-1.png){width=960}\n:::\n:::\n\n\n## Statistical Analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a linear model\nmodel &lt;- lm(mpg ~ wt + hp, data = mtcars)\n\n# Display model summary\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ wt + hp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n```\n\n\n:::\n:::\n\n\n## Conclusion\n\nBased on our analysis, there is a significant negative relationship between car weight and fuel efficiency. For every 1,000 lb increase in weight, the miles per gallon decreases by approximately 3.88 units.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#exercises",
    "href": "14-rmarkdown.html#exercises",
    "title": "14 R Markdown",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 1\n\n\n\nCreate a new R Markdown document that includes: - A title and your name - A brief introduction - A code chunk that loads and summarizes a dataset of your choice - A visualization of the data - A brief conclusion\n\nExercise 2\nExperiment with different output formats (HTML, PDF, Word) and observe the differences.\n\n\nExercise 3\nCreate an R Markdown document with a table of contents, code folding, and a custom theme.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#next-steps",
    "href": "14-rmarkdown.html#next-steps",
    "title": "14 R Markdown",
    "section": "Next Steps",
    "text": "Next Steps\nIn the next lesson, we’ll explore Git and GitHub as tools for version control and collaboration in data science projects.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "14-rmarkdown.html#additional-resources",
    "href": "14-rmarkdown.html#additional-resources",
    "title": "14 R Markdown",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nR Markdown: The Definitive Guide\nR Markdown Cheat Sheet\nR Markdown Cookbook",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "14 R Markdown"
    ]
  },
  {
    "objectID": "12-1-way-anova.html",
    "href": "12-1-way-anova.html",
    "title": "12 ANOVA",
    "section": "",
    "text": "Arrange the arithmetic to compare more than two groups",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "12 ANOVA"
    ]
  },
  {
    "objectID": "12-1-way-anova.html#analysis-of-variance-anova",
    "href": "12-1-way-anova.html#analysis-of-variance-anova",
    "title": "12 ANOVA",
    "section": "1 ANalysis Of VAriance (ANOVA)",
    "text": "1 ANalysis Of VAriance (ANOVA)\n\nThe analysis of variance is not a mathematical theorem, but rather a convenient method of arranging the arithmetic. R. A. Fisher (via Wishart 1934. Sppl. J. Roy. Soc. 1(1):26-61.)\n\nPerhaps more so than any other tool, the Analysis of Variance (ANOVA) played a role in literally revolutionizing the idea of objectivity in using data to produce evidence to support claims for certain experimental designs. Invented by the famous statistician and biologist R. A. Fisher while he worked at Rothamsted Research, the intention was for ANOVA to be a useful tool to analyze agriculture experiments. Today, despite many innovations and competing approaches, it remains at the foundation of the basic practice of statistics.\n\n\n1.1 Objectives\n\nThe question of 1-way ANOVA\nData and assumptions\nGraphing\nTest and alternatives\nPractice exercises",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "12 ANOVA"
    ]
  },
  {
    "objectID": "12-1-way-anova.html#the-question-of-1-way-anova",
    "href": "12-1-way-anova.html#the-question-of-1-way-anova",
    "title": "12 ANOVA",
    "section": "2 The question of 1-way ANOVA",
    "text": "2 The question of 1-way ANOVA\nThere are several reasons to use a “1-way ANOVA” experimental design. The scenario usually involves:\n\none numeric continuous dependent variable of interest\na factor that contains 2 or more levels, often with a control\nWhen there are just two levels, the 1-ANOVA is conceptually equivalent to the t-test\n\nAn example might be something like a classic field trial, where crop pest damage is measured (the numeric continuous dependent variable) and the factor compares pest treatment with 3 levels: a control level (no pesticide), an organic pesticide, and a chemical pesticide. The basic question is whether there is an overall difference in the numeric dependent variable amongst the factor levels, however several kinds of questions are also possible to answer:\n\noverall difference test of means between the factors\nComparison of difference of each factor level with the control or other reference factor level\npost hoc tests of difference between specific factor levels, e.g. pairwise tests for a factor with levels A, B, and C might test all possible comparisons A:B, A:C, and B:C.\nExamination of the “sources of variation” observed in the dependent variable, e.g. what proportion of total variation can be accounted for by the factor\n\n\nThe test statistic for ANOVA is the F ratio, which is proportion of variance in the dependent variable between the groups, relative to that within the categories. We will (very briefly) look at this calculation with the aim of gaining a practical understanding of what is going.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "12 ANOVA"
    ]
  },
  {
    "objectID": "12-1-way-anova.html#data-and-assumptions",
    "href": "12-1-way-anova.html#data-and-assumptions",
    "title": "12 ANOVA",
    "section": "3 Data and assumptions",
    "text": "3 Data and assumptions\nThe data we will look at is an experiment in animal genetics, looking at the weight of male chickens (8-week old weight in grams), where weight is the continuous dependent variable. The factor is the sire identity, where the measure young male chicks were sired by one of 5 sires, thus sire is a factor with 5 levels A, B, C, D, and E.\n\n\n3.1 Wide format data\nHere, the numeric data are stored in five vectors, each corresponding to one factor level. One row does not correspond to a single “case” because each column contains measures from different individual offspring. This is an unusual way to store data like this (these days), but we will look at this “wide format” first.\n\n# Try this\n# Data in \"wide format\" \nA &lt;- c(687, 691, 793, 675, 700, 753, 704, 717)\nB &lt;- c(618, 680, 592, 683, 631, 691, 694, 732)\nC &lt;- c(618, 687, 763, 747, 687, 737, 731, 603)\nD &lt;- c(600, 657, 669, 606, 718, 693, 669, 648)\nE &lt;- c(717, 658, 674, 611, 678, 788, 650, 690)\n\nhead(chicken.wide &lt;- data.frame(A, B, C, D, E))\n\n    A   B   C   D   E\n1 687 618 618 600 717\n2 691 680 687 657 658\n3 793 592 763 669 674\n4 675 683 747 606 611\n5 700 631 687 718 678\n6 753 691 737 693 788\n\n\n\n\n\n3.2 Long format data (preferred)\nHere, the numeric data is stored in a single vector, with a factor vector for the sire data. Each row corresponds to a single, independent “case”. This format is preferred and adheres to the “Tidy Data” standard, although it is not hard to move between wide and long formats.\n\n# Try this\n# Data in \"wide format\"  ####\nA &lt;- c(687, 691, 793, 675, 700, 753, 704, 717)\nB &lt;- c(618, 680, 592, 683, 631, 691, 694, 732)\nC &lt;- c(618, 687, 763, 747, 687, 737, 731, 603)\nD &lt;- c(600, 657, 669, 606, 718, 693, 669, 648)\nE &lt;- c(717, 658, 674, 611, 678, 788, 650, 690)\n\nhead(chicken.wide &lt;- data.frame(A, B, C, D, E))\n\n    A   B   C   D   E\n1 687 618 618 600 717\n2 691 680 687 657 658\n3 793 592 763 669 674\n4 675 683 747 606 611\n5 700 631 687 718 678\n6 753 691 737 693 788\n\n# Data in \"long format\"  ####\n# The hard way\nweight &lt;- c(A,B,C,D,E)\n\nsire &lt;- c(rep(\"A\", 8),\n          rep(\"B\", 8),\n          rep(\"C\", 8),\n          rep(\"D\", 8),\n          rep(\"E\", 8) )\n\nhead(data.frame(weight, sire))\n\n  weight sire\n1    687    A\n2    691    A\n3    793    A\n4    675    A\n5    700    A\n6    753    A\n\ntail(data.frame(weight, sire))\n\n   weight sire\n35    674    E\n36    611    E\n37    678    E\n38    788    E\n39    650    E\n40    690    E\n\n# The \"programm-ey\" way\nweight1 &lt;- c(A,B,C,D,E)\n\nsire1 &lt;- vector(mode = \"character\", length = 40)\nfor(i in 1:5) { sire1[(8*i-8)+c(1:8)] &lt;- rep(LETTERS[i], 8) }\n\nhead(data.frame(weight1, sire1))\n\n  weight1 sire1\n1     687     A\n2     691     A\n3     793     A\n4     675     A\n5     700     A\n6     753     A\n\ntail(data.frame(weight1, sire1))\n\n   weight1 sire1\n35     674     E\n36     611     E\n37     678     E\n38     788     E\n39     650     E\n40     690     E\n\n# With function from {tidyr}\nhead(chicken.wide) # From above\n\n    A   B   C   D   E\n1 687 618 618 600 717\n2 691 680 687 657 658\n3 793 592 763 669 674\n4 675 683 747 606 611\n5 700 631 687 718 678\n6 753 691 737 693 788\n\nlibrary(reshape2) # For melt()\n#?melt\nnew.long &lt;- melt(chicken.wide)\n\nNo id variables; using all as measure variables\n\nhead(new.long) # Not bad but note the variable names... \n\n  variable value\n1        A   687\n2        A   691\n3        A   793\n4        A   675\n5        A   700\n6        A   753\n\nnames(new.long)\n\n[1] \"variable\" \"value\"   \n\n# Flash challenge: change the variable names in new.long\n\nnames(new.long) &lt;- c('Sire', 'Weight')\nnames(new.long)\n\n[1] \"Sire\"   \"Weight\"\n\n# NB, you should probably just use long format for your data in the first place!\n\nR output\n&gt; head(new.long) # Not bad but note the variable names\n  variable value\n1        A   687\n2        A   691\n3        A   793\n4        A   675\n5        A   700\n6        A   753\n\n##4 Assumptions of ANOVA\nThe assumptions of ANOVA are similar to those of regression (indeed, both are a specific kind of Linear Model and share the assumptions of the Gaussian Linear Model). The most important to consider now are:\n\nGaussian residuals (we test graphically and with NHST for Gaussian residual distribution)\nHomoscedasticity (we test graphically with a residuals versus fitted values plot)\nEquality of variance (plot of residual versus factor and NHST for == variance)\nIndependent observations (we assume this for now with the chicken data, but will not test is formally)\n\n\n\n## **Assumptions** ####\n\n## - Gaussian residuals ####\n# Make the model object with aov()\n\n# ?aov\nm1 &lt;- aov(formula = Weight ~ Sire, \n         data = new.long)\n\n# Graph to examine Gaussian assumption of residuals\n# NB we use rstandard()\npar(mfrow = c(1,2))\nhist(rstandard(m1),\n     main = \"Gaussian?\")\n\n# Look at residuals with qqPlot()\nlibrary(car) # For qqPlot()\n\nLoading required package: carData\n\nqqPlot(x = m1,\n       main = \"Gaussian?\")\n\n\n\n\n\n\n\n\n[1] 24 38\n\npar(mfrow=c(1,1))\n\n\n\n\n4.1 Formal test of Gaussian residuals\nAt a glance, there are no serious issues with the assumption of Gaussian residual distribution. We can use NHST to help us decide; we will try the shapiro.test().\n\n\nshapiro.test(rstandard(m1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  rstandard(m1)\nW = 0.99182, p-value = 0.9913\n\n\n\n\nThere is no evidence of difference to Gaussian in our residuals for our ANOVA model (Shapiro-Wilk: W = 0.99, n = 40, P = 0.99).\n\n\n\n\n4.2 Homoscedasticity check\nWe will look at the residuals relative to the fitted values.\n\n\n# Plot for homoscedasticity check\nplot(formula = rstandard(m1) ~ fitted(m1),\n     ylab = \"m1: residuals\",\n     xlab = \"m1: fitted values\",\n     main = \"Spread similar across x?\")\nabline(h = 0,\n       lty = 2, lwd = 2, col = \"red\")\n\n# Make the mean residual y points (just to check)\ny1 &lt;- aggregate(rstandard(m1), by = list(new.long$Sire), FUN = mean)[,2]\n# Make the x unique fitted values (just to check)\nx1 &lt;- unique(round(fitted(m1), 6))\n\npoints(x = x1, y = y1, \n       pch = 16, cex = 1.2, col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n4.3 Bartlett test\nFinally, we can use NHST just to have a final check of whether the variance in weight is equal between factor levels. There are several ways to do this; we will use the Bartlett test using bartlett.test(), which compares the variance for more than 2 groups.\n\n\n# NHST to examine  assumption of homoscedasticity\n# (homoscedasticiyy good, heteroscedasticity bad)\n\nbartlett.test(formula = weight~sire, data = new.long)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  weight by sire\nBartlett's K-squared = 1.6868, df = 4, p-value = 0.7931\n\n\n\n\nWe find no evidence that variance in offspring weight differs between sires (Bartlett test: K-sqared = 1.69, df = 4, P = 0.79).",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "12 ANOVA"
    ]
  },
  {
    "objectID": "12-1-way-anova.html#graphing-anova",
    "href": "12-1-way-anova.html#graphing-anova",
    "title": "12 ANOVA",
    "section": "5 Graphing ANOVA",
    "text": "5 Graphing ANOVA\nThe classic way to visualize the 1-way ANOVA is with boxplot, with some way to show the central tendency of the data separately for each factor level. For continuous variables, boxplots show this perfectly. For count variables, barplots are sometimes used with the height set to the mean, along with some form of error bar. Here we will use a boxplot.\n\n\n## basic boxplot ####\n\n# It always pays to make a nice plot\n\n# Do you think sire affects offspring weight?\nboxplot(Weight ~ Sire, data = new.long, \n        main = \"Is this plot good enough?\") \n\n\n\n\n\n\n\n\n\nSo, it looks like sire identity could have an effect on mean male offspring weight. But, is this graph good enough? Can we make it better? Let’s critique it:\n\n(important) The y axis does not indicate the unit of measurement\n(important) Neither axis title is capitalized\n(optional) Adding on the points might add interesting detail\n(optional) Reference line for control (we do not really have a control here) or of the “grand mean” might be useful\n\n\n5.1 Make a better graph\n\n## **Make a better graph** ####\n\nboxplot(Weight ~ Sire, data = new.long,\n        ylab = \"Weight (g)\",\n        xlab = \"Sire\",\n        main = \"Effect of Sire on 8-wk weight\",\n        cex = 0) # Get rid of the outlier dot (we will draw it back)\n\n# Make horizontal line for grand mean\nabline(h = mean(new.long$Weight), \n       lty = 2, lwd = 2, col = \"red\") # Mere vanity\n\n# Draw on raw data\nset.seed(42)\npoints(x = jitter(rep(1:5, each = 8), amount = .1),\n       y = new.long$Weight,\n       pch = 16, cex = .8, col = \"blue\") # Mere vanity",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "12 ANOVA"
    ]
  },
  {
    "objectID": "12-1-way-anova.html#anova-f-test-statistic-and-alternatives",
    "href": "12-1-way-anova.html#anova-f-test-statistic-and-alternatives",
    "title": "12 ANOVA",
    "section": "6 ANOVA F test statistic and alternatives",
    "text": "6 ANOVA F test statistic and alternatives\nThe basic application of ANOVA in R is the aov() function. There are actually a lot of alternative ways to perform the exact same test in R.\nTo micro-digress, ANOVA is a subset of the Gaussian linear model; the Gaussian linear model is a subset of the General Linear Model; and the General Linear Model is a subset of the GeneralIZED Linear Model. For now we will forget all of that and go with aov().\n\n\n6.1 perform the ANOVA**\nWe have a few things to do here:\n\nPerform 1-way ANOVA and look at the basic output for the overall effect of sire\nLook at how to examine contrasts (differences between the control or a reference factor level, and each of the others) and post hoc testing (e.g. all pairwise comparisons between factor levels)\nExamine what happens in the ANOVA in a little more detail\n\n\n\n\n6.2 ANOVA basic output\n\n## Perform 1-way ANOVA ####\n# Try this\n\n# NB if the factor is a character, it \"should\" be coerced to a factor\n# by R, \"the passive aggressive butler\"\n# If in doubt, explicitly make the vector class == factor()\nm1 &lt;- aov(formula = Weight ~ factor(Sire), \n          data = new.long)\n\nsummary(m1)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)\nfactor(Sire)  4  17426    4356   1.872  0.137\nResiduals    35  81442    2327               \n\n\n\nThe output is formatted in a classic “ANOVA Table” style. There are 2 rows - one for the “main effect” of the sire factor, one for the residual error. The test statistic is the F value (1.87), the P-value column is named “Pr(&gt;F)” (0.14), and there are 4 degrees of freedom for this test for the factor (the factor degrees of freedom is the number of factor levels minus 1 = 5 factor levels - 1 = 4; the residual degrees of freedom is the total number of observations minus the number of factor levels = 40 - 5 = 35).\n\nHere we can see that the overall effect of sire does not significantly explain variation in weight (1-way ANOVA: F = 1.87, df = 4,35, P = 0.14).\n\n\n\n\n6.3 Contrasts and post hoc test\nAn alternative to the ANOVA table format, and possibly different to the question of an overall mean effect of the factor, is the approach for a regular linear model looking at differences for each factor level relative to a reference factor level like a control. For our experiment, let’s say that sire C is our reference level sire, against which we would like to statistically compare offspring weight for other sires.\n\n\n# Use lm() and summary() to generate contrasts\n# Use relevel() to set sire C to the reference factor level\n\n# make Sire C the reference level\nnew.long$Sire &lt;- relevel(new.long$Sire, ref=\"C\")\n\n# calculate linear model\nm2 &lt;- lm(formula = Weight ~ Sire, \n         data = new.long)\n\nsummary(m2)\n\n\nCall:\nlm(formula = Weight ~ Sire, data = new.long)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-93.625 -29.312  -2.875  33.906 104.750 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   696.63      17.05  40.846   &lt;2e-16 ***\nSireA          18.38      24.12   0.762    0.451    \nSireB         -31.50      24.12  -1.306    0.200    \nSireD         -39.13      24.12  -1.622    0.114    \nSireE         -13.38      24.12  -0.555    0.583    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 48.24 on 35 degrees of freedom\nMultiple R-squared:  0.1763,    Adjusted R-squared:  0.08211 \nF-statistic: 1.872 on 4 and 35 DF,  p-value: 0.1373\n\nplot(Weight ~ Sire, \n     data = new.long)\n\n\n\n\n\n\n\n\nR output\n&gt; m2 &lt;- lm(formula = Weight ~ Sire, \n+          data = new.long)\n&gt; summary(m2)\n\nCall:\nlm(formula = Weight ~ Sire, data = new.long)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-93.625 -29.312  -2.875  33.906 104.750 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   696.63      17.05  40.846   &lt;2e-16 ***\nSireA          18.38      24.12   0.762    0.451    \nSireB         -31.50      24.12  -1.306    0.200    \nSireD         -39.13      24.12  -1.622    0.114    \nSireE         -13.38      24.12  -0.555    0.583    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 48.24 on 35 degrees of freedom\nMultiple R-squared:  0.1763,    Adjusted R-squared:  0.08211 \nF-statistic: 1.872 on 4 and 35 DF,  p-value: 0.1373\n\nNotice how the output format has changed. This is because the summary function (and lots of functions in fact) “behave differently” in response to the specific class() of object we pass to it (here an lm object; before an aov object). One big difference we see is the table of contrasts. Now, there is are 5 rows: one for the intercept coefficient (testing whether the “grand mean” of Weight is different to zero - NB this is not at all interesting for us), and for rows comparing each factor level to the reference mean for sire “C”.\nHere, the Estimate column is an estimate of the AMOUNT of difference in weight relative to the reference mean weight for that sire. E.g., sire B offspring weight is estimated at -31.5 grams (the negative indicate less than) compared to offspring weight for sire C. However, this observed sample difference is not statistically significant (P = 0.20).\nNotice the overall F value and p-value for the 1-way are also present at the bottom of the output, which is exactly the same as that produced by the aov() function.\n\nWe can make a new boxplot based on our new sire variable and notice how it automatically moves sire C to the leftmost “reference position”\nplot(Weight ~ Sire, \n     data = new.long,\n     main = \"Sire C as reference\")\n\n\n6.4 Post hoc tests\n\nThe collection and analysis of data should be driven by the question. We should always be careful to make the distinction between data analysis that SHOULD be done versus that which merely CAN be done. The former is driven by prediction and motivated by evidence, expectation and the design of data collection. The latter is often a waste of time, or worse, a “fishing expedition” in crass pursuit of a P-value, any P-value, which is &lt; 0.05 .\n\n\n\n\n\n\nThe meaning of significance\n\n\n\n\nPost hoc tests are often interesting in research, and the 1-way ANOVA is a good example, where an overall question of “is there a difference?” can be enhanced by asking whether there are particular or specific differences, say between pairs of means. The phrase post hoc implies that the sometimes these questions can be an afterthought. Thus, consideration should be given as to whether these specific questions NEED to be asked.\n\n\n\n6.5 Type I errors\nThe alpha (\\(\\alpha\\)) value, the value to which we compare our P-value, can be interpreted as the (maximum) probability we are willing to accept of being wrong if we conclude there is significance in our data. Traditionally, this alpha value is accepted to be 0.05, or a 5% chance of making a false positive error. When multiple tests are made on the same data, it increases the chance of discovering a false positive, by chance alone, to above 5%. Thus, there are methods that are used to avoid doing this, by adjusting the P-value to keep the overall likelihood of false positive error at 5%.\nThe Bonferroni adjustment see Bland and Altman 1995 is a baseline, conservative adjustment to the alpha value to avoid false positive errors. This might be typically applied to a 1-way ANOVA situation where specific, pairwise comparisons between means are required that are not covered by the overall test, or by the contrasts. E.g. in our chicken data, we know there is not a difference in offspring weight between sire C and all other sire offsring weights: C:A, C:B, C:D, C:E, but what if we really wanted to test the others? We could use the Bonferroni adjustment as implemented in the pairwise.t.test() function.\nThe Bonferroni adjustment simply divides the alpha expected value by the number of post hoc pairwise comparisons. NB the Bonferroni adjustment is conservative, and there are alternatives. The point here is just to illustrate how these tests function and, like with many things, more study will be required to round out foundational knowledge for post hoc testing procedures. The Bonferroni test is nice to know about and understand because it is easy to use, and manually calculate, and can be applied to any situation.\n\n\n# Try this:\n\n## Bonferroni ####\n\n# ?pairwise.t.test\n\n# there are a few p.adjust.methods\n# c(\"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\",\n#   \"fdr\", \"none\")\n# we will use \"bonferroni\"\npairwise.t.test(x = new.long$Weight, \n                g = new.long$Sire,\n                p.adjust.method = \"bonferroni\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  new.long$Weight and new.long$Sire \n\n  C    A    B    D   \nA 1.00 -    -    -   \nB 1.00 0.46 -    -   \nD 1.00 0.23 1.00 -   \nE 1.00 1.00 1.00 1.00\n\nP value adjustment method: bonferroni \n\n\n\nNB the output here is a matrix of (Bonferroni adjusted!) p-values for each possible pairwise comparison (none are significant, i.e., less than 0.05). It is also possible to simply calculate uncorrected p-values and compare them to Bonferroni adjust alpha values, as described above!\n\n\n\n6.6 Tukey HSD (Honestly Significant Differences)\nThe Tukey HSD test is ideal for 1-way ANOVA, and is less conservative than the Bonferroni adjustment. We use the Tukey.HSD() function to apply it here.\n\n## Tukey Honestly Significant Differences ####\n# ?TukeyHSD\n\nTukeyHSD(m1) # NB m1 - this function requires an \"aov\" object\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Weight ~ factor(Sire), data = new.long)\n\n$`factor(Sire)`\n       diff        lwr       upr     p adj\nB-A -49.875 -119.21883  19.46883 0.2565333\nC-A -18.375  -87.71883  50.96883 0.9397600\nD-A -57.500 -126.84383  11.84383 0.1436866\nE-A -31.750 -101.09383  37.59383 0.6830523\nC-B  31.500  -37.84383 100.84383 0.6893101\nD-B  -7.625  -76.96883  61.71883 0.9977281\nE-B  18.125  -51.21883  87.46883 0.9425341\nD-C -39.125 -108.46883  30.21883 0.4937665\nE-C -13.375  -82.71883  55.96883 0.9806452\nE-D  25.750  -43.59383  95.09383 0.8216369\n\nplot(TukeyHSD(m1))\n\n\n\n\n\n\n\n\n\nNotice the table of pairwise comparisons. The format is slightly different than that for the simple pairwise.t.test() function output and there is a bit more information. Also note that the p-values tend to be smaller for the exact same comparisons, but there are sill no significant comparisons.\n\n\n\n6.7 Alternatives to 1-way ANOVA\nIn case the assumptions of 1-way ANOVA cannot be met by the data, there are a few options:\n\nAttempt to transform the data (e.g. with log(), sqrt() or other transformation) to “coerce” the data to conform to the assumptions of 1-way ANOVA\nUse an alternative test for which assumptions are not violated\n\n\nThe simplest alternative test to use for a 1-way ANOVA design would be a “non-parameteric” test that simply does not make the assumptions of Gaussian residuals or of homscedasticity. NB there are other many other methods as well (e.g. the Generalized linear model, randomization, Bayesian modelling), which we will not cover here.\nNon-parametric tests have the advantage of being very easy to use, and being very easy to interpret as they tend to be analogous to tests that require assumptions of the data (so-called “parametric tests” like the t-test, regression and 1-way ANOVA).\nA downside to these non-parametric tests is that they tend to have less statistical power. That is, they are less likely than their parametric cousins to detect a significant difference even if one does exist!\n\n\n\n6.8 Kruskal-Wallis non-parametric alternative to the 1-way ANOVA\nHere we will look at a non-parametric test that is perfect to use instead of, the Kruskal-Wallis test.\n\n# Try this:\n# ?kruskal.test\nkruskal.test(formula = Weight ~ Sire,\n             data = new.long)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Weight by Sire\nKruskal-Wallis chi-squared = 7.648, df = 4, p-value = 0.1054\n\n\n\nThe result is qualitatively the same as that for our 1-way ANOVA test, which is not surprising. Reporting the results of this test is similar as well.\n\n\nWe found no evidence of a difference in offspring weight for different sires (Kruskal-Wallis: chi-squared = 7.65, df = 4, P = 0.11).",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "12 ANOVA"
    ]
  },
  {
    "objectID": "12-1-way-anova.html#anova-calculation-details",
    "href": "12-1-way-anova.html#anova-calculation-details",
    "title": "12 ANOVA",
    "section": "7 ANOVA calculation details",
    "text": "7 ANOVA calculation details\n# Our data\nchicken.wide\n\n\n7.1 ANOVA Equations\n{width = “600px”}\n\n\n\n7.2 ANOVA Variables\n{width = “400px”}\n\n\n\n7.3 ANOVA Sources of variation table\n\n\n\nI like the cocksure argument that you never need to actually “do an ANOVA by hand” because we have computers for that sort of thing these days. Which, we do. But, I think I would make a distinction between someone who has never, or fears they cannot, calculate an ANOVA, versus someone who might occasionally do so just to understand better how the world works. I know which one I would rather be.\n\n\n- Unknown philosopher\n\n\n\n\n7.4 Do an ANOVA “by hand” programmatically\n\n## ANOVA details ####\n\n# Try this:\n# For the code below, try to follow what is going on in the code\n# It is okay if not every detail is clear yet\n# Do we get the same answer as aov()?\n\nn.groups &lt;- ncol(chicken.wide)\nn.per.group &lt;- vector(mode = \"integer\", length = ncol(chicken.wide))\n\nfor(i in 1:ncol(chicken.wide)) {n.per.group[i] &lt;- length(chicken.wide[,i])}\n\nn.individuals &lt;- ncol(chicken.wide)*nrow(chicken.wide)\ndf.between &lt;- n.groups - 1\ndf.within &lt;- n.individuals - n.groups\nmean.total &lt;- mean(as.matrix(chicken.wide))\nmean.per.group &lt;- colMeans(chicken.wide)\n\nss.between &lt;- sum(n.per.group*(mean.per.group - mean.total)^2)\n\nss.within &lt;- sum(sum((chicken.wide[,1] - mean.per.group[1])^2),\n                 sum((chicken.wide[,2] - mean.per.group[2])^2),\n                 sum((chicken.wide[,3] - mean.per.group[3])^2),\n                 sum((chicken.wide[,4] - mean.per.group[4])^2),\n                 sum((chicken.wide[,5] - mean.per.group[5])^2)\n                 )\nms.between &lt;- ss.between/df.between\nms.within &lt;- ss.within/df.within\n\n# Manual F\n(myF &lt;- ms.between/ms.within)\n\n[1] 1.872189\n\n# Anova table\nanova(m1)$\"F value\"[1] # Store-bought F\n\n[1] 1.872189",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "12 ANOVA"
    ]
  },
  {
    "objectID": "12-1-way-anova.html#practice-exercises",
    "href": "12-1-way-anova.html#practice-exercises",
    "title": "12 ANOVA",
    "section": "8 Practice exercises",
    "text": "8 Practice exercises\nFor these exercises, run the code below to recreate the data object pest. There are 40 rows and 2 variables with 2 variables: damage and treatment.\n\npest &lt;- structure(list(damage = c(113.7, 94.4, 103.6, \n                                  106.3, 104, 98.9, \n                          115.1, 99.1, 120.2, 99.4, \n                          88, 97.9, 61.1, 72.2, 73.7, \n                          81.4, 72.2, 48.4, 50.6, 88.2, \n                          46.9, 32.2, 48.3, 62.1, 69, \n                          45.7, 47.4, 32.4, 54.6, 43.6, \n                          104.6, 107, 110.4, 93.9, 105, \n                          82.8, 92.2, 91.5, 75.9, 100.4), \n                       treatment = c(\"control\", \"control\", \n                                     \"control\", \"control\",\n                                     \"control\", \"control\", \n                                     \"control\", \"control\", \n                                     \"control\", \"control\",\n                                     \"x.half\", \"x.half\", \n                                     \"x.half\", \"x.half\", \n                                     \"x.half\", \"x.half\", \n                                     \"x.half\", \"x.half\", \n                                     \"x.half\", \"x.half\", \n                                     \"x.full\", \"x.full\", \n                                     \"x.full\", \"x.full\",\n                                     \"x.full\", \"x.full\", \n                                     \"x.full\", \"x.full\", \n                                     \"x.full\", \"x.full\", \n                                     \"organic\", \"organic\", \n                                     \"organic\", \"organic\", \n                                     \"organic\", \"organic\", \n                                     \"organic\", \"organic\", \n                                     \"organic\", \"organic\")), \n                  class = \"data.frame\", \n                  row.names = c(NA, -40L))\n\n\nThink of this data as the result of an experiment looking at the effectiveness of pesticide treatment on leaf damage. Let us imagine that this experiment measured leaf damage (variable “damage” measured in mm squared) and that the plants were treated with one of 4 treatment levels:\nVariable treatment with levels\n\ncontrol\nx.half\nx.full\norganic\n\nThe experiment is of course designed to look at an overall effect the various treatments may have to reduce leaf damage relative to the control. In addition, it is of interest to examine the effect of the organic treatment compared to that of the x.half to the x.full.\nThe experiment ran using 40 potted plants spaced 1m from each other in a greenhouse setting. Each treatment was randomly assigned to 10 plants. Onto each plant was placed 5 red lily beetle (Lilioceris lilii) pairs.\n\n\n\n\n\nRed lily beetle\n\n\n\n\n\n8.1\nMake a good, appropriate graph representing the overall experiment. Show your code. Describe any trends in the data that are apparent from the graph, as well as an initial assessment of principle assumptions of 1-way ANOVA based only on your single graph.\n\n\n\n8.2\nTest the assumption of Gaussian residuals for 1-way ANOVA using any graphs or NHST approach that you deem appropriate. Show your code and briefly describe your EDA findings and conclusion as to whether these data adhere to the Gaussian assumption.\n\n\n\n8.3\nTest the assumption of homoscedasticity of residuals for 1-way ANOVA using any graphs or NHST approach that you deem appropriate. Show your code and briefly describe your EDA findings and conclusion as to whether these data adhere to the homoscedasticity assumption.\n\n\n\n8.4\nPerform either a 1-way ANOVA or an appropriate alternative based on your findings in the previous answers. Show your code, state your results in the technical style and briefly interpret your findings.\n\n\n\n8.5\nPerform an appropriate set of post hoc tests to compare pairwise mean differences in these data. Focus on the post hoc questions of interest: Is the organic pesticide effective? Does dose matter in the non-organic treatments?\n\n\n\n8.6\nWrite a plausible practice question involving any aspect of data handling, graphing or analysis for the 1-way ANOVA framework for the iris data (data(iris); help(iris)).",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "12 ANOVA"
    ]
  },
  {
    "objectID": "10-regression.html",
    "href": "10-regression.html",
    "title": "10 Regression",
    "section": "",
    "text": "We should be suspicious if the data points all fall exactly on the straight line of prediction",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "10 Regression"
    ]
  },
  {
    "objectID": "10-regression.html#regression-to-the-mean",
    "href": "10-regression.html#regression-to-the-mean",
    "title": "10 Regression",
    "section": "1 Regression to the mean",
    "text": "1 Regression to the mean\n\n“The general rule is straightforward but has surprising consequences: whenever the correlation between two scores is imperfect, there will be regression to the mean.”\n\n\n- Francis Galton\n\nOne of the most common and powerful tools in the statistical toolbox is linear regression. The concept and basic toolset was created in conjunction with investigating the heritable basis of resemblance between children and their parents (e.g. height) by Francis Galton.\nExemplary of one of the greatest traditions in science, a scientist identified a problem, created a tool to solve the problem, and then immediately shared the tool for the greater good. This is a slight digression from our purposes here, but you can learn more about it here:\n\nStigler 1989. Francis Galton’s Account of the Invention of Correlation\nStanton 2017. Galton, Pearson, and the Peas: A Brief History of Linear Regression for Statistics Instructors\n\n\n\n1.1 Objectives\n\nThe question of simple regression\nData and assumptions\nGraphing\nTest and alternatives\nPractice exercises",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "10 Regression"
    ]
  },
  {
    "objectID": "10-regression.html#the-question-of-simple-regression",
    "href": "10-regression.html#the-question-of-simple-regression",
    "title": "10 Regression",
    "section": "2 The question of simple regression",
    "text": "2 The question of simple regression\nThe essential motivation for simple linear regression is to relate the value of a numeric variable to that of another variable. There may be several objectives to the analysis:\n\nPredict the value of a variable based on the value of another\nQuantify variation observed in one variable attributable to another\nQuantify the degree of change in one variable attributable to another\nNull Hypothesis Significance Testing for aspects of these relationships",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "10 Regression"
    ]
  },
  {
    "objectID": "10-regression.html#a-few-definitions",
    "href": "10-regression.html#a-few-definitions",
    "title": "10 Regression",
    "section": "2.1 A few definitions",
    "text": "2.1 A few definitions\n\nEquation (1) is the classic linear regression model. (NB, here we make a distinction between the equation representing the statistical model, and the R formula that we will use to implement it)\n\n\\(\\alpha\\) (alpha, intercept) and \\(\\beta\\) (beta, slope) are the so-called regression parameters\ny and x are the dependent and predictor variables, respectively\n\\(\\epsilon\\) (epsilon) represents the “residual error” (basically the error not accounted for by the model)\n\n\nEquation 2 is our assumption for the residual error\n\nGaussian with a mean of 0 and a variance we estimate with our model\n\n\nEquation 3 is our sum of squares (SS) error for the residuals\n\nthe variance of residuals is the SSres/(n-2), where n is our sample size\n\n\nEquation 4 \\(\\hat\\beta\\) is our estimate of the slope\n\nEquation 4 \\(\\hat\\alpha\\) is our estimate of the intercept",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "10 Regression"
    ]
  },
  {
    "objectID": "10-regression.html#data-and-assumptions",
    "href": "10-regression.html#data-and-assumptions",
    "title": "10 Regression",
    "section": "3 Data and assumptions",
    "text": "3 Data and assumptions\nWe will explore the simple regression model in R using the Kaggle fish market dataset.\n\n\nlibrary(openxlsx)\n\n# NB your file may be in a different location to mine!\nfish &lt;- read.xlsx('data/10-fish.xlsx')\n\n\n# Download the fish data .xlsx file linked above and load it into R\n# (I named my data object \"fish\") \n# Try this:\n\nnames(fish)\n\n[1] \"Species\" \"Weight\"  \"Length1\" \"Length2\" \"Length3\" \"Height\"  \"Width\"  \n\ntable(fish$Species)\n\n\n    Bream    Parkki     Perch      Pike     Roach     Smelt Whitefish \n       35        11        56        17        20        14         6 \n\n# slice out the rows for Perch\n\nfish$Species==\"Perch\" #just a reminder\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n[133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[157] FALSE FALSE FALSE\n\nperch &lt;- fish[fish$Species==\"Perch\" , ]\nhead(perch)\n\n   Species Weight Length1 Length2 Length3 Height  Width\n73   Perch    5.9     7.5     8.4     8.8 2.1120 1.4080\n74   Perch   32.0    12.5    13.7    14.7 3.5280 1.9992\n75   Perch   40.0    13.8    15.0    16.0 3.8240 2.4320\n76   Perch   51.5    15.0    16.2    17.2 4.5924 2.6316\n77   Perch   70.0    15.7    17.4    18.5 4.5880 2.9415\n78   Perch  100.0    16.2    18.0    19.2 5.2224 3.3216\n\n\n\n\n3.1 Assumptions\nThe principle assumptions of simple linear regression are:\n\nLinear relationship between variables\nNumeric continuous data for the dependent variable (y); numeric continuous (or numeric ordinal) data on the for the predictor variable (x)\nIndependence of observations (We assume this for the different individual Perch in our data)\nGaussian distribution of residuals (NB this is not the same as assuming the raw data are Gaussian! We shall diagnose this)\nHomoscedasticity (this means the residual variance is approximately the same all along the x variable axis - we shall diagnose this)",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "10 Regression"
    ]
  },
  {
    "objectID": "10-regression.html#graphing",
    "href": "10-regression.html#graphing",
    "title": "10 Regression",
    "section": "4 Graphing",
    "text": "4 Graphing\nThe traditional way to graph the simple linear regression is with a scatterplot, with the dependent variable on the y axis and the predictor variable on the x axis. The regression equation above can be used to estimate the line of best fit for the sample data, which is predicted value of y. Thus, prediction is one of the functions here (as in predicting the value of y given a certain value of x if there were to be further data collection). This regression line is often incorporated in plots representing regression.\nThe simple regression function in R is lm() (for linear model). In order to estimate the line of best fit and the regression coefficients, we will make use of it.\n\n\n# Try this:\n# A simple regression of perch Height as the predictor variable (x)\n# and Width as the dependent (y) variable\n\n# First make a plot\nplot(y = perch$Height, x = perch$Width,\n     ylab = \"Height\", xlab = \"Width\",\n     main = \"My perch regression plot\",\n     pch = 20, col = \"blue\", cex = 1)\n\n# Does it look there is a strong linear relationship\n# (it looks very strong to me)\n\n# In order to draw on the line of best fit we must calculate the regression\n\n# ?lm \n\n# We usually would store the model output in an object\n\nmylm &lt;- lm(formula = Height ~ Width, # read y \"as a function of\" x \n           data =  perch)\nmylm # NB the intercept (0.30), and the slope (1.59)\n\n\nCall:\nlm(formula = Height ~ Width, data = perch)\n\nCoefficients:\n(Intercept)        Width  \n     0.2963       1.5942  \n\n# We use the abline() function to draw the regression line onto our plot\n# NB the \n\n# ?abline\n\nabline(reg = mylm) # Not bad\n\n# Some people like to summarize the regression equation on their plot\n# We can do that with the text() function\n# y = intercept + slope * x\n\n# ?text\n\ntext(x = 3,    # x axis placement\n     y = 11,   # y axis placement\n     labels = \"y = 0.30 + (1.59) * x\")\n\n\n\n\n\n\n\n\n\n\n4.1 Testing the assumptions\nThe data scientist must take responsibility for the assumptions of their analyses, and for validating the statistical model. A basic part of Exploratory Data Analysis (EDA) is to formally test and visualize the assumptions. We will briefly do this in a few ways.\nBefore we begin, it is important to acknowledge that this part of the analysis is subjective and it is subtle, which is to say that it is hard to perform without practice. As much as we wish that Null Hypothesis Significance Testing is totally objective, the opposite is true, and the practice of data analysis requires experience.\nHere, we will specifically test two of the assumption mentioned above, that of Gaussian residual distribution, and that of homoscedasticity. We will examine both graphically, and additionally we will formally test the assumption of Gaussian residuals.\nTo start with, let’s explicitly visualize the residuals. This is a step that might be unusual for a standard exploration of regression assumptions, but for our purposes here it will serve to be explicit about what the residuals actually are.\n\n\n## Test assumptions ####\n# Try this:\n\n# Test Gaussian residuals\n\n# Make our plot and regression line again\nplot(y = perch$Height, x = perch$Width,\n     ylab = \"Height\", xlab = \"Width\",\n     main = \"My perch RESIDUAL plot\",\n     pch = 20, col = \"blue\", cex = 1)\nabline(reg = mylm)\n\n# We can actually \"draw on\" the magnitude of residuals\narrows(x0 = perch$Width,\n       x1 = perch$Width,\n       y0 = predict(mylm), # start residual line on PREDICTED values\n       y1 = predict(mylm) + residuals(mylm), # length of residual\n       length = 0) # makes arrowhead length zero (or it looks weird here)\n\n\n\n\n\n\n\n\n\nNote the residuals are perpendicular the the x-axis. This is because residuals represent DEVIATION of each OBSERVED y from the PREDICTED y for a GIVEN x.\nThe Gaussian assumption is that relative to the regression line, the residual values should be, well, Gaussian (with mean of 0 and a variance we estimate)! There should be more dots close to the line with small distance from the regression line, and few residuals farther away\n\n\n\n\n4.2 Closer look at the residual distribution\nRemember how we visually examine distributions? With a frequency histogram and possibly a q-q plot right? Here we will do those for a peek, but we will also add a formal, objective test of deviation from normality. This part of exploratory data analysis is subtle and requires experience (i.e. it is hard), and there are many approaches. Our methods here are a starting point.\n\n\n# residual distribution\n# Try this:\n\nlibrary(car) # for qqPlot()\n\nLoading required package: carData\n\npar(mfrow = c(1,2)) # Print graphs into 1x2 grid (row,column)\n\nhist(residuals(mylm), main = \"\")\nqqPlot(residuals(mylm))\n\n\n\n\n\n\n\n\n124 118 \n 52  46 \n\npar(mfrow = c(1,1)) # Set back to 1x1\n\n\n\n\n4.3 Diagnosis - take 1\n\nThe histogram is “shaped a little funny” for Gaussian\nSlightly too many points in the middle, slightly too few between the mean and the extremes in the histogram\nVery slight right skew in the histogram\nMost points are very close to the line on the q-q plot, but there are a few at the extremes that veer off\nTwo points are tagged as outliers a little outside the error boundaries on the q-q plot (rows 118 and 124, larger than expected observations)\n\n\n\n\n4.4 Diagnosis - take 2\n\nIt is your job as a data scientist to be skeptical of data, assumptions, and conclusions. Do not pussyfoot this.\n\n\nIt is not good enough to merely make these diagnostic graphs robotically; the whole point is to judge whether the the assumptions have been violated. This is important (and remember, hard) because if the assumptions are not met it is unlikely that the dependent statistical model is valid. Here, we can look a little closer at the histogram and the expected Gaussian distribution, and we can also perform a formal statistical test to help us decide.\n\n## Gussie up the histogram ####\n\n# Make a new histogram\nhist(residuals(mylm), \n     xlim = c(-2, 2), ylim = c(0,.9),\n     main = \"\",\n     prob = T) # We want probability density this time (not frequency)\n\n# Add a density line to just help visualize \"where the data are\"\nlines(                       # lines() function\n  density(residuals(mylm)),   # density() function\n  col = \"green4\", lty = 1, lwd = 3) # Mere vanity\n\n# Make x points for theoretical Gaussian\nx &lt;- seq(-1,+1,by=0.02) \n\n# Draw on theoretical Gaussian for our residual parameters\ncurve(dnorm(x, mean = mean(residuals(mylm)),\n            sd = sd(residuals(mylm))),\n      add = T,\n      col = \"blue\", lty = 3, lwd = 3) # mere vanity\n\n# Draw on expected mean\nabline(v = 0, # vertical line at the EXPECTED resid. mean = 0\n       freq = F,\n       col = \"red\", lty = 2, lwd = 3) # mere vanity\n\nWarning in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): \"freq\" is\nnot a graphical parameter\n\n# Add legend\nlegend(x = .6, y = .9,\n       legend = c(\"Our residuals\", \"Gaussian\", \"Mean\"),\n       lty = c(1,3,2),\n       col = c(\"green4\", \"blue\",\"red\"), lwd = c(3,3,3))\n\n\n\n\n\n\n\n\n\nDiagnosis\n\nNear the mean, our residual density is slightly higher than expected under theoretical Gaussian\nBetween -0.5 and -1 and also between 0.5 and +1 our residual density is lower than expected under theoretical Gaussian\nOverall the differences are not very extreme\nThe distribution is mostly symmetrical around the mean\n\n\nFinally, let’s perform a statistical test of whether there is evidence our residuals deviate from Gaussian. There are a lot of options for this, but we will only consider one here for illustration, in the interest of brevity. We will (somewhat arbitrarily) use the Shapiro-Wilk test for Gaussian.\nSide note: Tests like this are a bit atypical within the NHST framework, in that usually when we perform a statistical test, we have a hypothesis WE BELIEVE TO BE TRUE that there is a difference (say between the regression slope and zero, or maybe between 2 means for a different test). In this typical case we are testing against the null of NO DIFFERENCE. When we perform such a test and examine the p-value, we compare the p-value to our alpha value.\n\nThe tyranny of the p-value\nThe rule we traditionally use is that we reject the null of no difference if our calculated p-value is lower than our chosen alpha (usually 0.05**). When testing assumptions of no difference we believe to be true, like here, we still typically use the 0.05 alpha threshold. In this case, when p &gt; 0.05, we can take it as a lack of evidence that there is a difference. NB this is slightly different than consituting EVIDENCE that there is NO DIFFERENCE!\n**The good old p-value is sometimes misinterpreted, or relied on “too heavily”. Read more about this important idea in Altman and Krzywinski 2017.\n\n## Shapiro test ####\n# Try this:\n  \nshapiro.test(residuals(mylm))\nR output\n\n\nReporting the test of assumptions\nThe reporting of evidence supporting claims that assumptions underlying statistical tests have been tested and are “OK”, etc., are often understated even though they are a very important part of the practice of statistics. Based on the results of our Shapiro-Wilk test, we might report our findings in this way in a report (in a Methods section), prior to reporting the results of our regression (in the Results section):\n\nWe found no evidence our assumption of Gaussian residual distribution was violated (Shapiro-Wilk: W = 0.97, n = 56, p = 0.14)\n\n\nDiagnostic plots and heteroscedasticity\nDespite being challenging to pronounce and spell heteroscedasticiy, (help pronouncing it here; strong opinion about spelling it here), the concept of heteroscedasticity is simple - the that variance of the residuals should be constant across the predicted values. We usually examine this visually, which is easy to do in R.\n## Heteroscedsticity ####\n\n# Try this:\nplot(y = residuals(mylm), x = fitted(mylm),\n     pch = 16, cex = .8) \n\n# There is a lot hidden inside our regression object\nsummary(mylm)$sigma # Voila: The residual standard error\n\n(uci &lt;- summary(mylm)$sigma*1.96) # upper 95% confidence interval\n(lci &lt;- -summary(mylm)$sigma*1.96) # upper 95% confidence interval\n\n# Add lines for mean and upper and lower 95% CI\nabline(h = c(0, uci, lci),\n       lwd = c(2,2,2),\n       lty = c(2,3,3),\n       col = c(\"blue\", \"red\", \"red\"))\n\n\nWhat we are looking for in this graph, ideally, is an even spread of residuals across the x-axis representing our fitted values. Remember, the x axis here represent perch Width, and each data point is a single observation of perch Height. The blue reference line is the mean PREDICTED perch Height for each value of Width. The difference between each data point and the horizontal line at zero is the residual difference, or residual error.\nWe are also looking for an absence of any systematic pattern in the data, that might suggest a lack of independence.\n\nWe see:\n\nThere is not a perfect spread of residual variation across the whole length of the fitted values. Because our sample size is relatively small, it is a matter of opinion whether this is “okay” or “not okay”.\nThere seem to be two groupings of values along the x-axis. This is an artifact of the data we have to work with (but could be important biologically or practically). For each of these groups, the residual spread appears similar.\nThe left hand side of the graph appears to have very low residual variance, but then there are only a few data points there and we expect most of the points to be near the line prediction anyway.\nAll things considered, one might be inclined to proceed, concluding there is no strong evidence of heteroscedasticity.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "10 Regression"
    ]
  },
  {
    "objectID": "10-regression.html#test-and-alternatives",
    "href": "10-regression.html#test-and-alternatives",
    "title": "10 Regression",
    "section": "5 Test and alternatives",
    "text": "5 Test and alternatives\nYou have examined your data and tested assumption of simple linear regression, and are happy to proceed. Let’s look at the main results of regression.\n\n## Regression results ####\n# Try this:\n\n# Full results summary\nsummary(mylm)\n\n\nCall:\nlm(formula = Height ~ Width, data = perch)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.23570 -0.28886 -0.02948  0.27910  1.55439 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.29630    0.20543   1.442    0.155    \nWidth        1.59419    0.04059  39.276   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5342 on 54 degrees of freedom\nMultiple R-squared:  0.9662,    Adjusted R-squared:  0.9656 \nF-statistic:  1543 on 1 and 54 DF,  p-value: &lt; 2.2e-16\n\n\n\nThis full results summary is important to understand (NB the summary() function will produce different output depending on the class() and kind of object passed to it).\n\nCall This is the R formula representing the simple regression statistical model\nResiduals This is summary statistics of the residuals. Nice, but typically we would go beyond this in our EDA like we did above.\nCoefficients in “ANOVA” table format. This has the estimate and standard erropr of the estimates for your regression coefficients, for the intercept (Intercept) and for the slope for you dependent variable Width. Here, the y-intercept coefficient is 0.30 and the slope is 1.59.\nThe P-values in simple regression are associated with the parameter estimates (i.e., are they different to zero). If the P-value is much less than zero, standard R output converts it to scientific notation. Here, the P-value is reported in the column called Pr(&gt;|t|). The intercept P-value is 0.16 ( which is greater than alpha = 0.05, so we conclude there is no evidence of difference to 0 for the intercept). The slope P-value is output as &lt;2e-16, which is 0.00..&lt;11 more zeros&gt;..002. We would typically report P-values less than 0.0001 as P &lt; 0.0001\nMultiple R-Squared The simple regression test statistics is typically reported as the R-squared value, which can be interpreted as the proportion of variance in the dependent variable explained by our model. This is very high for our model, 0.97 (i.e. 97% of the variation in perch Width is explained by perch Height).",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "10 Regression"
    ]
  },
  {
    "objectID": "10-regression.html#reporting-results",
    "href": "10-regression.html#reporting-results",
    "title": "10 Regression",
    "section": "6 Reporting results",
    "text": "6 Reporting results\nA typical way to report results for our regression model might be:\n\nWe found a significant linear relationship for Height predicting Weight in perch (regression: R-squared = 0.97, df = 1,54, P &lt; 0.0001).\n\nOf course, this would be accompanied by an appropriate graph if important and relevant in the context of other results.\nAs usual, reporting copied and pasted results that have not been summarized appropriately is regarded as very poor practice, even for beginning students.\n\n\n6.1 Alternatives to regression**\nThere are actually a large number of alternatives to simple linear regression in case our data do not conform to the assumptions. Some of these are quite advanced and beyond the scope of this Bootcamp (like weighted regression, or else specifically modelling the variance in some way). The most reasonable solutions to try first would be data transformation, or possibly if it were adequate to merely demonstrate a relationship between the variables, Spearman Rank correlation. A final alternative of intermediate difficulty, might be to try nonparametric regression, like implemented in Kendal-Theil-Siegel nonparametric regression.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "10 Regression"
    ]
  },
  {
    "objectID": "10-regression.html#practice-exercises",
    "href": "10-regression.html#practice-exercises",
    "title": "10 Regression",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\nFor the following exercises, we continue to use the fish dataset\n\n\n7.1\nTest whether the assumption of Gaussian residuals holds for the R formula Weight ~ Length1 for perch in the fish dataset. Describe the evidence for why or why not; show your code.\n\n\n\n7.2\nPerform the regression for Weight ~ Height for the species Bream. Assess whether the residuals fit the Gaussian assumption. Present any graphical tests or other results and your conclusion in the scientific style.\n\n\n\n7.3\nFor the analysis in #2 above present the results of your linear regression (if the residuals fit the Gaussian assumption) or a Spearman rank correlation (if they did not).\n\n\n\n7.4\nPlot perch$Weight ~ perch$Length2. The relationship is obviously not linear but curved. Devise and execute a solution to enable the use of linear regression, possibly by transforming the data. Show any relevant code and briefly explain your results and conclusions.\n\n\n\n7.5\nExplore the data for perch and describe the covariance of all of the morphological, numeric variables using all relevant means, while being as concise as possible. Show your code.\n\n\n\n7.6\nWrite a plausible practice question involving the the exploration or analysis of regression. Make use of the fish data from any species except for Perch.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "10 Regression"
    ]
  },
  {
    "objectID": "1-setup.html#bootcamp-introduction-first-day-with-r",
    "href": "1-setup.html#bootcamp-introduction-first-day-with-r",
    "title": "1 Setup & intro",
    "section": "1 Bootcamp Introduction (first day with R)",
    "text": "1 Bootcamp Introduction (first day with R)\n\nThis page is intended to guide people during their first installation and use of R and RStudio.\n\n\n1.1 Objectives\nHere is what we will work on:\n\nHow the R Stats Bootcamp works\nR motivation\nInstall R and RStudio or set up RStudio Cloud\nRStudio components and setup\nWorkflow for scripts in R\nPractice exercises",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "1 Setup & intro"
    ]
  },
  {
    "objectID": "1-setup.html#how-the-r-stats-bootcamp-works",
    "href": "1-setup.html#how-the-r-stats-bootcamp-works",
    "title": "1 Setup & intro",
    "section": "2 How the R Stats Bootcamp works",
    "text": "2 How the R Stats Bootcamp works\nThe R Stats Bootcamp aims to provide practical, open instructional materials to support learning the R programming language, to review simple statistics in R, and to introduce reproducibility and collaboration tools. The content is a blend of practical, referenced material with videos and self-assessment.\nWe also a have a friendly community Slack channel - go over and introduce yourself and say hi!",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "1 Setup & intro"
    ]
  },
  {
    "objectID": "1-setup.html#r-motivation",
    "href": "1-setup.html#r-motivation",
    "title": "1 Setup & intro",
    "section": "3 R motivation",
    "text": "3 R motivation\nThe motivation for using R is that it is designed to help people with no programming experience to perform sophisticated statistical analysis with minimum effort. R has grown in popularity recently and is used extensively by universities, companies, and researchers everywhere. Because of this, there is a very large community of users and a demand in business and academia for skills using R.\nR is free and open source. R is easy to learn and works the same for folks with fast and slow computers, no matter what kind of operating system or computer they like to use, and it is easy to use via the web on any device.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "1 Setup & intro"
    ]
  },
  {
    "objectID": "1-setup.html#install-r-and-rstudio-or-set-up-rstudio-cloud",
    "href": "1-setup.html#install-r-and-rstudio-or-set-up-rstudio-cloud",
    "title": "1 Setup & intro",
    "section": "4 Install R and RStudio or set up RStudio Cloud",
    "text": "4 Install R and RStudio or set up RStudio Cloud\nYou have two options for following along with these materials as they are intended.\n\nOption 1 Download and install R from CRAN and then download and install RStudio desktop.\nInstall R first, then RStudio. It is probably a good idea to go ahead and install the latest version of each if you have older versions installed. If you have a PC or laptop you regularly use, this option is probably best and will work for almost all hardware and operating systems.\nHelp for Windows\nHelp for Macs\nHelp for Linux\n\nOption 2 If you can’t install R or do not wish to, or if you prefer to work in “the cloud” from a web browser, you may wish to start a free account at RStudio Cloud and follow along that way.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "1 Setup & intro"
    ]
  },
  {
    "objectID": "1-setup.html#rstudio-components-and-setup",
    "href": "1-setup.html#rstudio-components-and-setup",
    "title": "1 Setup & intro",
    "section": "5 RStudio components and setup",
    "text": "5 RStudio components and setup\nRStudio desktop is an environment to write R code, perform statistical analysis, organize big or small projects with multiple files, and view and organize outputs. There are many features of RStudio, but we are only going to point out a few. One of the most useful features is syntax highlighting, that gives visual cues to help you write computer code.\n\n\n\n\nRStudio layout\n\n\n\nBe aware (beware?) of:\nThe Script window\nThe script window is located in the upper left of the RStudio console by default. You may need to open a script or start a new one: File &gt; New File &gt; R Script (hotkey Ctrl+Shift+N).\nThe script window is where you are likely to spend most of your time building scripts and executing commands you write. You can have many scripts open at the same time (in “tabs”), and you can have different kinds of scripts, e.g., for different parts of a project or even for programming languages.\n\nThe Console window\nThe Console window is in the lower left by default. Notice there are several other tabs visible, but we will only mention the Console for now. The Console is the place where text outputs will be printed (e.g. the results of statistical tests), and also is a place where R will print Warning and Error messages.\n\nThe Global Environment\nThe Global Environment is in the Environment tab in the upper right of RStudio by default. This pane is useful in displaying data objects that you have loaded and available.\n\nThe Plots window\nThe Plots window is a tab in the lower right by default. This is the place where graphics output is displayed and where plots can be named, resized, copied and saved. There are some other important tabs here as well, which you can also explore. When a new plot is produced, the Plots tab will become active.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "1 Setup & intro"
    ]
  },
  {
    "objectID": "1-setup.html#workflow-for-r-scripts",
    "href": "1-setup.html#workflow-for-r-scripts",
    "title": "1 Setup & intro",
    "section": "6 Workflow for R scripts",
    "text": "6 Workflow for R scripts\nScript setup\nAn R script is a plain text file where the file name ends in “dot R” (.R) by default.\nAn R script serves several purposes:\nFirst, it documents your analysis allowing it to be reproduced exactly by yourself (your future self!) or by others like collaborators, friends, colleagues, your professor, your student, etc.\nSecond, it is the interface between your commands and R software.\nA goal is that your scripts should contain only important R commands and information, in an organized and logical way that has meaning for other people, maybe for people you have never spoken to. A typical way to achieve this is to organize every script according to the same plan.\n\n\nYour R script should be a file good enough to show to a person in the future (like a supervisor, or even your future self). Someone who can help you, but also someone who you may not be able to explain the contents to. The script should be documented and complete. Think of this future person as a friend you respect.\n\n\nAlthough there are many ways to achieve this, for the purposes of the Bootcamp we strongly encourage you to organize you scripts like this:\n\nHeader\nContents\nOne separate section for each item of contents\n\n\nHeader\nStart every script with a Header, that contains your name, the date of the most recent edit, and a short description of the PURPOSE of the script.\n# A typical script Header\n\n## HEADER ####\n## Who: &lt;your name&gt;\n## What: My first script\n## Last edited: yyyy-mm-dd (ISO 8601 date format... Google it!)\n####\n\n\n6.1 Contents\nA Contents section should also be present near the top, to provide a road map for the analysis.\n# A typical script Contents section\n\n## CONTENTS ####\n## 00 Setup\n## 01 Graphs\n## 02 Analysis\n## 03 Etc\n\n\n\n6.2 Section for each item of contents\nFinally, code chunk breaks should be used to aid the readability of the script and to provide a section for each item in your table of contents. A code chunk is just a section of code set off from other sections.\nBelow is the beginning of a typical code chunk in an R script.\n\nCode chunks must start with at least one hash sign “#”,\nshould have a title descriptive of code chunk contents,\nand end with (at least) four hash signs “####”\nconsecutively numbered titles makes things very tidy\n\n## 01 This here is the first line of MY CODE CHUNK ####\n\nWe will practice each of these components.\n\n\n\n6.3 Comments\nComments are messages that explain code in your script, and they should be used throughout every script. You can think of comments like the methods section of a scientific paper - there should be enough detail to exactly replicate and understand the script, but it should also be concise.\nComment lines begin with the # character and are not treated as “code” by R.\n\n# Make a vector of numbers &lt;--- a comment\nmy_variable &lt;- c(2,5,3,6,3,4,7)\n\n# Calculate the mean of my_variable &lt;--- another comment\nmean(my_variable)\n\n[1] 4.285714\n\n\n\n\n\n6.4 Submitting commands\nA final thing that must be mentioned here is how to actually submit commands in your R script for R to execute. There are a few ways to do this.\n\nrun the whole line of code your cursor rests on (no selection) Ctrl+Enter (Cmd+Return in Macs)\nrun code you have selected with your cursor Ctrl+Enter (Cmd+Return in Macs).\nUse the “Run” button along the top of the Script window\nRun code from the menu Code &gt; Run Selected Line(s).",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "1 Setup & intro"
    ]
  },
  {
    "objectID": "1-setup.html#practice-exercises",
    "href": "1-setup.html#practice-exercises",
    "title": "1 Setup & intro",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\n\n7.1\nDownload this script and open it with RStudio. Save the script in a specific folder on your computer that you can find again and where you will save other scripts for the Bootcamp.\nRead the script comments and examine the structure of the code chunks. Run the code in the script using one of the methods above, and examine the output in the Console window.\n\n\n\n7.2\nAdd a code chunk title to your CONTENTS section and to your script. Make sure to write brief comments for your code. Add the following code to your chunk run it and examine the output:\nDon’t worry about understanding the code for now. We are just working on interfacing with R and submitting commands.\n# Create a new variable\nmy_variable &lt;- c(6.5, 1.35, 3.5)\n\n# Calculate the mean of my_variable\nmean(my_variable)\n\n# Calculate the standard deviation of my_variable\nsd(my_variable)",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "1 Setup & intro"
    ]
  },
  {
    "objectID": "11-t-test.html",
    "href": "11-t-test.html",
    "title": "11 T-test",
    "section": "",
    "text": "Measure and compare",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "11 T-test"
    ]
  },
  {
    "objectID": "11-t-test.html#students-t-test",
    "href": "11-t-test.html#students-t-test",
    "title": "11 T-test",
    "section": "1 Student’s T-test",
    "text": "1 Student’s T-test\n\nThe t-test and t-distribution are widely considered to be at the very foundation of modern statistic science and they form an important foundation for the practice of statistics. Who would believe they were invented to make great beer better?\n\nWilliam Sealy Gosset is credited with inventing and applying the idea of the t-test to assist in scientific quality control while working for the Guinness brewery. The idea was refined and supported by the great statistician R. A. Fisher, and the idea was initially described in a paper anonymously by “Student”, in order to protect the commercial interests of Guinness. Today, it is perhaps one of the most prevalent and basic tools in statistics, and it is a fascinating story.\nHere, we will briefly look at the practical basis of the t-test before going on the look at different ways it can be applied to data.\n\n\n1.1 Objectives\n\nThe question of the t-test\nData and assumptions\nGraphing\nTest and alternatives\nPractice exercises",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "11 T-test"
    ]
  },
  {
    "objectID": "11-t-test.html#the-question-of-the-t-test",
    "href": "11-t-test.html#the-question-of-the-t-test",
    "title": "11 T-test",
    "section": "2 The question of the t-test",
    "text": "2 The question of the t-test\nThere are three common versions of the t-test.\nThe typical premise of the t-test is that it is used to compare populations you are interested in, which you measure with independent samples. There are a few versions of the basic question.\n\n2.1 2 independent samples\nHere you have measured a numeric variable and have two samples. The question is: are the means of the two samples different (i.e. did the samples come from different populations)? A typical design here might be an experiment with a control and one treatment group. Another more general design might be a sample taken under one defined condition, is compared to a sample taken under a different condition.\nThe data could be summarized in two different ways. The “long format” way would be to have a vector with the measured variable, and another vector that is a factor with 2 levels defining the two sample conditions (one numeric vector, one factor vector).\n\n## 2 sample t-test long format data\n\ndensity &lt;- c(rep('high', 7), rep('low', 7))\nheight &lt;- c(2,3,4,3,4,3,2,\n            6,8,6,9,7,8,7)\n\n(long.data &lt;- data.frame(density,height))\n\n   density height\n1     high      2\n2     high      3\n3     high      4\n4     high      3\n5     high      4\n6     high      3\n7     high      2\n8      low      6\n9      low      8\n10     low      6\n11     low      9\n12     low      7\n13     low      8\n14     low      7\n\n\n\nThe “wide format” way would be to have a different numeric column for each of the samples (2 numeric vectors).\n\n## 2 sample t-test wide format data\n\n(wide.data &lt;- data.frame(high.ht = c(2,3,4,3,4,3,2),\n                         low.ht = c(6,8,6,9,7,8,7)))\n\n  high.ht low.ht\n1       2      6\n2       3      8\n3       4      6\n4       3      9\n5       4      7\n6       3      8\n7       2      7\n\n\n\nA typical graph representing data like this, would be a boxplot(). Optionally, to be maximally informative, one can add the raw data as points over the box summaries.\n\n# Boxplot\nboxplot(height ~ density, data = long.data,\n        main = \"2 independent samples\")\n\n# Optional: add raw data points\n# jitter() nudges the x-axis placement so that the points do not overlap\nset.seed(42)\npoints(x = jitter(c(1,1,1,1,1,1,1,\n                    2,2,2,2,2,2,2), \n                  amount = .2),\n       y = long.data$height,\n       col = \"red\", pch = 16, cex = .8) # Mere vanity\n\n\n\n\n\n\n\n\n\n\n\n2.2 Compare 1 sample to a known mean\nHere you have one sample which you wish to compare to a mean value. The basic question is did the sample come from a population exhibiting the known mean?\nThe data are simply a single numeric vector, and the population mean for comparison.`\n\n\n# The data\nmysam &lt;- c(2.06, 1.77, 1.9, 1.94, 1.91, 1.83, \n           2.08, 1.84, 2.15, 1.84, \n           2.05, 2.19, 1.64, 1.81, 1.83)\n\nboxplot(mysam, \n         main = \"Is your sample population different from the dashed line?\")\n\npoints(x = jitter(rep(1,15), amount = .1),\n        y = mysam,\n        col = \"red\", pch = 16, cex = .8) # Mere vanity\n\nabline(h = 2.0,\n        col = \"blue\", lty = 2, lwd = 2)  # Mere vanity\n\n\n\n\n\n\n\n\n\n\n\n2.3 Paired samples\nThis is a third kind of t-test question. Here the individual observation comprising the 2 samples are not independent. A typical example might be the measurement of some variable before and after a treatment (e.g. measure crop yield in plots in a field before and after a soil treatment in successive years); another classic example would be measuring plots that are paired spatially (e.g., plots are chosen and within each plot a treatment and untreated measurement is made.).\nFor each of these examples, there is a unit, patient, or plot identification, that represents the relationship of each paired measure.\n\n\n\n2.4 Plotting paired samples\n\n# Biochar application, measure N before and after\n\n# Data \n# (the code are kind of ugly, but run it to \"make\" biochar)\nbiochar &lt;- structure(list(\n  plot = c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \n           \"J\", \"K\", \"L\", \"M\", \"N\", \"O\"), \n  N.first = c(13.4, 16.7, 17.9, 18.5, 18.6, 18.6, 18.7, \n              20.5, 20.6, 21.5, 24.2, 24.5, 25, 27.1, 28.1), \n  N.second = c(16, 16.7, 18.7, 18.7, 22.1, 22.7, 23.1, \n               23.1, 23.2, 23.5, 25.4, 25.9, 27.6, 28, 29.7)), \n  class = \"data.frame\", \n  row.names = c(NA, -15L))\n\nbiochar\n\n   plot N.first N.second\n1     A    13.4     16.0\n2     B    16.7     16.7\n3     C    17.9     18.7\n4     D    18.5     18.7\n5     E    18.6     22.1\n6     F    18.6     22.7\n7     G    18.7     23.1\n8     H    20.5     23.1\n9     I    20.6     23.2\n10    J    21.5     23.5\n11    K    24.2     25.4\n12    L    24.5     25.9\n13    M    25.0     27.6\n14    N    27.1     28.0\n15    O    28.1     29.7\n\n# boxplot() would work, but hides pairwise relationship\n# Try this:\n\nplot(x = jitter(c(rep(1,15), rep(2,15)),amount = .02),\n     y = c(biochar$N.first, biochar$N.second),\n     xaxt = \"n\", xlim = c(0.5, 2.5),\n     cex = .8, col = \"blue\", pch = 16,  # Mere vanity\n     xlab = \"Biochar treatment\",\n     ylab = \"Soil N\",\n     main = \"Do the lines tend to increase?\")\n\nmtext(side = 1, at = 1:2, text = c(\"before\", \"after\"), line = 1)\n\n# Get crazy: add horizontal lines to visualize the plot pairs\nfor(i in 1:15){\nlines(x = c(1.05,1.95),\n      y = c(biochar$N.first[i], biochar$N.second[i]),\n      lty = 2, lwd = 1, col = \"red\") # Mere vanity\n}",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "11 T-test"
    ]
  },
  {
    "objectID": "11-t-test.html#data-and-assumptions",
    "href": "11-t-test.html#data-and-assumptions",
    "title": "11 T-test",
    "section": "3 Data and assumptions",
    "text": "3 Data and assumptions\nThe principle assumptions of the t-test are:\n\nGaussian distribution of observations WITHIN each sample\nHeteroscedasticity (our old friend) - i.e., the variance is equal in each sample\nIndependence of observations\n\n\n\n3.1 Evaluating and testing the assumptions\nThe t-test is thought to be somewhat robust to violation of assumptions. For example, if the assumptions of Gaussian distribution and heteroscedasticiy are violated (a little), it is not likely to greatly bias your results. The assumption of independence of observations is always of high importance.\n\nIf in doubt about using your personal subjective judgement, based on your personal data analysis experience, it is always best to fully evaluate the evidence whether assumptions have been violated for parametric statistics tests with formal testing.\n\n\n\n\n3.2 Gaussian distribution of observations WITHIN each sample\nWe would typically first test the assumption of Gaussian distribution using graphical evaluation with, for examples, a histogram (hist()) and a q-q plot (e.g., with qqplot()), possibly along with a statistical test evaluating whether data are Gaussian (e.g., shapiro.test().\nNB 1 the assumption of Gaussian does not apply to to ALL OBSERVATIONS TOGETHER for both samples of a two sample t-test, but applies TO EACH SAMPLE SEPARATELY (this is sometimes confusing for beginners). The reason for this is that the very nature of the two sample t-test hypothesizes that the two samples come from DIFFERENT POPULATIONS, but that each population adheres to the Gaussian distribution.\nNB 2 testing whether each sample is Gaussian is analogous to testing whether the residuals are Gaussian for regression.\nThe following code explores the ideas here further.\n\n# First make some data as an example.\n\n# This is height in cm data measured from 30 females and 30 males\n# NB because we simulate this data, we literally specify the \n# samples are indeed from DIFFERENT, GAUSSIAN populations.\n# Obviously in a real sampling situation you would not know this,\n# hence we will test it formally.\n\nset.seed(1)\nheight &lt;- c(rnorm(30,185,10),\n            rnorm(30,150,10))\nsex &lt;- c('m','m','m','m','m','m','m','m','m','m',\n         'm','m','m','m','m','m','m','m','m','m',\n         'm','m','m','m','m','m','m','m','m','m',\n         'f','f','f','f','f','f','f','f','f','f',\n         'f','f','f','f','f','f','f','f','f','f',\n         'f','f','f','f','f','f','f','f','f','f')\n         \n# Package the variables we made into a data frame\n# and remove the variables outside the dataframe to \n# keep our global environment tidy (not required, but satisfying!)\n\ndata &lt;- data.frame(height,sex)\nrm(height, sex)\n\n# plot raw data\nhist(x = data$height, \n     main = 'Height ignoring sex',\n     xlab = 'Height (cm)',\n     freq = F)\n     \n# Draw expected Gaussian\n\n\n# Draw overall mean and the means\n# for each level of sex\n\nabline(v = c(mean(data$height),                 # mean overall\n             mean(data$height[data$sex=='f']),  # mean f\n             mean(data$height[data$sex=='m'])), # mean m\n       lty = c(1,2,2),                          # line types for each\n       lwd = 3,                                 # line width for all 3\n       col = c(\"black\", \"red\", \"blue\"))         # colour for each\n\n# Draw expected Gaussian density curve\nmv &lt;- data$height                               # generalizes following code\n\nxcoord &lt;- seq(min(mv),                          # make x coordinates\n              max(mv),\n              length = length(mv))\nycoord &lt;- dnorm(x = xcoord,                     # make y coordinates\n               mean = mean(mv),\n               sd = sd(mv))\n\nlines(x = xcoord, y = ycoord,                  # draw curve\n      col = 'darkgreen', lwd = 3)\n      \n# for clarity here, add on some labels...\ntext(x = 155, y = .01, labels = \"f\\nmean\", col = \"red\")\ntext(x = 182, y = .011, labels = \"m\\nmean\", col = \"blue\")\ntext(x = 172.1, y = .015, labels = \"grand\\nmean\")\ntext(x = 140, y = .012, labels = \"Gaussian\\ndensity\", col = \"darkgreen\")\n\n\n\n\n\n\n\n\n\nNote the gray bars on the histogram have two peaks, and is terribly non-Gaussian looking. Also note the data does not look similar to Gaussian! To emphasize the point, we do not even expect these height measures to come from the same population (in fact our hypothesis is that they do not), therefore we do not expect the WHOLE vector of data to be Gaussian.\nWe could further formally test this of course, with a quantile-quantile (“qq”) plot, and perhaps a statistical test of Gaussian like the Shapiro Test.\n\n\nqqnorm(data$height,\n        main = \"Q-Q Gaussian line for our Height data\")\nqqline(data$height)\n\n\n\n\n\n\n\n\n\nNotice the q-q plot shows divergences from the Gaussian expected line at both ends and in the middle! Compare this to our histogram and expected density curve above.\n\nThe Shapiro Test will confirm that these data are not Gaussian.\n\nshapiro.test(data$height)\n\n\n    Shapiro-Wilk normality test\n\ndata:  data$height\nW = 0.91918, p-value = 0.0007111\n\n\n\nSo testing formally, we find that our data are different to the expected Gaussian (W = 0.92, n = 60, p = 0.0007).\n\n\n\n3.3 Properly testing Gaussian for the two sample t-test\nTo properly test the assumption of Gaussian for a 2 sample t-test, you would test the assumption separately for each group. Here is an example that uses graphs and the Shapiro test:\n\n# step 1 graph a hist() of the continuous variable\n# separately for each factor level\n\npar(mfrow = c(2,1))                                # set so hist's \"stack\"\n\n# draw males hist\nhist(data$height[data$sex == 'm'],                 # select males\n     xlim = c(min(data$height), max(data$height)), # set limit for ALL data\n     col = \"blue\", freq = F,\n     xlab = \"Height (cm)\", main = \"Males\") \n\nmv &lt;- data$height[data$sex == 'm']\nxcoord &lt;- seq(min(mv),                          # make x coordinates\n              max(mv),\n              length = length(mv))\nycoord &lt;- dnorm(x = xcoord,                     # make y coordinates\n               mean = mean(mv),\n               sd = sd(mv))\n\nlines(x = xcoord, y = ycoord,                  # draw curve\n      col = 'lightblue', lwd = 3)\n\n# draw females hist\nhist(data$height[data$sex == 'f'],                 # select females\n     xlim = c(min(data$height), max(data$height)), # set limit for ALL data\n     col = \"red\", freq = F,\n     xlab = \"Height (cm)\", main = \"Females\")\n\nmv &lt;- data$height[data$sex == 'f']\nxcoord &lt;- seq(min(mv),                          # make x coordinates\n              max(mv),\n\n              length = length(mv))\nycoord &lt;- dnorm(x = xcoord,                     # make y coordinates\n               mean = mean(mv),\n               sd = sd(mv))\nlines(x = xcoord, y = ycoord,                   # draw curve\n      col = 'pink', lwd = 3)\n\n\n\n\n\n\n\n# set default layout back\npar(mfrow = c(1,1)) \n\n\nYou can really see the difference here and each respective population seems to conform relatively closely to Gaussian (allowing for sampling error). This is EXACTLY what we would expect for morphological data.\nNow we can make q-q plots and formally test the distributions with the Shapiro Test.\nFirst we graphically examine the distributions:\n\n\n## QQ Plots\n\npar(mfrow = c(1,2))\n\n# males\nqqnorm(data$height[data$sex == 'm'],\n        main = \"Q-Q Gaussian line for male height\")\nqqline(data$height[data$sex == 'm'])\n\n# females\nqqnorm(data$height[data$sex == 'f'],\n        main = \"Q-Q Gaussian line for female height\")\nqqline(data$height[data$sex == 'f'])\n\n\n\n\n\n\n\npar(mfrow = c(1,1))\n\n\nSecond, we evaluate whether the distribution deviate from Gaussian with an objective test.\nThe q-q plots look much better for each sex separately for the height data - i.e., most of the points are near the expected lines.\n\n\n## Shapiro Tests\n\nshapiro.test(data$height[data$sex == 'm'])\n\n\n    Shapiro-Wilk normality test\n\ndata:  data$height[data$sex == \"m\"]\nW = 0.95011, p-value = 0.1703\n\nshapiro.test(data$height[data$sex == 'f'])\n\n\n    Shapiro-Wilk normality test\n\ndata:  data$height[data$sex == \"f\"]\nW = 0.98568, p-value = 0.9482\n\n\n\nThe Shapiro test for Gaussian showed that neither sample of height data for each respective sex deviates significantly from Gaussian (males: W = 0.95, n = 30, p = 0.17; females: W = 0.99, n = 30, p = 0.95).\n\nIf the Gaussian assumption cannot be met reasonably with the data, a common alternative that does not require it is the Mann-Whitney U-test, also known by the name Wilcoxon Test (which we will look at below).\n\n\n\n3.4 Heteroscedasticity assumption\nWe would typically examine the variance graphically or through calculation and comparison of descriptive statistics, although a formal test (e.g. using var.test()) is possible. However, in the case of the 2 sample t-test, there exist methods to “pool” the standard deviation if variances are not equal. Thus, if pooled SD is used and the 2 sample t-test is conducted assuming unequal variances (NB this is the default setting in the base R t.test()), it is not necessary to test this assumption for the specific case of the 2 sample t-test (but still may be interesting to be aware of as a feature of your data).\n\n\n\n3.5 Independence assumption\nThe assumption of independence of data is extremely important and related to making an INFERENCE on a POPULATION of interest via SAMPLING one or more populations of interest. If for example pairs of observations are not independent, an alternative test would be appropriate, like the Paired t-test.\nFurther information about the t-test and assumptions can be found in John MacDonald’s excellent online Biostats Handbook.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "11 T-test"
    ]
  },
  {
    "objectID": "11-t-test.html#graphing",
    "href": "11-t-test.html#graphing",
    "title": "11 T-test",
    "section": "4 Graphing",
    "text": "4 Graphing\nWe have already examined the most common cases for data, data arrangement and types of questions that fit the t-test. The principle graph types are simple:\n\n2 independent samples - Boxplot or similar graph, showing the central tendency of the data and making it easy to visually compare the two samples.\n1 sample - Again, boxplot or similar showing the variation of the single sample. Indicating the population mean as a reference is useful.\n2 paired samples - A boxplot is second best to a graph that indicates the tendency for change between the paired observations.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "11 T-test"
    ]
  },
  {
    "objectID": "11-t-test.html#examples-of-the-t-test-and-alternatives",
    "href": "11-t-test.html#examples-of-the-t-test-and-alternatives",
    "title": "11 T-test",
    "section": "5 Examples of the t-test and alternatives",
    "text": "5 Examples of the t-test and alternatives\nWe will cover four examples here looking at the t-test variants and alternatives when assumptions are not met:\n\nt-test for 2 independent samples\n1 sample t-test\n2 paired samples\nMann-Whitney U-test\n\n\n\n5.1 t-test for 2 independent samples\nThe example we will use here is the amount of tree growth over a period of time, where samples were taken of individual trees grown under two conditions - high density of trees versus low density. The hypothesis we are testing is whether there is evidence the samples came from different populations (by inference, we are possibly interested in whether there is an effect of density on growth). The data we will use is similar to the long.data from above.\n\n# 2-sample t-test\n# Try this\n# data\ndensity &lt;- c(\"high\",\"high\",\"high\",\"high\",\"high\",\"high\",\"high\",\n             \"low\",\"low\",\"low\",\"low\",\"low\",\"low\",\"low\")\nheight &lt;- c(2.1,3.5,4.3,3.2,4.5,3.7,2.7, \n            6.1,8,6.9,9.1,7.5,8,7.4)\n(treegrowth &lt;- data.frame(density,height))\n\n   density height\n1     high    2.1\n2     high    3.5\n3     high    4.3\n4     high    3.2\n5     high    4.5\n6     high    3.7\n7     high    2.7\n8      low    6.1\n9      low    8.0\n10     low    6.9\n11     low    9.1\n12     low    7.5\n13     low    8.0\n14     low    7.4\n\n# There is not much data to compare to the Gaussian distribution\nlibrary(car) # for qqPlot()\n\nLoading required package: carData\n\nhist(treegrowth$height[treegrowth$density == \"low\"])\n\n\n\n\n\n\n\nqqPlot(treegrowth$height[treegrowth$density == \"low\"])\n\n\n\n\n\n\n\n\n[1] 4 1\n\nhist(treegrowth$height[treegrowth$density == \"high\"])\n\n\n\n\n\n\n\nqqPlot(treegrowth$height[treegrowth$density == \"high\"])\n\n\n\n\n\n\n\n\n[1] 1 5\n\n# The histograms are a little \"wooly\", but there are no huge \n# deviations from the expectation of Gaussian and the \n# q-q plots look ok: proceed\n\n# ?t.test\n\n# NB 1 - the x argument can be a formula\n# x = height ~ density\n# or we can set our samples to x and y respectively\n# x = height[low], y = height[high]\n\nt.test(formula = height ~ density, \n       data = treegrowth)\n\n\n    Welch Two Sample t-test\n\ndata:  height by density\nt = -8.6257, df = 11.868, p-value = 1.865e-06\nalternative hypothesis: true difference in means between group high and group low is not equal to 0\n95 percent confidence interval:\n -5.190611 -3.095103\nsample estimates:\nmean in group high  mean in group low \n          3.428571           7.571429 \n\n# Flash challenge: Make a good graph of the data       \n\n\nThere are a few points in the output.\n\nThe main test statistic is the “t value”, which can be be positive or negative (depending on which sample is larger on average); the absolute value of t should increase with the probability that the samples came different populations.\nThe degrees of freedom, df, is adjusted to account for differences in sample variance thought of as a way to quantify the overall difference\nThe 95% confidence interval is an estimation of the true mean difference between populations from which the samples were taken.\nThe P-value\n\nHere, we might report our results in the technical style as follows (remember ALWAYS the test performed: the test statistic, the degrees of freedom or sample size, the P-Value):\n\nWe detected a significant difference between mean height for tree grown at high or low density (2-sample t-test: t = -8.63, df = 11.9, P &lt; 0.0001).\n\n\n\n\n5.2 1 sample t-test\nThe example is a sample of earwigs measured in length to the nearest 0.1 mm. There is a hypothesis that global warming has impacted the development in the population and they are getting larger. A long term dataset has established a mean earwig length of 17.0 (NB, this is our estimate of “mu”, \\(\\mu\\), the population mean we will compare our sample to). Are the earwigs getting bigger?\n\n# Try this:\n\n# Data\nearwigs &lt;- c(22.1, 16.3, 19.1, 19.9, 19.2, 17.7, 22.5, 17.7, 24.1, 17.8, \n21.9, 24.9, 13.8, 17.2, 17.6, 19.9, 17.1, 10, 10.7, 22)\n\n# Flash Challenge: Assess this data for adherence to the Gaussian assumption\n\nmymu &lt;- 17.0 # Our mu\n\n# ?t.test #notice the mu argument\n\nt.test(x = earwigs,\n       mu = mymu)\n\n\n    One Sample t-test\n\ndata:  earwigs\nt = 1.7845, df = 19, p-value = 0.09031\nalternative hypothesis: true mean is not equal to 17\n95 percent confidence interval:\n 16.72775 20.42225\nsample estimates:\nmean of x \n   18.575 \n\n# Flash challenge: Make a great graph representing this test       \n\n\nWe found no evidence the mean length of earwigs in our sample was different to the historical mean (1-sample t-test: t = 1.78, df = 19, P-value = 0.09).\n\n\n\n\n5.3 2 paired samples\nThe example here is a measure of the hormone cortisol in pregnant cows (related to stress in mammals). A measure was taken in each individual twice; once as a baseline measure, and once after a treatment of soothing music being played to the cows for 3 hours per day. The prediction is that the mean level of cortisol will decrease relative to baseline after experiencing the music treatment.\n\n# Data\n# Try this:\n# Data\ncort.t0 &lt;- c(0.59, 0.68, 0.74, 0.86, 0.54, 0.85, 0.7, 0.81, 0.79, 0.76, \n             0.49, 0.64, 0.74, 0.51, 0.57, 0.74, 0.77, 0.72, 0.52, 0.49)\n\ncort.t1 &lt;- c(1.13, 0.81, 0.77, 0.72, 0.45, 0.9, 0.7, 0.7, 0.98, 0.96, 1.1, \n             0.63, 0.91, 1.1, 0.99, 0.72, 1.11, 1.2, 0.77, 0.91)\n\n# ?t.test # NB the \"paired\" argument\n\nt.test(x = cort.t0,\n       y = cort.t1,\n       paired = TRUE)\n\n\n    Paired t-test\n\ndata:  cort.t0 and cort.t1\nt = -3.7324, df = 19, p-value = 0.001412\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.31605728 -0.08894272\nsample estimates:\nmean difference \n        -0.2025 \n\n# Flash Challenge:\n# 1) Make a great graph that represents these data\n# 2) do the data conform to the assumptions?\n# 3) Was your hypothesis upheld...?\n# 4) format and report the results in the technical style!\n\n\n\n\n5.4 Mann-Whitney U-test\nThe Mann-Whitney U-test (wilcox.test() in R, don’t ask why…) is an alternative to the t-test when your data cannot achieve the assumptions for the t-test. The t-test is robust, especially when sample size is large, or the deviation from assumptions is similar for both samples. However, when sample size is not very large (e.g. ~30 per sample or less), and there is skew or the samples are dissimilar, the Mann-Whitney U-test is a good choice. Two sample and one sample methods exist.\nThe example here is chicken egg count for a control, and a +bonemeal diet.\n\n## **Mann-Whitney U-test** ####\ndiet &lt;- c(3, 3, 1, 2, 2, 2, 2, 0, 2, 2, 1, 2, 3, 1, 1)\n\ndiet.bone &lt;- c(5, 6, 1, 2, 3, 5, 1, 7, 5, 1, 2, 2, 5, 2, 4)\n\n\n# Gaussian assumption\nlibrary(car)\nhist(diet)\n\n\n\n\n\n\n\nhist(diet.bone) # Dist not similar to diet\n\n\n\n\n\n\n\npar(mfrow = c(1,2))\nqqPlot(diet, \n       main = \"Not Gaussian\") # Divergence\n\n[1] 8 1\n\nqqPlot(diet.bone,\n       main = \"Diff. to diet?\") \n\n\n\n\n\n\n\n\n[1] 8 2\n\npar(mfrow = c(1,1))\n\n# ?wilcox.test\nwilcox.test(x = diet, y = diet.bone)\n\nWarning in wilcox.test.default(x = diet, y = diet.bone): cannot compute exact\np-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  diet and diet.bone\nW = 63.5, p-value = 0.0374\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nDon’t forget to make a good graph of the data.\n\nboxplot(diet, diet.bone, ylab = 'Egg count')\npoints(x=jitter(rep(1,15), 4), y = diet, \n       col = 'red', pch = 16) # pure vanity\npoints(x=jitter(rep(2,15), 2), y = diet.bone, \n       col = 'red', pch = 16) # pure vanity\n\n\n\n\n\n\n\n# Flash challenge: make this graph better!\n\n\n\nWe found evidence of a difference in the number of eggs lain under a control diet and a a diet supplemeted with bone meal (Mann-Whitney U-test: W = 63.5, n = [15, 15], P = 0.037) )",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "11 T-test"
    ]
  },
  {
    "objectID": "11-t-test.html#practice-exercises",
    "href": "11-t-test.html#practice-exercises",
    "title": "11 T-test",
    "section": "6 Practice exercises",
    "text": "6 Practice exercises\nFor the following exercises, use the dataset in the file 11-Davis.xlsx. The dataset is in tidy format; take advantage of this by looking at the terse information present in the data dictionary tab. The data are a result of a survey of some university students, who were asked to report their height and weight, and then their height and weight were measured. There will be some data handling as part of the exercises below, and practical and important part of every real data analysis.\n\n\n6.1\nPick the appropriate form of t-test to ask whether male reported and actual height are the same. Perform the test, make a great graph to illustrate, and report your results in the technical style. Show all required code.\n\n\n\n6.2\nDevise a similar test to the one in the previous question using the weight variables in females. Formally state your hypothesis, perform the test, make a great graph to illustrate, and report your results in the technical style. Show all required code.\n\n\n\n6.3\nCalculate the difference between reported height and reported weight for all study subjects and place the result in a new numeric vector. Use a t-test to discover whether the degree of discrepancy between reported height differs between males and females. Report your results in the technical style. Show all required code.\n\n\n\n6.4\nDevise a way to examine the question whether taller people tend to self report height similarly to whorter people. Discuss your approach and present any evidence, graphical or otherwise, to resolve your question.\n\n\n\n6.5\nThe subjects in this dataset were students at the University of California Davis in the Psychology Department. The average height of adult men in California has been estimated as 176.5 cm. Test whether males in our dataset is different. State your conclusion and results and briefly discuss an explanation for the pattern (i.e., the difference or lack of difference) that you observe. Comment on sampling assumptions when you do so.\n\n\n\n6.6\nWrite a plausible practice question involving any aspect of data handling, graphing or analysis for the t-test framework to ask a novel question for the Davis student height data.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "11 T-test"
    ]
  },
  {
    "objectID": "13-reproducibility.html",
    "href": "13-reproducibility.html",
    "title": "13 Reproducibility Principles",
    "section": "",
    "text": "Learning Objectives\n\n\n\nBy the end of this lesson, you will be able to:\n\nExplain why reproducibility matters in data science\nIdentify key components of a reproducible workflow\nImplement best practices for documentation\nOrganize project files effectively",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "13 Reproducibility Principles"
    ]
  },
  {
    "objectID": "13-reproducibility.html#why-reproducibility-matters",
    "href": "13-reproducibility.html#why-reproducibility-matters",
    "title": "13 Reproducibility Principles",
    "section": "Why Reproducibility Matters",
    "text": "Why Reproducibility Matters\nReproducibility is a cornerstone of scientific research and data analysis. It ensures that your findings can be verified, your methods can be understood, and your work can be built upon by others (including your future self!).\n\n\n\n\n\n\nDefinition\n\n\n\nReproducibility means that your analysis can be recreated by others using the same data and methods, producing the same results.\n\n\nIn practice, reproducibility provides several key benefits:\n\nVerification: Others can confirm your findings\nCollaboration: Team members can understand and contribute to your work\nEfficiency: You can revisit and build upon your own work more easily\nImpact: Your research can have greater influence when others can use it\nTrust: Reproducible research builds credibility in your findings",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "13 Reproducibility Principles"
    ]
  },
  {
    "objectID": "13-reproducibility.html#components-of-reproducible-workflows",
    "href": "13-reproducibility.html#components-of-reproducible-workflows",
    "title": "13 Reproducibility Principles",
    "section": "Components of Reproducible Workflows",
    "text": "Components of Reproducible Workflows\nA reproducible data science workflow typically includes the following elements:\n\n1. Documented Code\n# Example of well-documented code\n# Purpose: Calculate mean values by group\n# Input: data frame with numeric 'value' column and categorical 'group' column\n# Output: data frame of group means\n\ncalculate_group_means &lt;- function(data, value_col, group_col) {\n  # Check inputs\n  if (!is.data.frame(data)) {\n    stop(\"Input must be a data frame\")\n  }\n  \n  # Calculate means by group\n  result &lt;- aggregate(data[[value_col]], by = list(Group = data[[group_col]]), \n                     FUN = mean, na.rm = TRUE)\n  \n  # Rename columns for clarity\n  names(result)[2] &lt;- \"Mean\"\n  \n  return(result)\n}\n\n\n2. Version Control\nVersion control systems like Git help track changes to your code and files over time. We’ll cover this in detail in a later lesson.\n\n\n3. Environment Management\nDocumenting your software environment ensures others can recreate the same conditions:\n# Example of capturing package versions\nsessionInfo()\n\n# Or using the renv package for project-specific environments\n# install.packages(\"renv\")\n# renv::init()\n# renv::snapshot()\n\n\n4. Data Management\nProper data management includes:\n\nRaw data preservation (never modify the original data)\nData cleaning scripts (document all transformations)\nClear data documentation (metadata)\n\n\n\n5. Clear Documentation\nDocumentation should include:\n\nProject overview and purpose\nData sources and descriptions\nAnalysis methods and justification\nInstructions for reproducing results",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "13 Reproducibility Principles"
    ]
  },
  {
    "objectID": "13-reproducibility.html#file-organization-strategies",
    "href": "13-reproducibility.html#file-organization-strategies",
    "title": "13 Reproducibility Principles",
    "section": "File Organization Strategies",
    "text": "File Organization Strategies\nAn organized file structure makes your project more navigable and reproducible:\nproject/\n├── README.md           # Project overview and instructions\n├── data/\n│   ├── raw/            # Original, immutable data\n│   └── processed/      # Cleaned, transformed data\n├── code/\n│   ├── 01_clean.R      # Data cleaning script\n│   ├── 02_analyze.R    # Analysis script\n│   └── 03_visualize.R  # Visualization script\n├── results/\n│   ├── figures/        # Generated plots\n│   └── tables/         # Generated tables\n├── docs/\n│   └── report.Rmd      # R Markdown report\n└── renv/               # Package environment information\n\n\n\n\n\n\nBest Practice\n\n\n\nName your files in a way that indicates their order and purpose, such as 01_data_cleaning.R, 02_analysis.R, etc.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "13 Reproducibility Principles"
    ]
  },
  {
    "objectID": "13-reproducibility.html#documentation-best-practices",
    "href": "13-reproducibility.html#documentation-best-practices",
    "title": "13 Reproducibility Principles",
    "section": "Documentation Best Practices",
    "text": "Documentation Best Practices\nEffective documentation should:\n\nBe comprehensive - Include all necessary information\nBe clear - Use plain language and avoid jargon\nBe current - Update as your project evolves\nBe accessible - Store documentation with your project\n\nA good README file typically includes:\n\nProject title and description\nInstallation and setup instructions\nUsage examples\nData dictionary\nAnalysis workflow overview\nDependencies and requirements",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "13 Reproducibility Principles"
    ]
  },
  {
    "objectID": "13-reproducibility.html#exercises",
    "href": "13-reproducibility.html#exercises",
    "title": "13 Reproducibility Principles",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 1\n\n\n\nEvaluate a recent project of yours for reproducibility. Identify three specific improvements you could make to enhance its reproducibility.\n\nExercise 2\nCreate a file organization template for your next data analysis project, following the best practices outlined in this lesson.\n\n\nExercise 3\nWrite documentation for a simple R function that you commonly use, following the guidelines for effective code documentation.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "13 Reproducibility Principles"
    ]
  },
  {
    "objectID": "13-reproducibility.html#next-steps",
    "href": "13-reproducibility.html#next-steps",
    "title": "13 Reproducibility Principles",
    "section": "Next Steps",
    "text": "Next Steps\nIn the next lesson, we’ll explore R Markdown as a powerful tool for creating reproducible documents that combine code, results, and narrative.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "13 Reproducibility Principles"
    ]
  },
  {
    "objectID": "13-reproducibility.html#additional-resources",
    "href": "13-reproducibility.html#additional-resources",
    "title": "13 Reproducibility Principles",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nThe Turing Way: Guide to Reproducible Research\nTen Simple Rules for Reproducible Computational Research\nrOpenSci Reproducibility Guide",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "13 Reproducibility Principles"
    ]
  },
  {
    "objectID": "15-github-basics.html",
    "href": "15-github-basics.html",
    "title": "15 Git and GitHub Basics",
    "section": "",
    "text": "Learning Objectives\n\n\n\nBy the end of this lesson, you will be able to:\n\nUnderstand the basics of version control with Git\nSet up Git on your computer\nCreate a GitHub account and repository\nPerform basic Git operations (clone, add, commit, push, pull)\nUnderstand branching and merging concepts",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub Basics"
    ]
  },
  {
    "objectID": "15-github-basics.html#what-is-version-control",
    "href": "15-github-basics.html#what-is-version-control",
    "title": "15 Git and GitHub Basics",
    "section": "What is Version Control?",
    "text": "What is Version Control?\nVersion control is a system that records changes to files over time so that you can recall specific versions later. For data science projects, version control helps you:\n\nTrack changes to your code and documents\nCollaborate with others without overwriting each other’s work\nRevert to previous versions if something goes wrong\nDocument the evolution of your analysis\n\n\n\n\n\n\n\nKey Concept\n\n\n\nGit is a distributed version control system that tracks changes in any set of files. GitHub is a web-based platform that hosts Git repositories and adds collaboration features.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub Basics"
    ]
  },
  {
    "objectID": "15-github-basics.html#why-use-git-and-github",
    "href": "15-github-basics.html#why-use-git-and-github",
    "title": "15 Git and GitHub Basics",
    "section": "Why Use Git and GitHub?",
    "text": "Why Use Git and GitHub?\nFor data scientists and researchers, Git and GitHub provide several benefits:\n\nHistory tracking: Document how your analysis evolved\nCollaboration: Work with others seamlessly\nBackup: Store your code securely in the cloud\nReproducibility: Others can access and run your exact code\nOpen science: Share your work with the broader community",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub Basics"
    ]
  },
  {
    "objectID": "15-github-basics.html#setting-up-git",
    "href": "15-github-basics.html#setting-up-git",
    "title": "15 Git and GitHub Basics",
    "section": "Setting Up Git",
    "text": "Setting Up Git\n\nInstalling Git\nFirst, you need to install Git on your computer:\n\nWindows: Download and install from git-scm.com\nMac: Install via Homebrew with brew install git or download from git-scm.com\nLinux: Use your package manager, e.g., sudo apt-get install git\n\n\n\nConfiguring Git\nAfter installation, configure Git with your name and email:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub Basics"
    ]
  },
  {
    "objectID": "15-github-basics.html#creating-a-github-account",
    "href": "15-github-basics.html#creating-a-github-account",
    "title": "15 Git and GitHub Basics",
    "section": "Creating a GitHub Account",
    "text": "Creating a GitHub Account\n\nGo to github.com\nClick “Sign up” and follow the instructions\nChoose a free plan to start",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub Basics"
    ]
  },
  {
    "objectID": "15-github-basics.html#git-basics",
    "href": "15-github-basics.html#git-basics",
    "title": "15 Git and GitHub Basics",
    "section": "Git Basics",
    "text": "Git Basics\n\nKey Concepts\n\nRepository (repo): A directory where Git tracks changes\nCommit: A snapshot of changes at a point in time\nBranch: A parallel version of the repository\nRemote: A version of the repository hosted elsewhere (e.g., on GitHub)\nClone: Creating a local copy of a remote repository\nPush: Sending commits to a remote repository\nPull: Getting changes from a remote repository\n\n\n\nCreating a Repository\n\nOn GitHub:\n\nLog in to GitHub\nClick the “+” icon in the top-right corner\nSelect “New repository”\nEnter a repository name and description\nChoose public or private\nClick “Create repository”\n\n\n\nOn Your Computer:\n# Create a new directory\nmkdir my-project\ncd my-project\n\n# Initialize Git repository\ngit init\n\n# Connect to GitHub repository\ngit remote add origin https://github.com/yourusername/my-project.git\n\n\n\nBasic Git Workflow\nThe typical Git workflow involves these steps:\n\nMake changes to your files\nStage the changes for commit\nCommit the changes with a message\nPush the changes to GitHub\n\n# Check status of your repository\ngit status\n\n# Stage changes\ngit add filename.R\n\n# Stage all changes\ngit add .\n\n# Commit changes\ngit commit -m \"Add data cleaning script\"\n\n# Push to GitHub\ngit push origin main\n\n\nCloning an Existing Repository\nTo work with an existing repository:\n# Clone a repository\ngit clone https://github.com/username/repository.git\n\n# Navigate into the repository\ncd repository\n\n\nPulling Changes\nWhen working with others, you’ll need to get their changes:\n# Get latest changes\ngit pull origin main",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub Basics"
    ]
  },
  {
    "objectID": "15-github-basics.html#branching-and-merging",
    "href": "15-github-basics.html#branching-and-merging",
    "title": "15 Git and GitHub Basics",
    "section": "Branching and Merging",
    "text": "Branching and Merging\nBranches allow you to work on different features or experiments without affecting the main codebase.\n\nCreating and Using Branches\n# Create a new branch\ngit branch feature-analysis\n\n# Switch to the branch\ngit checkout feature-analysis\n\n# Create and switch in one command\ngit checkout -b new-feature\n\n# List all branches\ngit branch\n\n\nMerging Branches\nOnce you’re satisfied with your changes:\n# Switch back to main branch\ngit checkout main\n\n# Merge your feature branch\ngit merge feature-analysis\n\n# Push the merged changes\ngit push origin main",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub Basics"
    ]
  },
  {
    "objectID": "15-github-basics.html#using-github-for-collaboration",
    "href": "15-github-basics.html#using-github-for-collaboration",
    "title": "15 Git and GitHub Basics",
    "section": "Using GitHub for Collaboration",
    "text": "Using GitHub for Collaboration\nGitHub enhances Git with collaboration features:\n\nPull Requests\nPull requests (PRs) let you propose changes to a repository:\n\nFork a repository to your GitHub account\nClone your fork locally\nCreate a branch and make changes\nPush your branch to your fork\nCreate a pull request to the original repository\n\n\n\nIssues\nGitHub Issues help track tasks, enhancements, and bugs:\n\nCreate detailed issue descriptions\nAssign issues to team members\nLabel issues by type\nReference issues in commits and pull requests",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub Basics"
    ]
  },
  {
    "objectID": "15-github-basics.html#git-and-rstudio-integration",
    "href": "15-github-basics.html#git-and-rstudio-integration",
    "title": "15 Git and GitHub Basics",
    "section": "Git and RStudio Integration",
    "text": "Git and RStudio Integration\nRStudio provides a user-friendly interface for Git operations:\n\nCreate a new project with version control\nUse the Git pane to stage, commit, and push changes\nView file differences and history",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub Basics"
    ]
  },
  {
    "objectID": "15-github-basics.html#best-practices-for-data-science-projects",
    "href": "15-github-basics.html#best-practices-for-data-science-projects",
    "title": "15 Git and GitHub Basics",
    "section": "Best Practices for Data Science Projects",
    "text": "Best Practices for Data Science Projects\n\nCommit often: Make small, focused commits\nWrite clear commit messages: Explain what and why, not how\nUse .gitignore: Exclude large data files, outputs, and sensitive information\nStructure your repository: Follow a consistent organization pattern\nDocument your workflow: Include a README with setup instructions",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub Basics"
    ]
  },
  {
    "objectID": "15-github-basics.html#exercises",
    "href": "15-github-basics.html#exercises",
    "title": "15 Git and GitHub Basics",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 1\n\n\n\nCreate a GitHub account (if you don’t have one) and set up Git on your computer.\n\nExercise 2\nCreate a new repository on GitHub and clone it to your computer. Add a simple R script, commit it, and push it to GitHub.\n\n\nExercise 3\nFork an existing R project on GitHub, make a small improvement, and create a pull request.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub Basics"
    ]
  },
  {
    "objectID": "15-github-basics.html#next-steps",
    "href": "15-github-basics.html#next-steps",
    "title": "15 Git and GitHub Basics",
    "section": "Next Steps",
    "text": "Next Steps\nIn the next lesson, we’ll explore collaborative workflows and best practices for team-based data science projects.",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub Basics"
    ]
  },
  {
    "objectID": "15-github-basics.html#additional-resources",
    "href": "15-github-basics.html#additional-resources",
    "title": "15 Git and GitHub Basics",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nHappy Git and GitHub for the useR\nGitHub Guides\nGit Cheat Sheet\nGitHub Learning Lab",
    "crumbs": [
      "Home",
      "Module 3: Reproducible Research",
      "15 Git and GitHub Basics"
    ]
  },
  {
    "objectID": "2-r-lang.html",
    "href": "2-r-lang.html",
    "title": "2 R language",
    "section": "",
    "text": "Choose your language carefully",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "2 R language"
    ]
  },
  {
    "objectID": "2-r-lang.html#r-syntax-basics-r-as-a-passive-aggressive-butler",
    "href": "2-r-lang.html#r-syntax-basics-r-as-a-passive-aggressive-butler",
    "title": "2 R language",
    "section": "1 R Syntax basics (R as a passive-aggressive butler)",
    "text": "1 R Syntax basics (R as a passive-aggressive butler)\n\n\nWe love the R language, but sometimes it is a little bit like talking to a passive aggressive butler - if you aren’t careful with your language, the interaction may have unexpected outcomes…\n\nR is a very popular statistical programming language and open source software design to help scientists and other non-programmers perform statistical analyses and to make great graphs. This page is intended to guide people through some of the basics of the R programming language, just enough to get started. We hope that these pages help make learning R simple (though it can difficult at times while you are learning)\n\n\n1.1 Objectives\nHere is what we will work on:\n\nExample script, comments, help, pseudocode\nMath operators\nLogical Boolean operators\nRegarding “base R” and the Tidyverse\nPractice exercises",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "2 R language"
    ]
  },
  {
    "objectID": "2-r-lang.html#example-script-comments-help-pseudocode",
    "href": "2-r-lang.html#example-script-comments-help-pseudocode",
    "title": "2 R language",
    "section": "2 Example script, comments, help, pseudocode",
    "text": "2 Example script, comments, help, pseudocode\n\nThe most important thing to keep in mind right at the beginning of learning R is to view the script as something you are writing to document a piece of work (e.g. progress in a workshop like this, an analysis, a research project, etc.). Organizing every script you write is extremely important to build “good habits” for reproducibility of your work. A good guideline for best practice in writing scripts is to pretend you are writing the script, comments and contents, for a respected colleague - someone who you respect and want to impress, but might not be able to explain in person about the purpose of the script (even if this is your future self).\nTo get the most out of this page, we strongly recommend that you:\n\nWork through the instructions here while using R and RStudio as you go along.\nType you own code rather than using copy and paste\nDocument all the code in your own script and write clear, concise comments\n\n\n2.1 Example script\nDownload this example script.\nSave it in a logical place on your computer. Open it in RStudio.\n\nRight at the top you should see the HEADER.\n## HEADER ####\n## Who: &lt;YOUR NAME&gt;\n## What: 2 R language basics\n## Last edited: &lt;DATE TODAY in yyyy-mm-dd format)\n####\n\nGo ahead and fill in the header with your own information.\nNext you should see the CONTENTS section. The idea here is for you to list the CONTENTS of a script that can act as a roadmap to the user, but that can also help organize a large project into manageable chunks. Each title under the CONTENTS section will become the title in a code chunk below. You should see the contents section in the script you downloaded like this:\n## CONTENTS ####\n## 2 Example script, help, pseudocode  \n## 3 Math operators  \n## 4 Logical Boolean operators  \n## 5 Regarding base R and the Tidyverse   \n## 6 Practice exercises  \n\nEach item in the CONTENTS section will become the title of individual code chunks. RStudio recognizes code chunks that have a particular syntax:\nCode chunks begin with 2 “##” signs (1 works too, we prefer 2)\nCode chunks end with at least 4 “####” signs\nThe first code chunk should already be placed in your script\n## 2 Example script, help, pseudocode  ####\n\n\n\n2.2 Help\nOne of the great things about using R is the community and the spirit of helping others. However, there are so many websites, books, blogs and other resources that it can be overwhelming. Best practice is to learn to use the R Help system first, then seek help elsewhere.\nThe basic way to access the built-in help in R, is to use the help() function, with the name of tool you need help using inside the brackets. For example, to calculate the mean of some numbers, we would use the function mean(), and to display the help for the mean() function we would run. Run the following code in your own script:\n# Display help page for the function mean\nhelp(mean)\n\nYou should see:\n\n\n\n\nMean help page (pun intended)\n\n\n\n\nDoes this help look mean enough?\nLet’s orient to the information that is here because the help pages are essential to understand and every help page on every subject is organised in exactly the same way (and we will practice a lot using them).\n\n1 Function name {Package name} This field let’s you know what “R package” the function belongs to. We can ignore this for now, but it can be very useful.\n2 Short description This tells you in a few words what the function does.\n3 (longer) description This gives a longer description of what the function does\n4 Usage This usually gives an example of the function in use and lists the “arguments” that you are required to supply to the function for it to work on. Of course, you need to know about the arguments…\n5 Argument definitions This field tells you what the argument are and do!\n\n\n\n2.3 Deeper help\nUsing the Usage and Argument fields, we can figure out how to make the function do the work we want.\n# Under Usage:\n# mean(x, ...)\n\n# The \"x\" is an argument that is required\n# The \"...\" means there are other optional arguments\n\n# Under Arguments:\n\n# x \n# An R object... for numeric/logical vectors ...\n\n# try this code in your own script\nmy_length &lt;- c(101, 122, 97) # 3 numerical measures\nmean(x = my_length) \n\n\n\n2.4 Pseudocode\nThe idea of pseudocode is to break up a big task into a series of smaller tasks. An example of a task might be ANALYZE YOUR DATA (in shouty capitals because it is a big task). To accomplish this task, we might have to walk through a series of steps, e.g.,\nAnalyze your data:\n\nRead data into R\nTest assumption for statistical testing\nGraph the data\nPerform statistical test\nOrganize outputs to communicate in report\n\n\nIt is often a good idea to break down a task into pseudocode both to organize and document the methods in a logical way, but also to conceptually simplify a problem that is difficult to solve. Practically, the items in a typical table of contents in an R script might be similar to psuedocode. Note that this technique extends very well to any problem, not just R code and programming.\nE.g., what steps would be involved in a problem like: Send a rocket with people in it to Mars such that they survive and return to Earth.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "2 R language"
    ]
  },
  {
    "objectID": "2-r-lang.html#math-operators",
    "href": "2-r-lang.html#math-operators",
    "title": "2 R language",
    "section": "3 Math operators",
    "text": "3 Math operators\nBasic manipulation of numbers in R is very easy to do and is so intuitive that you may be able to guess what they are and what they do. There are just a few specifics that we will practice. This list is not exhaustive; the goal is to get enough to begin practicing.\n\n\n3.1 Arithmetic\nTry these in your practice script:\n# There are a few others, but these are the basics\n\n# Add with \"+\"\n2 + 5\n\n# Subtract with \"-\"\n10 - 15\n\n# Multiply with \"*\"\n6 * 4.2\n\n# Divide by \"/\"\n\n10 / 4\n\n# raise to the power of x\n2^3 \n9^(1/2) # same as sqrt()!\n\nYour output should look similar to this:\n\n# There are a few others, but these are the basics\n\n# Add with \"+\"\n2 + 5\n\n[1] 7\n\n# Subtract with \"-\"\n10 - 15\n\n[1] -5\n\n# Multiply with \"*\"\n6 * 4.2\n\n[1] 25.2\n\n# Divide by \"/\"\n\n10 / 4\n\n[1] 2.5\n\n# raise to the power of x\n2^3 \n\n[1] 8\n\n9^(1/2) # same as sqrt()!\n\n[1] 3\n\n\n\nOrder of operation\nThe “order of operation” refers to the order in which mathematical calculations are carried out. A phrase like 2 + 2 is simple, but we need to consider order for more complicated phrases like 2 + 2 * 8 - 6. In general multiplication and division are carried out before addition and subtraction unless specific order is coded.\n# Try this\n\n4 + 2 * 3\n\n# Order control - same\n4 + (2 * 3)\n\n# Order control - different...\n(4 + 2) * 3\n\n\n\n3.2 Use of spaces\nIn some cases, the use of spaces does not matter in the R language. Which one of the following ways of writing math operation might be easier to document and read?\n# Try this\n\n6+10                                  # no spaces\n7     -5                              # uneven spaces\n1.6             /                2.3  # large spaces\n16 * 3                                # exactly 1 space\n\n# exactly 1 space is probably easiest to read...\nYour output should look like this:\n\n# Try this\n\n6+10                                  # no spaces\n\n[1] 16\n\n7     -5                              # uneven spaces\n\n[1] 2\n\n1.6             /                2.3  # large spaces\n\n[1] 0.6956522\n\n16 * 3                                # exactly 1 space\n\n[1] 48",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "2 R language"
    ]
  },
  {
    "objectID": "2-r-lang.html#logical-boolean-operators",
    "href": "2-r-lang.html#logical-boolean-operators",
    "title": "2 R language",
    "section": "4 Logical “Boolean” operators",
    "text": "4 Logical “Boolean” operators\nBoolean operators are expressions that resolve TRUE (treated as 1 in most computing systems including R) versus FALSE (0). A typical expression might be something like asking if 5 &gt; 3, which is TRUE. More sophisticated phrases are possible, and sometimes useful.\n\n4.1 Boolean example\n# Try this\n# simplest example\n3 &gt; 5\n\n# 3 is compared to each element\n3 &lt; c(1, 2, 3, 4, 5, 6) \n\n# Logic and math\n# & (ampersand) means \"and\"\n# | (pipe) means \"or\"\n\n# This asks if both phrases are true (true AND true)\n# notice \"TRUE\" has a special meaning in R\n\nTRUE & TRUE # both phrases are the same and true, TRUE\n\n3 &gt; 1 & 1 &lt; 5 # both phrases are true\n\n# Are these phrases true?\n\nTRUE & FALSE # are both true?\n\nFALSE & FALSE # are both true?\n\nBoolean expressions are often used to select groups of data, for example asking whether values in a column of variables are greater than some threshold.\n\n\n4.2 Selecting with Booleans\nWe often use Booleans to select particular parts of our data in a powerful way, as an alternative to creating different versions of a particular dataset.\n# Try this\n\n# Put some data into a variable and then print the variable\n# Note \"&lt;-\" is the ASSIGNMENT syntax in R, which puts the value on the left \"into\" x\n\nx &lt;- c(21, 3, 5, 6, 22)\nx\n\nx &gt; 20\n\n# the square brackets act as the index for the data vector\nx[x &gt; 20]\n\n\n\n4.3 The “not” operator, ! (Sorry !Sorry)**\nThe ! operator sets a Boolean value to the opposite. This is sometimes used when an expression can be made simpler by representing it as an opposite. For now we will just demonstrate how it works.\n# Try this\nTRUE # plain true\n\n!FALSE # not false is true!\n\n6 &lt; 5 #definitely false\n\n!(6 &lt; 5) #not false...\n\n!(c(23, 44, 16, 51, 12) &gt; 50)",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "2 R language"
    ]
  },
  {
    "objectID": "2-r-lang.html#base-r-versus-the-tidyverse",
    "href": "2-r-lang.html#base-r-versus-the-tidyverse",
    "title": "2 R language",
    "section": "5 “Base R” versus the ‘Tidyverse’",
    "text": "5 “Base R” versus the ‘Tidyverse’\n\n5.1 Base R\nThe R language as it was invented and continues to be developed is extremely popular, powerful and easy to use, especially for people without formal programming training or experience in different computing languages. In general, we refer to this pure form of R as Base R.\nSince the late 1990s, the R-user community and Base R resources on have exploded on the web and this form of the language continues to be extremely popular for experts and beginners alike. If interested, you can read more here.\n\n\n\n5.2 Tidyverse R\nRelatively recently, a second version of R has evolved that puts forward different conventions in the R lanuage. The differences are practical, but also philosophical. This form of R use is generally referred to as The Tidyverse. The Tidyverse is extremely powerful and we love it. However, we feel that it is far more efficient to first learn base R for non-programmers, before learning the Tidyverse, and we will exclusively use base R for this bootcamp.\nThere is some disagreement over which “version” of R is better or easier to learn and teach with. You will definitely encounter the Tidyverse at some point and eventually you can choose how much or how little you will use it.\n\nThe Argument for Base R\nThe Argument for the Tidyverse",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "2 R language"
    ]
  },
  {
    "objectID": "2-r-lang.html#practice-exercises",
    "href": "2-r-lang.html#practice-exercises",
    "title": "2 R language",
    "section": "6 Practice exercises",
    "text": "6 Practice exercises\n\n\n6.1\nName and describe the purpose of the first 2 sections that should be present in every R script\n\n\n\n6.2\nWhat is the purpose of “subset” argument in the boxplot() function (hint: use help())\n\n\n\n6.3\nWrite an expression using good R spacing syntax that takes the sum of 3, 6, and 12 and divides it by 25\n\n\n\n6.4\nWrite pseudocode steps for calculating the volume of a cylinder (hint, if you do not know it by heart, you may need to research the equation for the volume of a cylinder!). For a cylinder of height = 3.2 cm and end radius of 5.5 cm, report the volume in cm to 2 decimal points of accuracy. Use at least 3 decimal points of accuracy for pi (hint, the quantity named pi is a standard variable in R!).\n\n\n\n6.5\nExecute the code and explain the outcome in comments.\nTRUE & 3 &lt; 5 & 6 &gt; 2 & !FALSE\n\n\n\n6.6\nWrite a plausible practice question involving the use of the not Boolean operator, !.\n\nNote: for this R Stats Bootcamp page, and only for this one, a sample of solutions is offered. For others, we avoid explicit solutions to encourage you to formulate your own unique solutions and to interact in Slack for help if you need support coming to your own solutions.\nSCRIPT 2 Example SOLUTIONS",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "2 R language"
    ]
  },
  {
    "objectID": "4-data.html",
    "href": "4-data.html",
    "title": "4 Data objects",
    "section": "",
    "text": "Data objects in ‘R’ space",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "4 Data objects"
    ]
  },
  {
    "objectID": "4-data.html#data-objects-in-r",
    "href": "4-data.html#data-objects-in-r",
    "title": "4 Data objects",
    "section": "1 Data objects in R",
    "text": "1 Data objects in R\n\nImagine you are floating in space in the R Global Environment and any data object you can see, you can call on by name to manipulate with functions. Let’s call it R Space\n\nThe fundamental way to analyse data in R is to be able to manipulate it with code. In order to do that, we need a system of containers to store the data in. This page is about the rules used for storing data in data objects. We will examine basic data types at the same time as how R stores them. R is actually very flexible in the way it handles data, making it as easy as possible for non-programmers to get going.\n\n\n1.1 Objectives\n\nBasic data types in R, str()\nData with factors\nclass() and converting variables\nVector and Matrix fun\nPractice exercises",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "4 Data objects"
    ]
  },
  {
    "objectID": "4-data.html#basic-data-types-str",
    "href": "4-data.html#basic-data-types-str",
    "title": "4 Data objects",
    "section": "2 Basic data types, str()",
    "text": "2 Basic data types, str()\n\n2.1 Storage containers types\nThe first things to examine are the way that R variables are named, and the organization system for storing the data. The organization part is particularly important, because it is used to actually access and use data.\nVariable names in R are simple and do not need to be “declared” like in some computing languages, and they can be almost anything, but there are a few rules. The R system has a built-in error message and warning message system (also known as the Passive-Aggressive Butler), which will usually give a hint when some of these rules are violated.\n\n\n\n2.2 The Global Environment\nOne of the things we notice when people begin using R, even if they are experienced in data analysis, is that they expect to “see” data and data sets, almost as they are physical things. This might be because of experience using Excel and seeing the visual representation of data in spreadsheets (indeed, a graphical representation of physical spreadsheets!).\nThe regular R system for interacting with data is a little more abstract which can be disconcerting to new users. Typical use is to create variables in code script files and, usually, the bring data into the Global Environment from external files on the local PC, or from the web. We will practice using the Global Environment is the main way to interact with data.\nYou can use the class() function to find out the variable type (using this this is a good idea, since R occasionally guesses the intended data type incorrectly).\n# Try this\n\nvariable_1 &lt;- c(4,5,7,6,5,4,5,6,7,10,3,4,5,6) # a numeric vector\n\nvariable_2 &lt;- c(TRUE, TRUE, TRUE, FALSE) # a logical vector\n\nvariable_3 &lt;- c(\"Peter Parker\", \"Bruce Wayne\", \"Groo the Wanderer\") # a character vector\n\nclass(variable_1) # \"numeric\"\nclass(variable_2) # \"logical\"\nclass(variable_3) # \"character\"\n\nLook at the upper right pane of your RStudio and you should see something like:\n\n\n\n\nRStudio Global Environment\n\n\n\n\nThe Environment tab contains the Global Environment (labelled A in the picture above). There are some other tabs, but we will ignore these for now. The Global Environment itself contains information about the variables that are held in memory. If we think of this as “R Space”, a general rule is that if you can see a variable here in the Global Environment, you can manipulate it and work with it.\nNotice that there is quite a lot of information in the Global Environment about the actual variables (B in the picture). There is a column with the variable NAME (variable_1, variable_2, etc.), a column with the variable TYPES (num, logi, etc.), a column with the variable dimensions ([1:14] is an index like a unique “street address” for each of the 14 numeric values contained in “variable_1”)\n\n\n\n2.3 Naming conventions for variables\nVariable names:\n\nCan contain letters, numbers, some symbolic characters\nMust begin with a letter\nMust not contain spaces\nSome forbidden characters like math operators, “@”, and a few others\nShould be human-readable, consistent, and not too long\nCase sensitive\n\n# Try this\n\n## Variable name rules ####\n\n# Can contain letters, numbers, some symbolic characters\nx1 &lt;- 5  # OK\n\nx2 &lt;- \"It was a dark and stormy night\" # OK\n\nmy_variable_9283467 &lt;- 1 # technically works, but hard to read\n\n# Must begin with a letter \n\nvarieties &lt;- c(\"red delicious\", \"granny smith\") # OK\n\nx432 &lt;- c(\"a\", \"b\") # OK\n\n22catch &lt;- c(TRUE, TRUE, FALSE)  # nope\n\n# Must not contain spaces\nmy_variable &lt;- 3 # OK\n\nmy.variable &lt;- 4 # OK\n\nmyVariable &lt;- 5 # OK\n\nmy variable &lt;- 6 # nope\n\n\"my variable\" &lt;- 7 # nope\n\n# Must not contain forbidden characters like \n# math operators, \"@\", and a few others\nmy@var &lt;- 1 # nope\n\nmy-var &lt;- 1 # nope\n\nmy=var &lt;- 1 # nope\n\n# etc.\n\n# Should be human-readable, consistent, and not too long\n\nDiameter_Breast_Height_cm &lt;- c(22, 24, 29, 55, 43) # legal but too long\n\nDBH_cm &lt;- c(22, 24, 29, 55, 43) # much better\n\n#Case sensitive\nheight &lt;- c(180, 164, 177) # OK\n\nHeight # Error: object 'Height' not found (notice capital H)\n\nheight # OK",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "4 Data objects"
    ]
  },
  {
    "objectID": "4-data.html#data-with-factor",
    "href": "4-data.html#data-with-factor",
    "title": "4 Data objects",
    "section": "3 Data with factor()",
    "text": "3 Data with factor()\nSometimes you will need to analyze data that is a factor, where the different values are categories. Factors in R can be a cause for confusion, but there needn’t be problems if you understand them. The information here is a starting point, and we skip some complexities, but essentially\n\n3.1 Two types of factors, non-ordered and ordinal\nNon-ordered factors are simply categories and the levels are simply the names of the categories. By definition, non-ordered factor do not have a specific order! an example here might be plant varieties.\nOrdered factors have a specific order, which can be important for analysis or for graphing. You usually have to specify the order explicitly to get this right. An example here might be the days of the week, where the order of day is important.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "4 Data objects"
    ]
  },
  {
    "objectID": "4-data.html#class-and-converting-variables",
    "href": "4-data.html#class-and-converting-variables",
    "title": "4 Data objects",
    "section": "4 Class() and converting variables",
    "text": "4 Class() and converting variables\nWe use the class() function to query what data category a variable is set too. R is pretty good at setting this correctly, but it is a good idea to check sometimes and occasionally you will have to manually set variable characteristics.\n\n# try this\n\n# non-ordered factor\nvariety &lt;- c(\"short\", \"short\", \"short\",\n             \"early\", \"early\", \"early\",\n             \"hybrid\", \"hybrid\", \"hybrid\")\nclass(variety)  # \"character\", but this is really a factor...\nvariety # Notice the character strings are just printed out\n\nvariety &lt;- factor(variety) # use factor() to convert the character vector to a factor\nclass(variety)  # now variety is a \"factor\"\nvariety # notice the output has changed\n\nNow, try some code manipulating ordered factors.\n# Ordered factors\nday &lt;- c(\"Monday\", \"Monday\", \n         \"Tuesday\", \"Tuesday\", \n         \"Wednesday\", \"Wednesday\", \n         \"Thursday\")\nclass(day) # character\n\n#make day a factor\nday &lt;- factor(day)\nclass(day)\nday # Notice the Levels: Monday Thursday Tuesday Wednesday\n\n# To set the order explicitly we need to set them explicitly\nhelp(factor) # notice the levels argument - it sets the order of the level names\n\nday &lt;- factor(x = day, levels = c(\"Monday\", \"Tuesday\",\n                         \"Wednesday\", \"Thursday\"))\nday # Notice the level order now",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "4 Data objects"
    ]
  },
  {
    "objectID": "4-data.html#vector-and-matrix-fun",
    "href": "4-data.html#vector-and-matrix-fun",
    "title": "4 Data objects",
    "section": "5 Vector and Matrix fun",
    "text": "5 Vector and Matrix fun\nVector and matrix data structures are two fundamental ways to arrange data in R. We have already looked at vectors, which store data in a single dimension.\nThere are actually a few different data organisation structures in R.\n\nVector - stores data in a single dimension from 1 to i, my_vec[i]\nMatrix - stores data in two dimensions 1 to i rows, 1 to j columns my_mat[i, j]\nArray - Three (or more) dimensions from 1 to to i, j, and k, my_array[i, j, k]\n\nVectors, Matrices and Arrays can only store the same TYPE of data.\n\n\n5.1 Vector practice\n# Try this\n\nmyvec1 &lt;- c(1,2,3,4,5) # numeric vector\nmyvec1\nclass(myvec1) # see? I told ya!\n\nmyvec2 &lt;- as.character(myvec1) #convert to character\nmyvec2 # notice the quotes\nclass(myvec2) # now character\n\nmyvec3 &lt;- c(2, 3, \"male\")\nmyvec3 #notice the numbers now have quotes - forced to character...\n\nmyvec4 &lt;- as.numeric(myvec3) #notice the warning\nmyvec4 # The vector element that could not be coerced to be a numeric was converted to NA\n\n\n\n5.2 Matrices\nMatrices can be quite useful - you can manipulate data into matrix form with the matrix() function. By default rows and columns are merely numbered, but they can be named as well.\n\n# Try this\n\nvec1 &lt;- 1:16 # make a numeric vector with 16 elements\nvec1 \n\nhelp(matrix) #notice the ncol, nrow and byrow arguments\n\nmat1 &lt;- matrix(data = vec1, ncol = 4, byrow = FALSE) #byrow = FALSE is the default\n\nmat1 # Notice the numbers filled in by columns\ncolnames(mat1) # The Columns and Rows have no names\n\ncolnames(mat1) &lt;- c(\"A\", \"B\", \"C\", \"D\") # Set the column names for mat1\ncolnames(mat1)\n\nmat1 # Yep the columns shows names\n\n\n\n5.3 Matrix challenge\n# Challenge 1: Set the Row names for Mat1 using the rownames() function\n\n# Challenge 2: make a matrix with 3 rows with the following vector, \n# so the the first COLUMN contains the numbers 2, 5, and 9, in the order,\n# for rows 1, 2, and 3 respectively:\n\nvec2 &lt;- c(2,3,5,4,5,6,7,8,9,5,3,1)",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "4 Data objects"
    ]
  },
  {
    "objectID": "4-data.html#practice-exercises",
    "href": "4-data.html#practice-exercises",
    "title": "4 Data objects",
    "section": "6 Practice exercises",
    "text": "6 Practice exercises\n\n\n6.1\nCreate a vector named my_var1 that contains the following 6 integers: 3, 6, 12, 7, 5, 1. Create a Second vector called my_var2 that contains the following 2 integers: 2, 3. Evaluate the expression my_var1 + my_var 2. Explain the output in terms of R mechanics in your own words.\n\n\n\n6.2\nCreate a character vector with the names of the 12 months of the year. Convert the vector to a factor, with the month names in chronological order. Show your code.\n\n\n\n6.3\nWhat is wrong with the following code? Describe, show the code, and justify a fix for the problem in your own words.\nmymat &lt;- matrix(data = c( 12,   23,   45,\n                         \"34\", \"22\", \"31\"))\n\n\n\n6.4\nUse the array() function to make a 2 x 2 x 3 array to produce the following output:\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n, , 3\n\n     [,1] [,2]\n[1,]    9   11\n[2,]   10   12\n\n\n\n6.5\nShow the code to make the following matrix:\n       cat dog\nmale    22  88\nfemale  71  29\n\n\n\n6.6\nWrite a plausible practice question involving the use of the matrix() and vector() functions.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "4 Data objects"
    ]
  },
  {
    "objectID": "6-data-manipulation.html",
    "href": "6-data-manipulation.html",
    "title": "6 Data subsetting",
    "section": "",
    "text": "Command data and you are powerful",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "6 Data subsetting"
    ]
  },
  {
    "objectID": "6-data-manipulation.html#subsetting-and-manipulation-data-sumo",
    "href": "6-data-manipulation.html#subsetting-and-manipulation-data-sumo",
    "title": "6 Data subsetting",
    "section": "1 Subsetting and Manipulation (data sumo)",
    "text": "1 Subsetting and Manipulation (data sumo)\n\nWith a good basic set of moves for subsetting and manipulating data, you can overpower any dataset no matter how large and powerful they may be. Then, you will have strong data Sumo.\n\nSubsetting and manipulating data is probably the commonest activity for anyone who works with data. This is a core activity for exploratory data analysis, but is also extensively used in simple data acquisition, analysis and graphing, while also being related to more general data manipulating activities, for example database queries. This page is an introduction to the core syntax and some of the tools for manipulating and subsetting data in R.\n\n\n1.1 Objectives\n\nIndexing concept\nUsing which() and subsetting\nSelection on data.frame objects\nUsing aggregate()\nPractice exercises",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "6 Data subsetting"
    ]
  },
  {
    "objectID": "6-data-manipulation.html#indexing-concept",
    "href": "6-data-manipulation.html#indexing-concept",
    "title": "6 Data subsetting",
    "section": "2 Indexing concept",
    "text": "2 Indexing concept\n\nIf you would like to slice and dice your data, you will need to learn all about indexing!\n\nThe basics of the indexing concept in R syntax is very simple, where data storage objects like vectors (1 dimension), matrices (2 dimensions) and arrays (3 or more dimensions) store individual data values that can be accessed by the “address” of the dimension(s).\n\n\n2.1 How indexing works\nSay you have a numeric vector called my_vector that has 10 values. The index values will be 1 to 10, with each value corresponding consecutively to the data value at that position.\nmy_vector &lt;- c(11.3, 11.2, 10.4, 10.4, 8.7, \n               10.8, 10.5, 10.3, 9.7, 11.2)\n  \nmy_vector\n[1] 11.3 11.2 10.4 10.4 8.7 10.8 10.5 10.3  9.7 11.2\n\nNotice the [1] in the R console output? This indicates the index of value right next to it and the R system will provide an index value for longer vectors as the wrap in the console. If we could see the actual index values it would look something like this:\n&gt; my_vector\n [1] 11.3 11.2 10.4 10.4  8.7 10.8 10.5 10.3  9.7 11.2\n#     ^    ^    ^    ^    ^    ^    ^    ^    ^    ^\n#     1    2    3    4    5    6    7    8    9    10\n\n\n\n2.2 Vectors\nYou can create vector subsets by manipulating the index. Vector objects have indices in 1 dimension. For example, my_vector[1:i], where i is the length of the vector.\n## Vectors ####\n# Try this\n\nmy_vector &lt;- c(11.3, 11.2, 10.4, 10.4, \n               8.7, 10.8, 10.5, 10.3, 9.7, 11.2)\n\n# Return all values\nmy_vector        # Typical way\nmy_vector[ ]     # Blank index implies all index values\nmy_vector[ 1:10] # Returns all index values explicitly\n\n# Return the first 3 values\n1:3 # Reminder of the function of the colon operator \":\"\nmy_vector[ 1:3] # Notice consecutive indices can use the \":\" operator\n\n# Return 5th and 9th values\nmy_vector[ c(5, 9)] # Notice we have to place non-consecutive index values in the c() function\n\n\n\n2.3 Matrices\nMatrix objects have 2 dimensions denoted as my_matrix[1:i, 1:j], where i is the number of rows and j is the number of columns.\n## Matrices ####\n# Try this\n\nmy_matrix &lt;- matrix(data = c(2,3,4,5,6,6,6,6),\n                    nrow = 2, byrow = T)\n\nmy_matrix # notice how the arguments arranged the data\n\n# Flash challenge: make a matrix with the same data vector above to look like...\n#      [,1] [,2]\n# [1,]    2    6\n# [2,]    3    6\n# [3,]    4    6\n# [4,]    5    6\n\n# \"Slicing\" out a row or column\nmy_matrix[1,  ] # Slice out row 1\nmy_matric[ , 3] # Slice out column 3\n\n# Matrix columns and rows often have names\nnames(my_matrix) # No names yet\n\nnrow(my_matrix) # Returns number of rows (useful for large matrices)\nrownames(my_matrix) # No row names; 2 rows, need two names\n\nrownames(my_matrix) &lt;- c(\"dogs\", \"cats\")\nmy_matrix # Now the rows have names!\nrownames(my_matrix) # Get them this way too!\n\n# Flash challenge: Name the columns of my_matrix \"a\", \"b\", \"c\", \"d\" with colnames()\n\nmy_matrix\n\n# Should look like this:\n#      a b c d\n# dogs 2 3 4 5\n# cats 6 6 6 6\n\n# You can also slice out matrix portions by name\nmy_matrix[\"dogs\", c(\"b\", \"d\")]\n\n# Finally, functions act on values, not the index\nmean(my_matrix[\"dogs\", c(\"b\", \"d\")])\n\n\n\n2.4 Arrays\nArrays are data objects with more than 2 dimensions (well, technically a matrix with 2 dimensions is also an array, but let’s ignore that for now). Array dimensions are denoted as my_array[1:i, 1:j, 1:k], where i is the number of rows and j the columns and k the “depth” of i * j.\n\n\n\nmy_array\n\n\n## Arrays ####\n# Try this\n\n# help(runif)\n# help(round)\n# Try it to see what it does... \nmy_vec &lt;- round(runif(n = 27, min = 0, max = 100), 0)\nmy_vec # See what we did there?\n\nlength(my_vec) # Just checking\n\nmy_array &lt;- array(data = my_vec,\n                  dim = c(3, 3, 3))\nmy_array\n\n# Flash challenge: \n# Specify and print the 1st and 3rd  slice of the k dimension of my_array\n# Assuming my_array has dimensions i, j, k like my_array[i,j,k]",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "6 Data subsetting"
    ]
  },
  {
    "objectID": "6-data-manipulation.html#which-and-subsetting",
    "href": "6-data-manipulation.html#which-and-subsetting",
    "title": "6 Data subsetting",
    "section": "3 which() and subsetting",
    "text": "3 which() and subsetting\nSubsetting data objects is done by exploiting the index system. We usually do this by either specifying index values explicitly (effective, but it requires that you know A LOT about the data object), or by constructing queries that choose subset of data based on particular values. The which() function is a powerful way to construct queries.\n# Try this\nhelp(which) # Notice how the x argument is required to be a LOGICAL vector?\n\n# Make a NUMERIC vector\nvector_a &lt;- c(3, 4, 5, 4, 3, 4, 5, 6, 6, 7)\n\n# Use a boolean phrase to ask which elements of vector_a are greater than 5\nvector_a &gt; 5 # Interesting... it is a LOGICAL vector!\n\n# which() will return the index values of TRUE values\n# In other words, WHICH values in vector_a are greater than 5?\nwhich(vector_a &gt; 5)\n\nWhat is the point of all this? THE POINT is to be able to use expressions to obtain indices and values in data structures…\n# What VALUES in vector_a are &gt; 5?\nvector_a[which(vector_a &gt; 5)]\n\n# This also works on vectors of other types\n# Consider a character vector\nchar_vec &lt;- c(\"wheat\", \"maize\", \"wheat\", \"maize\", \"wheat\", \"wheat\")\n\n# Which elements are equivalent to \"wheat\"?\nchar_vec == \"wheat\"\nwhich(char_vec == \"wheat\")\n\nchar_vec[ which(char_vec == \"wheat\")] # This works\nchar_vec[ char_vec == \"wheat\"]        # Same output\n\n# Flash challenge: Explain in your own words why \n# the previous 2 lines of code have identical output?\n\nWe are just beginning to scratch the surface of possibilities with the which() function. Keep this function in mind and practice it when you can.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "6 Data subsetting"
    ]
  },
  {
    "objectID": "6-data-manipulation.html#selection-on-data.frame-objects",
    "href": "6-data-manipulation.html#selection-on-data.frame-objects",
    "title": "6 Data subsetting",
    "section": "4 Selection on data.frame objects",
    "text": "4 Selection on data.frame objects\n\nData frames are the ultimate data object for getting, storing, organizing and analyzing data. A good scientist must learn to communicate the subtlety of data. A good statistician must learn not to underestimate the subtletly of data. A good student must learn that\n\nsubtlety may exist, even in simple data.\nThere are a couple of data object types that have a special characteristic in that they store data of different types, where vectors, matrices and arrays can only store one type of data (e.g., numeric, character, logical, etc.). The special data objects that can contain multiple data types are list objects, and data frames. Here we will focus on data frames.\nData frames can have different vector types arranged by column but there is a constraint that each vector must be the same length, that is, each ROW is considered an observation for each variable value (though there may be missing data coded by NA).\nThere are a few ways to think about selecting values in a data frame. The first is simply to access values through the variable names, which can either be done by using the data frame name with the $ operator and the variable name, or by using the [ , ] syntax with either the variable name or the column number of the variable of interest see here.\nA second powerful way to access variables in a data frame is by selecting particular rows of a data frame. This may be done by selecting the rows of a data based on values of one or more variables. We will practice doing this using which(), the [ , ] syntax, and boolean phrases is the following code block.\nFor the following section, we will use the OrchardSprays dataset that exists as a data frame in the in-built {datasets} package. You can use help(OrchardSprays) to see the help page (the help page is characteristically terse, so some description is given here).\n\n\n4.1 OrchardSprays data\nThis is a classic dataset based on an experiment looking at how a chemical additive could be used to deter honeybees from being attracted to crops and subsequently killed by pesticides.\nThe experiment involved having a treatment consisting of adding a “lime sulfur emulsion” (honeybee deterrent) in increasing concentrations to a sucrose solution. The treatment variable had 8 levels including a control (no deterrent) and 7 other levels with increasing concentration of the deterrent. The treatment levels were named A (the highest amount of deterrent), B (second highest deterrent) through to G (lowest deterrent) and H (control - no deterrent) The decrease variable was a measure of the quantity of sucrose solution that was taken by honeybees (the prediction here is that higher concentrations of the deterrent should result in a lower decrease in the sucrose solution).\nThe experiment involved a Latin Square design, with the order of the 8 treatments arranged randomly in an array of 8 columns (the purpose of this design is to randomize any effect of the treatment ORDER or POSITION on the response variable). This resulted in an 8 row by 8 column experiment. The response was measured after placing 100 honeybees into an experimental chamber with the 64 containers of sucrose solution.\n\n## OrchardSprays ####\n## Understand the data - an important step\n\n# Try this\n# Load the OrchardSpray data using the data() function\ndata(OrchardSprays) # Should see OrchardSprays &lt;promise&gt; in the Global Env.\n\n# Look at the data head()\nhead(OrchardSprays) # First 6 rows\n\n# Look at variable types with str()\nhelp(str) # Good function to see info about data object\nstr(OrchardSprays)\n\n# First let's just look at the data\n# Don't worry too much about the code for these graphs if you have not encountered it before\nboxplot(decrease ~ treatment, data = OrchardSprays, \n        main = \"The pattern fits the prediction\",\n        ylab = \"Amount of sucrose consumed\",\n        xlab = \"Lime sulpher treatment amount in decreasing order (H = control)\")\n\n# This is the experimental design\n# Latin Square is kind of like Sudoku\n# No treatment can be in row or column more than once\nplot(x = OrchardSprays$colpos,  # NB use of $ syntax to access data\n     y = OrchardSprays$rowpos, \n     pch = as.character(OrchardSprays$treatment),\n     xlim = c(0,9), ylim = c(0,9),\n     main = \"The Latin Square design of treatments\",\n     xlab = \"\\\"Column\\\" position\",\n     ylab = \"\\\"Row\\\" position\")\n\n\n\n4.2 Practice selecting parts a data frame\nSelecting particular parts of a data frame based on the values of one variable is a common and extremely useful task.\n## Practice selecting parts a data frame ####\n\n# Select the rows of the dataset for treatment \"D\"\n\n# (Pseudocode steps to solve) \n# Break it down to make small steps easy to read\n\n# 01 Boolean phrase to identify rows where treatment value is \"D\"\n# 02 which() to obtain index of TRUE in boolean vector\n# 03 Exploit [ , ] syntax with data frame object to slice out rows\n\n# 01 Boolean phrase\nOrchardSprays$treatment # Just print variable to compare visually to boolean\nOrchardSprays$treatment == \"D\" # logical vector - TRUE in \"D\" positions\n\n# 02 which()\nwhich(OrchardSprays$treatment == \"D\") # Index of TRUE values\nmy_selection &lt;- which(OrchardSprays$treatment == \"D\") # Place index in a variable\nmy_selection # Just checking\n\n# 03 Exploit [ , ] syntax with data frame object to slice out rows\nOrchardSprays[my_selection, ]\n\n# Flash challenge: Select and print all rows at \"colpos\" values of 2\n\n\n\n4.3 Selection based on more than one variable value\nUsing the basic building blocks of boolean selection, more complex rules for selecting data can be made.\n## Compound boolean for selection ####\n\n# Select all rows of the data frame where \n# rowpos equals 4 OR 6 AND treatment equals \"A\" OR \"H\"\n# What we expect is exactly 2 values (A or H) for each powpos (4 or 6)\n\n# rowpos 4 and 6\nOrchardSprays$rowpos == 4 # The 4s\nOrchardSprays$rowpos == 6 # The 6s\n\nOrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6 # All together\n\n# now with which()\nwhich(OrchardSprays$rowpos == 4) # The 4s\nwhich(OrchardSprays$rowpos == 6) # The 6s\n\nwhich(OrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6) # All together\n\n# treatment A and H\nwhich(OrchardSprays$treatment == \"A\" | OrchardSprays$treatment == \"H\") # All together\n\n# Now we need the intersection of value that are in both our which() vectors\n\nwhich((OrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6) &  # It works\n        (OrchardSprays$treatment == \"A\" | OrchardSprays$treatment == \"H\") ) \n  \n# NB this is a long way of spelling out our selection, \n# but trying to be very explicit with what is going on\n\nmy_selec2 &lt;- which((OrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6) &  \n                     (OrchardSprays$treatment == \"A\" | OrchardSprays$treatment == \"H\") ) \n\nOrchardSprays[my_selec2, ] # Double check it works and is similar to expectation...\n\n# Flash challenge: Calculate the mean of decrease for treatment \"A\" \n# and the mean of decrease for treatment \"H\"",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "6 Data subsetting"
    ]
  },
  {
    "objectID": "6-data-manipulation.html#aggregate-function",
    "href": "6-data-manipulation.html#aggregate-function",
    "title": "6 Data subsetting",
    "section": "5 aggregate() function",
    "text": "5 aggregate() function\nWe often may wish to summarize parts of a data set according to some index of variable values. A very convenient tool for the is the aggregate() function, which we will practice here.\nhelp(aggregate)\n\n# A few important things to note about how this function works:\n# The \"x\" argument is a data object you input, but should only contain numeric values usually\n# If a data.frame object is input as x, a data.frame object is the output\n# The \"by\" argument must be a list() object and can be one or more indices\n# The FUN argument is the name of the function that will act on the \"x\" argument data\n\n# Let's try a few examples\n\n## 1 calculate the mean of decrease by treatment in OrchardSprays\n\naggregate(x = OrchardSprays$decrease,\n          # NB use of list() and naming it \"treatment\"\n          by = list(treatment = OrchardSprays$treatment), \n          FUN = mean)\n\n# we can \"recycle\" the code above to apply different functions\n# standard deviation with sd()\naggregate(x = OrchardSprays$decrease,\n          # NB use of list() and naming it \"treatment\"\n          by = list(treatment = OrchardSprays$treatment), \n          FUN = sd)\n\n# Range with range()\naggregate(x = OrchardSprays$decrease,\n          # NB use of list() and naming it \"treatment\"\n          by = list(treatment = OrchardSprays$treatment), \n          FUN = range)\n\n# What if we want several summary statistics?\n\naggregate(x = OrchardSprays$decrease,\n          # NB use of list() and naming it \"treatment\"\n          by = list(treatment = OrchardSprays$treatment), \n          # NB use of function() \n          FUN = function(x) c(mean = mean(x), # Add naming\n                              sd = sd(x), \n                              range = range(x)))\n\n## Example of use of aggregate object\n# Say you would like to graph a barplot of the MEAN of decrease by treatment\n# and you would like to show STANDARD DEVIATION error bars\n\n# Make data frame with summary values using aggregate()\nmy_mean &lt;- aggregate(x = OrchardSprays$decrease,\n                        by = list(treatment = OrchardSprays$treatment), \n                        FUN = mean)\n\nmy_sd &lt;- aggregate(x = OrchardSprays$decrease,\n                   by = list(treatment = OrchardSprays$treatment), \n                   FUN = sd)\n\nmy_mean\nmy_sd\n\n# Tidy things up in a new data frame using data.frame()\n# Take care of naming variables for clarity\nhelp(data.frame) # Continue using help() as a good habit\nnew_data &lt;- data.frame(treatment = my_mean$treatment,\n                       mean = my_mean$x,\n                       sd = my_sd$x)\nnew_data # Looks good\n\n# There is a lot going on in the following code\n# The point is to show what is possible\n\n(bar_centers &lt;- barplot(new_data$mean,#   use mean for barheight\n                        ylim = c(0, 115),\n                        ylab = \"Mean solution decrease (+- 1 SD)\",\n                        xlab = \"Treatment\"))\n# NB bar_centers holds the numerical position value of the bars...\n\nhelp(arrows)  # Use to draw error bars\narrows(x0 = bar_centers, \n       x1 = bar_centers,\n       y0 = new_data$mean , # start error bar at top of the bar!\n       y1 = new_data$mean + new_data$sd, # end error bar here!\n       angle = 90,\n       length = 0.1)\n\n# Last step: label the x axis\naxis(side = 1,\n     at = bar_centers,\n     labels = new_data$treatment)\n     \n# Flash challenge: Draw a new barplot by recycling the code above\n# This time, add error bars showing on both the top and the bottom of the mean values",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "6 Data subsetting"
    ]
  },
  {
    "objectID": "6-data-manipulation.html#practice-exercises",
    "href": "6-data-manipulation.html#practice-exercises",
    "title": "6 Data subsetting",
    "section": "6 Practice exercises",
    "text": "6 Practice exercises\nFor the following exercises, use the trees dataset built into R, which has Girth, Height and Volume variables for 31 Black Cherry trees.\n# Examine the data\nhelp(trees)\ndata(trees)\nstr(trees)\n\n\n6.1\nShow code to calculate the mean Girth of Black Cherry trees with Height less than 75 ft.\n\n\n\n6.2\nUse help(cut) and then use the cut() function to create a new factor variable based on the Height numeric variable in the trees dataset. Try setting the breaks argument to 2 or 3. Rename the levels of your new factor to something meaningful. Show the code.\n\n\n\n6.3\nUsing the new factor from question 2, use aggregate() to calculate the mean and standard deviation of all three variables in the trees data. Show your code and report the results to 2 decimal points of accuracy.\n\n\n\n6.4\nShow the code using which() and boolean phrases as appropriate to find the rows in the trees dataset where Girth is higher than 11 and Height is lower than 75.\n\n\n\n6.5\nRun the following code:\ndata_1 &lt;- data.frame(volume = c(4,5,6,5,6,7,6,5,6,8,7,3,8,7,NA,10),\n           population = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n           \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\"))\nUse aggregate() to calculate the mean of volume for each population (hint: you may need to use help for the functions involved and pay close attention to your data frame…).\n\n\n\n6.6\nWrite a plausible practice question involving any aspect of using which(), boolean phrases and/or aggregate() involving the in-built R dataset iris.",
    "crumbs": [
      "Home",
      "Module 1: R Foundations",
      "6 Data subsetting"
    ]
  },
  {
    "objectID": "8-sampling-dist.html",
    "href": "8-sampling-dist.html",
    "title": "8 Distributions",
    "section": "",
    "text": "Can you guess the weight of this bull? What about you and 99 friends?",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "8 Distributions"
    ]
  },
  {
    "objectID": "8-sampling-dist.html#describing-the-shape-of-data",
    "href": "8-sampling-dist.html#describing-the-shape-of-data",
    "title": "8 Distributions",
    "section": "1 Describing the shape of data",
    "text": "1 Describing the shape of data\n\nOverview\n\nI. A curve has been found representing the frequency distribution of standard deviations of samples drawn from a normal population.\n\n\n\nA curve has been found representing the frequency distribution of values of the means of such samples, when these values are measured from the mean of the population in terms of the standard deviation of the sample\n\n\n\n- Gosset. 1908, Biometrika 6:25.\n\nThe idea of sampling underpins traditional statistics and is fundamental to the practice of statistics. The basic idea is usually that there is a population of interest, which we cannot directly measure. We sample the population in order to estimate the real measures of the population. Because we merely take samples, there is error assiociated with our estimates and the error depends on both the real variation in the population, but also on chance to do with which subjects are actually in our sample, as well as the size of our sample. Traditional statistical inference within Null Hypothesis Significance Testing (NHST) exploits our estimates of error associated with our samples. While this is an important concept, it is beyond the scope of this page to review it, but you may wish to refresh your knowledge by consulting a reference, such as Irizarry 2020 Chs 13-16.\nIn this page, we will briefly look at some diagnostic tools in R for examining the distribution of data, and talk about a few important distributions that are common to encounter.\n\n\n1.1 Objectives\n\nUse of the histogram\nGaussian: that ain’t normal\nPoisson\nBinomial\nDiagnosing the distribution\nPractice exercises",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "8 Distributions"
    ]
  },
  {
    "objectID": "8-sampling-dist.html#use-of-the-histogram",
    "href": "8-sampling-dist.html#use-of-the-histogram",
    "title": "8 Distributions",
    "section": "2 Use of the histogram",
    "text": "2 Use of the histogram\nThe histogram is a graph type that typically plots a numeric variable on the x axis (either continuous numeric values, or integers), and has the frequency of observations on the y axis (i.e., the count), or sometimes the proportion of observation on the y axis.\n\n\n2.1 Typical histogram for continuous variable\n# Try this:\n\n# Adult domestic cats weight approximately 4 Kg on average\n# with a standard deviation of approximately +/1 0.5 Kg\n\n# Let's simulate some fake weight data for 10,000 cats\nhelp(rnorm) # A very useful function\nhelp(set.seed) # If we use this, we can replicate \"random data\"\n\nhelp(hist)\n\nset.seed(42)\ncats &lt;- rnorm(n = 10000,  # 10,000 cats\n              mean = 4,   \n              sd = 0.5)\n\ncats[1:10] # The first 10 cats\n\n [1] 4.685479 3.717651 4.181564 4.316431 4.202134 3.946938 4.755761 3.952670\n [9] 5.009212 3.968643\n\nhist(x = cats,\n     xlab = \"Cat weight (Kg)\")\n\n\n\n\n\n\n\n\n\nNotice a few things:\n\nThe bars are the count of the number of cats at each weight on the x axis\nThe width of each vertical column is a (non-overlapping) range of weights - these are called “bins” and can be defined, but usually are automatically determined based on the data\nFor count data, each bar is usually one or more integer values, rather than a range of continuous values (as it is for cat weight above in the figure)\nThe shape of a histogram can be used to infer the distribution of the data\n\n\n\n\n2.2 Sampling and populations\nRemember the concept of population versus sample? Well, let’s assume there are only 10,000 cats in the whole world and we just measured the whole population (usually not possible, remember). In this case we can calculate the exact population mean.\nWhat if we tried to estimate the real mean of our population from a sample of around 100 cats? The theory is that our sample mean would be expected to differ from the real population mean randomly. If we did a bunch of samples, most of the guesses would be close to the real population mean and less would be farther out, but all of these sample means would be expected to randomly vary, either less than or greater than the true population mean. We can examine this with a simulation of samples.\n\n# Try this:\n# simulation of samples\n\nhelp(sample) # randomly sample the cats vector\nhelp(vector) # Initialize a variable to hold our sample means\n\n# We will do a \"for loop\" with for()\n\n\nmymeans &lt;- vector(mode = \"numeric\",\n                  length = 100)\nmymeans # Empty\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\nfor(i in 1:100){\n  mysample &lt;- sample(x = cats, # Takes a random sample\n                    size = 30)\n  \n  mymeans[i] &lt;- mean(mysample) # stores sample mean in ith vector address\n  }\n\nmymeans # Our samples\n\n  [1] 3.899662 3.963986 4.019242 3.975304 3.903256 3.877346 3.911380 4.013805\n  [9] 3.989836 3.950881 4.002299 4.024990 4.091362 4.074493 3.989800 3.974894\n [17] 4.000338 3.873104 3.945492 4.061428 4.080559 4.011375 4.023977 3.863940\n [25] 4.047250 3.921947 4.049521 4.085961 3.853068 4.081517 3.987747 4.039110\n [33] 3.940955 3.954955 4.008512 3.942036 3.955110 3.968722 3.896042 3.979187\n [41] 3.957636 4.021170 4.107460 3.989197 3.931964 3.981774 4.125465 4.031625\n [49] 4.081076 3.939582 4.185512 3.997635 3.986411 3.817746 4.075256 4.074309\n [57] 4.136248 3.926092 3.976513 4.008376 3.984264 3.900717 4.138209 3.913901\n [65] 4.123326 3.894216 4.087260 4.145020 3.896286 4.142604 3.865085 4.014336\n [73] 4.053620 3.767552 3.981785 4.130483 4.165097 4.046661 4.077928 4.041611\n [81] 3.873046 4.100438 4.015098 3.947361 4.029464 4.123772 3.860191 3.820483\n [89] 4.071399 4.145585 3.982339 3.950332 3.987406 4.167345 4.126462 4.047683\n [97] 4.035958 3.987601 3.960099 3.869092\n\nhist(x = mymeans,\n     xlab = \"Mean of samples\",\n     main = \"100 cat weight samples (n = 30/sample)\")\nabline(v = mean(mymeans), col = \"red\", lty = 2, lwd = 2)\n\n\n\n\n\n\n\n\n\nNotice a few things (NB your results might look slightly different to mine - remember these are random samples):\n\nThe samples vary around the true mean of 4.0 Kg\nMost of the samples are pretty close to 4.0, fewer are farther away\nThe mean of the means is close to our true population mean\n\nTry our simulation a few more times, but vary the settings. How does adjusting the sample size (say up to 100 or down to 10)? How about the number of samples (say up to 1000 or down to 10)?",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "8 Distributions"
    ]
  },
  {
    "objectID": "8-sampling-dist.html#gaussian-that-aint-necessarily-normal",
    "href": "8-sampling-dist.html#gaussian-that-aint-necessarily-normal",
    "title": "8 Distributions",
    "section": "3 Gaussian: That ain’t necessarily normal!",
    "text": "3 Gaussian: That ain’t necessarily normal!\n\nThe Gaussian distribution is sometimes referred to as the Normal distribution. This is not a good practice: do not refer to the Gaussian distribution as the Normal distribution. Referring to the Gaussian distribution as the normal distribution implies that Gaussian is “typical”, which is patently untrue.\n\n\n- The Comic Book Guy, The Simpsons\n\n\nThe Gaussian distribution is the classic “Bell curve” shaped distribution. It is probably the most important distribution to master, because of its importance in several ways:\n\nWe expect continuous numeric variables that “measure” things to be Gaussian (e.g., human height, lamb weight, songbird wing length, Nitrogen content in legumes, etc., etc.)\nExample data: length of chicken beaks in mm\n\nR output\n# chicken beak lengths\n\n [1] 19.1 19.5 16.5 20.9 18.7 20.9 21.4 22.1 18.8 21.0 16.6 18.4 18.3 15.2 20.1 20.4\n[17] 19.3 21.5 18.5 17.3\n\n\n3.1 The Gaussian assumption (important topic)\n\nFor linear models like regression and ANOVA (we will review these models), we assume the residuals (the difference between each observation and the mean) to be Gaussian distributed and we often must test and evaluate this assumption\nThe Gaussian is described by 2 quantities: the mean and the variance\n\n\n# Example data\n(myvar &lt;- c(1,4,8,3,5,3,8,4,5,6))\n\n [1] 1 4 8 3 5 3 8 4 5 6\n\n# Mean the \"hard\" way\n(myvar.mean &lt;- sum(myvar)/length(myvar))\n\n[1] 4.7\n\n# Mean the easy way\nmean(myvar)\n\n[1] 4.7\n\n# Variance the \"hard\" way \n# (NB this is the sample variance with [n-1])\n(sum((myvar-myvar.mean)^2 / (length(myvar)-1)))\n\n[1] 4.9\n\n# Variance the easy way \nvar(myvar)\n\n[1] 4.9\n\n# Std dev the easy way\nsqrt(var(myvar))\n\n[1] 2.213594\n\n\n\n\n\n3.2 Gaussian histograms\nWe can describe the expected perfect (i.e., theoretical) Gaussian distribution based just on the mean and variance. The value of this mean and variance control the shape of the distribution.\n\n\n\n\n\n3.3 More Gaussian fun\n\n## Gaussian variations ####\n# Try this:\n\n# 4 means\n(meanvec &lt;- c(10, 7, 10, 10))\n\n[1] 10  7 10 10\n\n# 4 standard deviations\n(sdvec &lt;- c(2, 2, 1, 3))\n\n[1] 2 2 1 3\n\n# Make a baseline plot\nx &lt;- seq(0,20, by = .1)\n\n# Probabilities for our first mean and sd\ny1 &lt;- dnorm(x = x, \n            mean = meanvec[1],\n            sd = sdvec[1])\n\n# Baseline plot of 1st mean and sd\nplot(x = x, y = y1, ylim = c(0, .4),\n     col = \"goldenrod\",\n     lwd = 2, type = \"l\",\n     main = \"Gaussian fun \n     \\n mean -&gt; curve position; sd -&gt; shape\",\n     ylab = \"Density\",\n     xlab = \"(Arbitrary) Measure\")\n\n# Make distribution lines\nmycol &lt;- c(\"red\", \"blue\", \"green\")\nfor(i in 1:3){\n  y &lt;- dnorm(x = x, \n                mean = meanvec[i+1],\n                sd = sdvec[i+1])\n  lines(x = x, y = y, \n        col = mycol[i],\n        lwd = 2, type = \"l\")\n}\n\n# Add a legend\nlegend(title = \"mean (sd)\",\n       legend = c(\"10 (2)\", \"  7 (2)\", \n                  \"10 (1)\", \"10 (3)\"),\n       lty = c(1,1,1,1), lwd = 2,\n       col = c(\"goldenrod\", \"red\", \"blue\", \"green\"),\n       x = 15, y = .35)\n\n\n\n\n\n\n\n\n\n\n\n3.4 Quartile-Quartile (Q-Q) plots\nIt is very often that you might want a peek or even more formally test whether data are Gaussian. This might be in a situation when looking at, for example, the residuals for a linear model to test whether they adhere to the assumption of a Gaussian distribution. In that case, a common diagnostic graph to construct is the quantile-quantile, or “q-q”” Gaussian plot.\nThe q-q Gaussian plot your data again the theoretical expectation of the “quantile”, or percentile, were your data perfectly Gaussian (a straight, diagonal line). Remember, samples are not necessarily expected to perfectly conform to Gaussian (due to sampling error), even if the population from which the sample was taken were to be perfectly Gaussian. Thus, this is a way to confront your data with a model, to help be completely informed. The degree to which your data deviates from the line (especially systematic deviation at the ends of the line of expectation), is the degree to which is deviates from Gaussian.\n\n\n## q-q- Gaussian ####\n\n# Try This:\n\nlibrary(car) # Might need to install {car}\n\nLoading required package: carData\n\n# Set graph output to 2 x 2 grid\n# (we will set it back to 1 x 1 later)\npar(mfrow = c(2,2))  \n\n# Small Gaussian sample\nset.seed(42)\nsm.samp &lt;- rnorm(n = 10, \n                 mean = 10, sd = 2)\n\nqqPlot(x = sm.samp, \n       dist = \"norm\", # C'mon guys, Gaussian ain't normal!\n       main = \"Small sample Gaussian\")\n\n[1] 9 2\n\n# Large Gaussian sample\nset.seed(42)\nlg.samp &lt;- rnorm(n = 1000, \n                 mean = 10, sd = 2)\n\nqqPlot(x = lg.samp, \n       dist = \"norm\", \n       main = \"Large sample Gaussian\")\n\n[1] 988 980\n\n# Non- Gaussian sample\nset.seed(42)\nuni &lt;- runif(n = 50, \n                 min = 3, max = 17)\n\nqqPlot(x = uni, \n       dist = \"norm\", \n       main = \"Big deviation at top\")\n\n[1] 35 37\n\npar(mfrow = c(1,1)) # set graph grid back",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "8 Distributions"
    ]
  },
  {
    "objectID": "8-sampling-dist.html#poisson-distribution",
    "href": "8-sampling-dist.html#poisson-distribution",
    "title": "8 Distributions",
    "section": "4 Poisson distribution",
    "text": "4 Poisson distribution\n\nLife is good for only two things, discovering mathematics and teaching mathematics.\n\n\n- Simeon-Denis Poisson\n\nThe description of the Poisson distribution was credited to Simeon-Denis Poisson, a (very, very) passionate mathematician. The classic example for use is for count data, where famously it was exemplified by the number of Prussian soldiers who were killed by being kicked by a horse in a particular year.\n\n\n4.1 The Poisson distribution\n\nCount data of discrete events, objects, etc.\nIntegers, for example the number of beetles caught each day in a pitfall trap:\n\n\nrpois(20, 4)\n\n [1] 3 3 3 5 1 5 5 2 3 4 5 9 5 4 6 2 3 6 5 3\n\n\n\n\nPoisson data are typically skewed to the right\nDescribed by a single parameter, \\(\\lambda\\) (lambda), which describes the mean and the variance\n\n\nThe Poisson parameter:\n\n\n\n\n4.2 Example Poisson data\n\n# Try this:\n\n# E.g. (simulated) Number of ewes giving birth to triplets\n# The counts were made in one year 1n 100 similar flocks (&lt;20 ewes each)\n\nset.seed(42)\nmypois &lt;- rpois(n = 30, lambda = 3)\n\nhist(mypois,\n     main = \"Ewes with triplets\",\n     xlab = \"Count of Triplets\")\n\n\n\n\n\n\n\n\n\n\n\n4.3 Density plot for different Poisson lambda values\n\n# Try this:\n# 3 lambdas\n(lambda &lt;- c(1, 3, 5))\n\n[1] 1 3 5\n\n# Make a baseline plot\nx &lt;- seq(0, 15, by = 1)\n\n# Probabilities for our first lambda\ny1 &lt;- dpois(x = x, \n            lambda = lambda[1])\n\n# Baseline plot Pois\nplot(x = x, y = y1, ylim = c(0, .4),\n     col = \"goldenrod\",\n     lwd = 2, type = \"b\",\n     main = \"Poisson fun\",\n     ylab = \"Density\",\n     xlab = \"(Arbitrary) Count\")\n\n# Make distribution lines\nmycol &lt;- c(\"red\", \"blue\")\nfor(i in 1:2){\n  y &lt;- dpois(x = x, \n             lambda = lambda[i+1])\n  lines(x = x, y = y, \n        col = mycol[i],\n        lwd = 2, type = \"b\")\n}\n\n# Add a legend\nlegend(title = \"lambda\",\n       legend = c(\"1\", \"3\", \"5\"),\n       lty = c(1,1,1,1), lwd = 2,\n       col = c(\"goldenrod\", \"red\", \"blue\"),\n       x = 8, y = .35)",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "8 Distributions"
    ]
  },
  {
    "objectID": "8-sampling-dist.html#binomial",
    "href": "8-sampling-dist.html#binomial",
    "title": "8 Distributions",
    "section": "5 Binomial",
    "text": "5 Binomial\n\nWhen faced with 2 choices, simply toss a coin. It works not because it settles the question for you, but because in that brief moment when the coin is in the air you suddenly know what you are hoping for.\n\nThe Binomial distribution describes data that has exactly 2 outcomes: 0 and 1, Yes and No, True and False, etc. (you get the idea).\nExamples of this kind of data include things like flipping a coin (heads or tails), successful germination of seeds (success or failure), or binary behavioral decisions (remain or disperse)\n\n\n5.1 The Binomial distribution:\n\nData are the count of “successes”” in (binary) outcomes of a series of independent events\nData coding can be variable, but an example would be success for failure while surveying for wildlife: check this nestbox; is there at least one dormouse (Muscardinus avellanarius) in it?.\n\n\n\n5.2 Ex 1 nest boxes\nLet’s say you check 50 nest boxes, there is exactly 1 result per nest box (occupied or not), and the probability of occupancy is 30%.\n\n# Try this:\n\n# dormouse presence:\nset.seed(42)\n(my_occ &lt;- rbinom(n = 50, # Number of \"experiments\", here nestboxes checked\n       size = 1, # Number of checks, one check per nestbox\n       prob = .3)) # Probability of presence\n\n [1] 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0\n[39] 1 0 0 0 0 1 0 1 1 0 1 0\n\nmosaicplot(table(my_occ), col = c(2,'goldenrod'))\n\n\n\n\n\n\n\n\n\n\n\n5.3 Ex 2 Flipping a coin: 20 people 10 times each\n\n# Try this:\n# Flip a fair coin:\nset.seed(42)\n(coin &lt;- rbinom(n = 20, # Number of \"experiments\", 20 people flipping a coin\n       size = 10, # Number of coin flips landing on \"heads\" out of 10 flips per person\n       prob = .5)) # Probability of \"heads\"\n\n [1] 7 7 4 7 6 5 6 3 6 6 5 6 7 4 5 7 8 3 5 5\n\nmosaicplot(table(coin), col = 1:unique(coin))\n\nWarning in 1:unique(coin): numerical expression has 6 elements: only the first\nused\n\n\n\n\n\n\n\n\n\n\nDescribed by 2 parameters, The number of trials with a binary outcome in a single “experiment” (\\(n\\)), and the probability of success for each binary outcome (\\(p\\)).\n\nThe Binomial parameters:\n\n\n\n\n5.4 Density plot for different Binomial parameters\n\n# Try this:\n\n# Binomial parameters\n# 3 n of trial values\n(n &lt;- c(10, 10, 20))\n\n[1] 10 10 20\n\n# 3 probability values\n(p &lt;- c(.5, .8, .5))\n\n[1] 0.5 0.8 0.5\n\n# Make a baseline plot\nx &lt;- seq(0, 20, by = 1)\n\n# Probabilities for our first set of parameters\ny1 &lt;- dbinom(x = x, \n            size = n[1],\n            prob = p[1])\n\n# Baseline plot Binom\nplot(x = x, y = y1, ylim = c(0, .4),\n     col = \"goldenrod\",\n     lwd = 2, type = \"b\",\n     main = \"Binomial fun\",\n     ylab = \"Density\",\n     xlab = \"(Arbitrary) # \\\"Successes\\\"\")\n\n# Make distribution lines\nmycol &lt;- c(\"red\", \"blue\")\nfor(i in 1:2){\n  y &lt;- dbinom(x = x, \n             size = n[i+1],\n             prob = p[i+1])\n  lines(x = x, y = y, \n        col = mycol[i],\n        lwd = 2, type = \"b\")\n}\n\n# Add a legend\nlegend(title = \"Parameters\",\n       legend = c(\"n = 10, p = 0.50\", \n                  \"n = 10, p = 0.80\",\n                  \"n = 20, p = 0.50\"),\n       lty = c(1,1,1,1), lwd = 2, bty='n',\n       col = c(\"goldenrod\", \"red\", \"blue\"),\n       x = 11, y = .35)",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "8 Distributions"
    ]
  },
  {
    "objectID": "8-sampling-dist.html#diagnosing-the-distribution",
    "href": "8-sampling-dist.html#diagnosing-the-distribution",
    "title": "8 Distributions",
    "section": "6 Diagnosing the distribution",
    "text": "6 Diagnosing the distribution\n\nA very common task faced when handling data is “diagnosing the distribution”. Just like a human doctor diagnosing an ailment, you examine the evidence, consider the alternatives, judge the context, and take an educated guess.\n\nThere are statistical tests to compare data to a theoretical model, and they can be useful, but diagnosing a statistical distribution is principally a subjective endeavor. A common situation would be to examine the residual distribution for a regression model compared to the expected Gaussian distribution. Good practice is to have a set of steps to adhere to when diagnosing a distribution.\n\nFirst, develop an expectation of the distribution, based on the type of data\nSecond graph the data, almost always with a histogram, and a q-q plot with a theoretical quartile line for comparison\nThird, compare q-q plots with different distributions for comparison if in doubt, and if it makes sense to do so!\nIf the assumption of a particular distribution is important (like Gaussian residuals), try transformation and compare, e.g., log(your-data), cuberoot(your-data), or others, to the Gaussian q-q expectation.\n\n\nIt is beyond the intention of this page to examine all the possibilities of examining and diagnosing data distributions, but instead the intention is to alert readers that this topic is wide and deep. Here are a few good resources that can take you farther:\nVitto Ricci, Fitting distributions with R\nBill Huber, Fitting distributions to data Quick-R, Probability plots",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "8 Distributions"
    ]
  },
  {
    "objectID": "8-sampling-dist.html#practice-exercises",
    "href": "8-sampling-dist.html#practice-exercises",
    "title": "8 Distributions",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\nFor the following exercises, run the code below to create the data oibject dat.\n# The code below loads and prints the data frame \"dat\"\n\n# Data dictionary for \"dat\", a dataset with different measures of 20 sheep\n# weight - weight in Kg\n# ked - count of wingless flies\n# trough - 2 feed troughs, proportion of times \"Trough A\" fed from\n# shear - minutes taken to \"hand shear\" each sheep\n\n(dat &lt;- data.frame(\n  weight = c(44.1, 38.3, 41.1, 41.9, 41.2, 39.7, 44.5, 39.7, 46.1, 39.8, \n    43.9, 46.9, 35.8, 39.2, 39.6, 41.9, 39.1, 32.0, 32.7, 44.0),\n  \n  ked = c(9, 4, 15, 11, 10, 8, 12, 12, 6, 11,\n             12, 13, 8, 11, 19, 19, 12, 7, 8, 14),\n  \n  trough = c(0.52, 0.74, 0.62, 0.63, 0.22, 0.22, 0.39, 0.94, 0.96, 0.74,\n              0.73, 0.54, 0.00, 0.61, 0.84, 0.75, 0.45, 0.54, 0.54, 0.00),\n  \n  shear = c(14.0, 8.0, 14.0, 11.0, 14.0, 5.0, 9.5, 11.0, 6.5, 11.0, \n            18.5, 11.0, 18.5, 8.0, 8.0, 6.5, 18.5, 15.5, 14.0, 8.0)\n  ))\n\n\n7.1\nDo you expect weight to be Gaussian distributed? How about ked? Explain your answer for each. \n\n\n7.2\nShow the code to graphically diagnose and decide whether weight is Gaussian and explain your conclusion.\n\n\n\n7.3\nShow the code to graphically diagnose and decide whether ked is Gaussian and explain your conclusion. If you choose another likely distribution, test it as well and similarly diagnose.\n\n\n\n7.4\nExplore whether trough is Gaussian, and explain whether you expect it to be so. If not, does transforming the data “persuade it” to conform to Gaussian? Discuss.\n\n\n\n7.5\nWrite a plausible practice question involving any aspect of graphical diagnosis of a data distribution using the iris data.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "8 Distributions"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the Bootcamp",
    "section": "",
    "text": "Data science skills for research",
    "crumbs": [
      "Home",
      "Getting Started",
      "About the Bootcamp"
    ]
  },
  {
    "objectID": "about.html#what-is-the-r-stats-bootcamp",
    "href": "about.html#what-is-the-r-stats-bootcamp",
    "title": "About the Bootcamp",
    "section": "What is the R Stats Bootcamp?",
    "text": "What is the R Stats Bootcamp?\nThe R Stats Bootcamp is a comprehensive, self-paced course designed for researchers and students who want to develop practical data science skills. Our mission is to provide accessible, high-quality training in R programming, statistical analysis, and reproducible research practices.\n\n\n\n\n\n\nOur Philosophy\n\n\n\nWe believe that data science should be: - Accessible to researchers at all levels - Practical with a focus on real-world applications - Reproducible to advance scientific integrity - Collaborative to encourage knowledge sharing",
    "crumbs": [
      "Home",
      "Getting Started",
      "About the Bootcamp"
    ]
  },
  {
    "objectID": "about.html#who-this-course-is-for",
    "href": "about.html#who-this-course-is-for",
    "title": "About the Bootcamp",
    "section": "Who This Course Is For",
    "text": "Who This Course Is For\nThis bootcamp is ideal for:\n\nResearch students beginning their data analysis journey\nAcademic staff looking to update their quantitative skills\nProfessionals transitioning to data-driven roles\nSelf-learners interested in R programming and statistics\n\nNo prior programming experience is required—we’ll guide you from the basics to more advanced concepts at a comfortable pace.",
    "crumbs": [
      "Home",
      "Getting Started",
      "About the Bootcamp"
    ]
  },
  {
    "objectID": "about.html#what-youll-learn",
    "href": "about.html#what-youll-learn",
    "title": "About the Bootcamp",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\n\nModule 1: R Foundations\nMaster the fundamentals of R programming and the RStudio environment. Learn to manipulate data, create functions, and work with different data structures.\n\n\nModule 2: Statistical Analysis\nApply your R skills to statistical analysis. Explore data visualization, hypothesis testing, correlation, regression, and more.\n\n\nModule 3: Reproducible Research\nDiscover tools and practices for reproducible science. Learn R Markdown for dynamic documents, Git for version control, and collaborative workflows.",
    "crumbs": [
      "Home",
      "Getting Started",
      "About the Bootcamp"
    ]
  },
  {
    "objectID": "about.html#learning-approach",
    "href": "about.html#learning-approach",
    "title": "About the Bootcamp",
    "section": "Learning Approach",
    "text": "Learning Approach\nThe R Stats Bootcamp follows a learning-by-doing approach:\n\nSelf-paced lessons that fit your schedule\nHands-on exercises to reinforce concepts\nReal-world examples relevant to research contexts\nProgressive difficulty that builds on previous knowledge\nMultiple learning modalities to accommodate different learning styles",
    "crumbs": [
      "Home",
      "Getting Started",
      "About the Bootcamp"
    ]
  },
  {
    "objectID": "about.html#course-development",
    "href": "about.html#course-development",
    "title": "About the Bootcamp",
    "section": "Course Development",
    "text": "Course Development\nThis bootcamp is continuously improved based on:\n\nCurrent best practices in data science education\nFeedback from learners and instructors\nAdvances in R packages and statistical methods\nResearch on effective teaching and learning",
    "crumbs": [
      "Home",
      "Getting Started",
      "About the Bootcamp"
    ]
  },
  {
    "objectID": "about.html#get-support",
    "href": "about.html#get-support",
    "title": "About the Bootcamp",
    "section": "Get Support",
    "text": "Get Support\nWe offer several ways to get help during your learning journey:\n\nJoin our Slack community for peer support\nCheck the FAQ section for common questions\nExplore additional resources linked throughout the course",
    "crumbs": [
      "Home",
      "Getting Started",
      "About the Bootcamp"
    ]
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "R Stats Bootcamp",
    "section": "",
    "text": "MIT License\nCopyright (c) 2025 W. Ed Harris\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  }
]