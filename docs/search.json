[
  {
    "objectID": "6-data-manipulation.html",
    "href": "6-data-manipulation.html",
    "title": "6 Data subsetting and manipulation",
    "section": "",
    "text": "Command data and you are powerful"
  },
  {
    "objectID": "6-data-manipulation.html#subsetting-and-manipulation-data-sumo",
    "href": "6-data-manipulation.html#subsetting-and-manipulation-data-sumo",
    "title": "6 Data subsetting and manipulation",
    "section": "1 Subsetting and Manipulation (data sumo)",
    "text": "1 Subsetting and Manipulation (data sumo)\n\nWith a good basic set of moves for subsetting and manipulating data, you can overpower any dataset no matter how large and powerful they may be. Then, you will have strong data Sumo.\n\nSubsetting and manipulating data is probably the commonest activity for anyone who works with data. This is a core activity for exploratory data analysis, but is also extensively used in simple data acquisition, analysis and graphing, while also being related to more general data manipulating activities, for example database queries. This page is an introduction to the core syntax and some of the tools for manipulating and subsetting data in R.\n\n\n1.1 Objectives\n\nIndexing concept\nUsing which() and subsetting\nSelection on data.frame objects\nUsing aggregate()\nPractice exercises"
  },
  {
    "objectID": "6-data-manipulation.html#indexing-concept",
    "href": "6-data-manipulation.html#indexing-concept",
    "title": "6 Data subsetting and manipulation",
    "section": "2 Indexing concept",
    "text": "2 Indexing concept\n\nIf you would like to slice and dice your data, you will need to learn all about indexing!\n\nThe basics of the indexing concept in R syntax is very simple, where data storage objects like vectors (1 dimension), matrices (2 dimensions) and arrays (3 or more dimensions) store individual data values that can be accessed by the “address” of the dimension(s).\n\n\n2.1 How indexing works\nSay you have a numeric vector called my_vector that has 10 values. The index values will be 1 to 10, with each value corresponding consecutively to the data value at that position.\nmy_vector <- c(11.3, 11.2, 10.4, 10.4, 8.7, \n               10.8, 10.5, 10.3, 9.7, 11.2)\n  \nmy_vector\n[1] 11.3 11.2 10.4 10.4 8.7 10.8 10.5 10.3  9.7 11.2\n\nNotice the [1] in the R console output? This indicates the index of value right next to it and the R system will provide an index value for longer vectors as the wrap in the console. If we could see the actual index values it would look something like this:\n> my_vector\n [1] 11.3 11.2 10.4 10.4  8.7 10.8 10.5 10.3  9.7 11.2\n#     ^    ^    ^    ^    ^    ^    ^    ^    ^    ^\n#     1    2    3    4    5    6    7    8    9    10\n\n\n\n2.2 Vectors\nYou can create vector subsets by manipulating the index. Vector objects have indices in 1 dimension. For example, my_vector[1:i], where i is the length of the vector.\n## Vectors ####\n# Try this\n\nmy_vector <- c(11.3, 11.2, 10.4, 10.4, \n               8.7, 10.8, 10.5, 10.3, 9.7, 11.2)\n\n# Return all values\nmy_vector        # Typical way\nmy_vector[ ]     # Blank index implies all index values\nmy_vector[ 1:10] # Returns all index values explicitly\n\n# Return the first 3 values\n1:3 # Reminder of the function of the colon operator \":\"\nmy_vector[ 1:3] # Notice consecutive indices can use the \":\" operator\n\n# Return 5th and 9th values\nmy_vector[ c(5, 9)] # Notice we have to place non-consecutive index values in the c() function\n\n\n\n2.3 Matrices\nMatrix objects have 2 dimensions denoted as my_matrix[1:i, 1:j], where i is the number of rows and j is the number of columns.\n## Matrices ####\n# Try this\n\nmy_matrix <- matrix(data = c(2,3,4,5,6,6,6,6),\n                    nrow = 2, byrow = T)\n\nmy_matrix # notice how the arguments arranged the data\n\n# Flash challenge: make a matrix with the same data vector above to look like...\n#      [,1] [,2]\n# [1,]    2    6\n# [2,]    3    6\n# [3,]    4    6\n# [4,]    5    6\n\n# \"Slicing\" out a row or column\nmy_matrix[1,  ] # Slice out row 1\nmy_matric[ , 3] # Slice out column 3\n\n# Matrix columns and rows often have names\nnames(my_matrix) # No names yet\n\nnrow(my_matrix) # Returns number of rows (useful for large matrices)\nrownames(my_matrix) # No row names; 2 rows, need two names\n\nrownames(my_matrix) <- c(\"dogs\", \"cats\")\nmy_matrix # Now the rows have names!\nrownames(my_matrix) # Get them this way too!\n\n# Flash challenge: Name the columns of my_matrix \"a\", \"b\", \"c\", \"d\" with colnames()\n\nmy_matrix\n\n# Should look like this:\n#      a b c d\n# dogs 2 3 4 5\n# cats 6 6 6 6\n\n# You can also slice out matrix portions by name\nmy_matrix[\"dogs\", c(\"b\", \"d\")]\n\n# Finally, functions act on values, not the index\nmean(my_matrix[\"dogs\", c(\"b\", \"d\")])\n\n\n\n2.4 Arrays\nArrays are data objects with more than 2 dimensions (well, technically a matrix with 2 dimensions is also an array, but let’s ignore that for now). Array dimensions are denoted as my_array[1:i, 1:j, 1:k], where i is the number of rows and j the columns and k the “depth” of i * j.\n\n\n\nmy_array\n\n\n## Arrays ####\n# Try this\n\n# help(runif)\n# help(round)\n# Try it to see what it does... \nmy_vec <- round(runif(n = 27, min = 0, max = 100), 0)\nmy_vec # See what we did there?\n\nlength(my_vec) # Just checking\n\nmy_array <- array(data = my_vec,\n                  dim = c(3, 3, 3))\nmy_array\n\n# Flash challenge: \n# Specify and print the 1st and 3rd  slice of the k dimension of my_array\n# Assuming my_array has dimensions i, j, k like my_array[i,j,k]"
  },
  {
    "objectID": "6-data-manipulation.html#which-and-subsetting",
    "href": "6-data-manipulation.html#which-and-subsetting",
    "title": "6 Data subsetting and manipulation",
    "section": "3 which() and subsetting",
    "text": "3 which() and subsetting\nSubsetting data objects is done by exploiting the index system. We usually do this by either specifying index values explicitly (effective, but it requires that you know A LOT about the data object), or by constructing queries that choose subset of data based on particular values. The which() function is a powerful way to construct queries.\n# Try this\nhelp(which) # Notice how the x argument is required to be a LOGICAL vector?\n\n# Make a NUMERIC vector\nvector_a <- c(3, 4, 5, 4, 3, 4, 5, 6, 6, 7)\n\n# Use a boolean phrase to ask which elements of vector_a are greater than 5\nvector_a > 5 # Interesting... it is a LOGICAL vector!\n\n# which() will return the index values of TRUE values\n# In other words, WHICH values in vector_a are greater than 5?\nwhich(vector_a > 5)\n\nWhat is the point of all this? THE POINT is to be able to use expressions to obtain indices and values in data structures…\n# What VALUES in vector_a are > 5?\nvector_a[which(vector_a > 5)]\n\n# This also works on vectors of other types\n# Consider a character vector\nchar_vec <- c(\"wheat\", \"maize\", \"wheat\", \"maize\", \"wheat\", \"wheat\")\n\n# Which elements are equivalent to \"wheat\"?\nchar_vec == \"wheat\"\nwhich(char_vec == \"wheat\")\n\nchar_vec[ which(char_vec == \"wheat\")] # This works\nchar_vec[ char_vec == \"wheat\"]        # Same output\n\n# Flash challenge: Explain in your own words why \n# the previous 2 lines of code have identical output?\n\nWe are just beginning to scratch the surface of possibilities with the which() function. Keep this function in mind and practice it when you can."
  },
  {
    "objectID": "6-data-manipulation.html#selection-on-data.frame-objects",
    "href": "6-data-manipulation.html#selection-on-data.frame-objects",
    "title": "6 Data subsetting and manipulation",
    "section": "4 Selection on data.frame objects",
    "text": "4 Selection on data.frame objects\n\nData frames are the ultimate data object for getting, storing, organizing and analyzing data. A good scientist must learn to communicate the subtlety of data. A good statistician must learn not to underestimate the subtletly of data. A good student must learn that\n\nsubtlety may exist, even in simple data.\nThere are a couple of data object types that have a special characteristic in that they store data of different types, where vectors, matrices and arrays can only store one type of data (e.g., numeric, character, logical, etc.). The special data objects that can contain multiple data types are list objects, and data frames. Here we will focus on data frames.\nData frames can have different vector types arranged by column but there is a constraint that each vector must be the same length, that is, each ROW is considered an observation for each variable value (though there may be missing data coded by NA).\nThere are a few ways to think about selecting values in a data frame. The first is simply to access values through the variable names, which can either be done by using the data frame name with the $ operator and the variable name, or by using the [ , ] syntax with either the variable name or the column number of the variable of interest see here.\nA second powerful way to access variables in a data frame is by selecting particular rows of a data frame. This may be done by selecting the rows of a data based on values of one or more variables. We will practice doing this using which(), the [ , ] syntax, and boolean phrases is the following code block.\nFor the following section, we will use the OrchardSprays dataset that exists as a data frame in the in-built {datasets} package. You can use help(OrchardSprays) to see the help page (the help page is characteristically terse, so some description is given here).\n\n\n4.1 OrchardSprays data\nThis is a classic dataset based on an experiment looking at how a chemical additive could be used to deter honeybees from being attracted to crops and subsequently killed by pesticides.\nThe experiment involved having a treatment consisting of adding a “lime sulfur emulsion” (honeybee deterrent) in increasing concentrations to a sucrose solution. The treatment variable had 8 levels including a control (no deterrent) and 7 other levels with increasing concentration of the deterrent. The treatment levels were named A (the highest amount of deterrent), B (second highest deterrent) through to G (lowest deterrent) and H (control - no deterrent) The decrease variable was a measure of the quantity of sucrose solution that was taken by honeybees (the prediction here is that higher concentrations of the deterrent should result in a lower decrease in the sucrose solution).\nThe experiment involved a Latin Square design, with the order of the 8 treatments arranged randomly in an array of 8 columns (the purpose of this design is to randomize any effect of the treatment ORDER or POSITION on the response variable). This resulted in an 8 row by 8 column experiment. The response was measured after placing 100 honeybees into an experimental chamber with the 64 containers of sucrose solution.\n\n## OrchardSprays ####\n## Understand the data - an important step\n\n# Try this\n# Load the OrchardSpray data using the data() function\ndata(OrchardSprays) # Should see OrchardSprays <promise> in the Global Env.\n\n# Look at the data head()\nhead(OrchardSprays) # First 6 rows\n\n# Look at variable types with str()\nhelp(str) # Good function to see info about data object\nstr(OrchardSprays)\n\n# First let's just look at the data\n# Don't worry too much about the code for these graphs if you have not encountered it before\nboxplot(decrease ~ treatment, data = OrchardSprays, \n        main = \"The pattern fits the prediction\",\n        ylab = \"Amount of sucrose consumed\",\n        xlab = \"Lime sulpher treatment amount in decreasing order (H = control)\")\n\n# This is the experimental design\n# Latin Square is kind of like Sudoku\n# No treatment can be in row or column more than once\nplot(x = OrchardSprays$colpos,  # NB use of $ syntax to access data\n     y = OrchardSprays$rowpos, \n     pch = as.character(OrchardSprays$treatment),\n     xlim = c(0,9), ylim = c(0,9),\n     main = \"The Latin Square design of treatments\",\n     xlab = \"\\\"Column\\\" position\",\n     ylab = \"\\\"Row\\\" position\")\n\n\n\n4.2 Practice selecting parts a data frame\nSelecting particular parts of a data frame based on the values of one variable is a common and extremely useful task.\n## Practice selecting parts a data frame ####\n\n# Select the rows of the dataset for treatment \"D\"\n\n# (Pseudocode steps to solve) \n# Break it down to make small steps easy to read\n\n# 01 Boolean phrase to identify rows where treatment value is \"D\"\n# 02 which() to obtain index of TRUE in boolean vector\n# 03 Exploit [ , ] syntax with data frame object to slice out rows\n\n# 01 Boolean phrase\nOrchardSprays$treatment # Just print variable to compare visually to boolean\nOrchardSprays$treatment == \"D\" # logical vector - TRUE in \"D\" positions\n\n# 02 which()\nwhich(OrchardSprays$treatment == \"D\") # Index of TRUE values\nmy_selection <- which(OrchardSprays$treatment == \"D\") # Place index in a variable\nmy_selection # Just checking\n\n# 03 Exploit [ , ] syntax with data frame object to slice out rows\nOrchardSprays[my_selection, ]\n\n# Flash challenge: Select and print all rows at \"colpos\" values of 2\n\n\n\n4.3 Selection based on more than one variable value\nUsing the basic building blocks of boolean selection, more complex rules for selecting data can be made.\n## Compound boolean for selection ####\n\n# Select all rows of the data frame where \n# rowpos equals 4 OR 6 AND treatment equals \"A\" OR \"H\"\n# What we expect is exactly 2 values (A or H) for each powpos (4 or 6)\n\n# rowpos 4 and 6\nOrchardSprays$rowpos == 4 # The 4s\nOrchardSprays$rowpos == 6 # The 6s\n\nOrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6 # All together\n\n# now with which()\nwhich(OrchardSprays$rowpos == 4) # The 4s\nwhich(OrchardSprays$rowpos == 6) # The 6s\n\nwhich(OrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6) # All together\n\n# treatment A and H\nwhich(OrchardSprays$treatment == \"A\" | OrchardSprays$treatment == \"H\") # All together\n\n# Now we need the intersection of value that are in both our which() vectors\n\nwhich((OrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6) &  # It works\n        (OrchardSprays$treatment == \"A\" | OrchardSprays$treatment == \"H\") ) \n  \n# NB this is a long way of spelling out our selection, \n# but trying to be very explicit with what is going on\n\nmy_selec2 <- which((OrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6) &  \n                     (OrchardSprays$treatment == \"A\" | OrchardSprays$treatment == \"H\") ) \n\nOrchardSprays[my_selec2, ] # Double check it works and is similar to expectation...\n\n# Flash challenge: Calculate the mean of decrease for treatment \"A\" \n# and the mean of decrease for treatment \"H\""
  },
  {
    "objectID": "6-data-manipulation.html#harper-adams-data-science",
    "href": "6-data-manipulation.html#harper-adams-data-science",
    "title": "6 Data subsetting and manipulation",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module supports students and staff at Harper Adams University and the MSc in Data Science for Global Agriculture, Food, and Environment led by Ed Harris."
  },
  {
    "objectID": "slides-overview.html",
    "href": "slides-overview.html",
    "title": "R Bootcamp Launch",
    "section": "",
    "text": "�\n\nhttps://dsgarage.netlify.app/"
  },
  {
    "objectID": "slides-overview.html#what-is-ds-garage-about",
    "href": "slides-overview.html#what-is-ds-garage-about",
    "title": "R Bootcamp Launch",
    "section": "What is DS garage about?",
    "text": "What is DS garage about?\n�\n\nModern tools for thinking about data :: evidence"
  },
  {
    "objectID": "slides-overview.html#traditional-bootcamp",
    "href": "slides-overview.html#traditional-bootcamp",
    "title": "Bootcamp overview",
    "section": "Traditional Bootcamp",
    "text": "Traditional Bootcamp\n\nDifficult, unfriendly,\nweed out the weak"
  },
  {
    "objectID": "slides-overview.html#eds-r-bootcamp",
    "href": "slides-overview.html#eds-r-bootcamp",
    "title": "R Bootcamp Launch",
    "section": "Ed’s R Bootcamp",
    "text": "Ed’s R Bootcamp\n�\n\nWorking together to do something good and worthwhile"
  },
  {
    "objectID": "slides-overview.html#why-r-bootcamp",
    "href": "slides-overview.html#why-r-bootcamp",
    "title": "R Bootcamp Launch",
    "section": "Why R bootcamp?",
    "text": "Why R bootcamp?\n�\n\nThe purpose of the bootcamp is to\n\n�\n\nbe a SELF-GUIDED resource to\nlearn basic R programming and basic statistics\nassume no prior knowledge"
  },
  {
    "objectID": "slides-overview.html#who-is-it-for",
    "href": "slides-overview.html#who-is-it-for",
    "title": "R Bootcamp Launch",
    "section": "Who is it for?",
    "text": "Who is it for?\n�\n\nAnyone who wants to learn from data\nAcademic colleagues who want to learn R\nLearning data scientists\nOther interesting people"
  },
  {
    "objectID": "slides-overview.html#what-will-you-learn",
    "href": "slides-overview.html#what-will-you-learn",
    "title": "Bootcamp overview",
    "section": "What will you learn",
    "text": "What will you learn\n\n\nR scripting basics\nBest practice for reproducible research\nTidy data practice\nA practical review of basic statistics (just enough…)"
  },
  {
    "objectID": "slides-overview.html#how-is-it-organised-httpsdsgarage.netlify.appbootcamp",
    "href": "slides-overview.html#how-is-it-organised-httpsdsgarage.netlify.appbootcamp",
    "title": "R Bootcamp Launch",
    "section": "How is it organised? https://dsgarage.netlify.app/bootcamp/",
    "text": "How is it organised? https://dsgarage.netlify.app/bootcamp/\n�\nModule 1 (pages 1.x): A bare-bones introduction to R and Rstudio for beginners\nModule 2 (pages 2.x): A bare-bones introduction to using R for traditional data analysis\nModule 3 (pages 3.x): An introduction to reproducible code, R Markdown, and Github"
  },
  {
    "objectID": "slides-overview.html#how-is-it-organised",
    "href": "slides-overview.html#how-is-it-organised",
    "title": "R Bootcamp Launch",
    "section": "How is it organised?",
    "text": "How is it organised?\n\nModule 1"
  },
  {
    "objectID": "slides-overview.html#how-is-it-organised-1",
    "href": "slides-overview.html#how-is-it-organised-1",
    "title": "R Bootcamp Launch",
    "section": "How is it organised?",
    "text": "How is it organised?\n\nModule 2"
  },
  {
    "objectID": "slides-overview.html#how-is-it-organised-2",
    "href": "slides-overview.html#how-is-it-organised-2",
    "title": "R Bootcamp Launch",
    "section": "How is it organised?",
    "text": "How is it organised?\n\nModule 3"
  },
  {
    "objectID": "slides-overview.html#coding",
    "href": "slides-overview.html#coding",
    "title": "Bootcamp overview",
    "section": "Coding",
    "text": "Coding\n\n\nIt is important to type and run code for practice"
  },
  {
    "objectID": "slides-setup.html",
    "href": "slides-setup.html",
    "title": "R Bootcamp RStudio & R Scripts",
    "section": "",
    "text": "https://dsgarage.netlify.app/bootcamp/0.1-bootcamp-intro/"
  },
  {
    "objectID": "slides-setup.html#what-will-you-learn",
    "href": "slides-setup.html#what-will-you-learn",
    "title": "R Bootcamp RStudio & R Scripts",
    "section": "What will you learn?",
    "text": "What will you learn?\n \n\nInstall R and RStudio\nPractice of good, reproducible scripting\nDesign vision of learning materials: Read the pages and type and run all code"
  },
  {
    "objectID": "slides-setup.html#tour-of-rstudio-interface",
    "href": "slides-setup.html#tour-of-rstudio-interface",
    "title": "R Bootcamp RStudio & R Scripts",
    "section": "Tour of RStudio interface",
    "text": "Tour of RStudio interface"
  },
  {
    "objectID": "slides-setup.html#reproducible-script",
    "href": "slides-setup.html#reproducible-script",
    "title": "R Bootcamp RStudio & R Scripts",
    "section": "Reproducible script",
    "text": "Reproducible script"
  },
  {
    "objectID": "slides-overview.html#r-stats-bootcamp",
    "href": "slides-overview.html#r-stats-bootcamp",
    "title": "Bootcamp overview",
    "section": "R Stats bootcamp",
    "text": "R Stats bootcamp\n\n\nhttps://rstats-bootcamp.github.io/website/"
  },
  {
    "objectID": "slides-overview.html#what-is-the-r-stats-bootcamp-about",
    "href": "slides-overview.html#what-is-the-r-stats-bootcamp-about",
    "title": "Bootcamp overview",
    "section": "What is the R Stats Bootcamp about?",
    "text": "What is the R Stats Bootcamp about?\n\n\nModern tools for thinking about data :: evidence :: claims"
  },
  {
    "objectID": "slides-overview.html#r-stats-bootcamp-1",
    "href": "slides-overview.html#r-stats-bootcamp-1",
    "title": "Bootcamp overview",
    "section": "R Stats Bootcamp",
    "text": "R Stats Bootcamp\n\nWork hard, do something good with others,\nhelp everyone succeed"
  },
  {
    "objectID": "slides-overview.html#purpose-of-r-stats-bootcamp",
    "href": "slides-overview.html#purpose-of-r-stats-bootcamp",
    "title": "Bootcamp overview",
    "section": "Purpose of R Stats Bootcamp",
    "text": "Purpose of R Stats Bootcamp\n\nProvide a self-guided resource to:\n\nLearn or refresh skills in R scripting, basic statistics\nBoost success in statistics for assessments, project work\nBe accessible with no prior knowledge"
  },
  {
    "objectID": "slides-overview.html#who-is-the-r-stats-bootcamp-for",
    "href": "slides-overview.html#who-is-the-r-stats-bootcamp-for",
    "title": "Bootcamp overview",
    "section": "Who is the R Stats Bootcamp for?",
    "text": "Who is the R Stats Bootcamp for?\n\n\nStudents in applied sciences\nAspiring data scientists\nAcademic colleague who want to learn R\nPostgraduate researchers\nOther nteresting people (who want to learn from data)"
  },
  {
    "objectID": "slides-overview.html#r-stats-bootcamp-organization",
    "href": "slides-overview.html#r-stats-bootcamp-organization",
    "title": "Bootcamp overview",
    "section": "R Stats Bootcamp organization",
    "text": "R Stats Bootcamp organization\n\n\nhttps://rstats-bootcamp.github.io/website/\n\n\nModule 1: Introduction to R and Rstudio\nModule 2: R for traditional data analysis\nModule 3: Reproducible code, R Markdown, and GithuB"
  },
  {
    "objectID": "slides-overview.html#r-stats-bootcamp-organization-1",
    "href": "slides-overview.html#r-stats-bootcamp-organization-1",
    "title": "Bootcamp overview",
    "section": "R Stats Bootcamp organization",
    "text": "R Stats Bootcamp organization\n\nModule 1: R scripting\n1.1 R syntax\n1.2 Functions and packages\n1.3 Data objects\n1.4 Data frames\n1.5 Data sub-setting and manipulation"
  },
  {
    "objectID": "slides-overview.html#r-stats-bootcamp-organization-2",
    "href": "slides-overview.html#r-stats-bootcamp-organization-2",
    "title": "Bootcamp overview",
    "section": "R Stats Bootcamp organization",
    "text": "R Stats Bootcamp organization\n\nModule 2: Statistics review\n2.1 Question, explore, analyze\n2.2 Sampling distributions\n2.3 Correlation\n2.4 Simple linear regression\n2.5 T-test\n2.6 1-way ANOVA"
  },
  {
    "objectID": "slides-overview.html#r-stats-bootcamp-organization-3",
    "href": "slides-overview.html#r-stats-bootcamp-organization-3",
    "title": "Bootcamp overview",
    "section": "R Stats Bootcamp organization",
    "text": "R Stats Bootcamp organization\n\nModule 3: reproducibility, collaboration\n3.1 Reproducibility\n3.2 Automate reports\n3.3 Git and version control"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "“Data science”"
  },
  {
    "objectID": "index.html#r-stats-bootcamp",
    "href": "index.html#r-stats-bootcamp",
    "title": "About",
    "section": "R Stats Bootcamp",
    "text": "R Stats Bootcamp\nThis is a self-guided tutorial designed for people new to data science, statistics and R, and for those who would like a review. The materials can help you get going with critical skills in R programming, traditional data analysis and open science tools. The aim is to provide open, foundational training you can build on in the most efficient format possible. This will help prepare you for further training and to get you working with data in R as quickly as possible.\nThe Bootcamp is self-paced and has built-in assessment materials. The material is meant to work through in order as an experience, but is not in any way intended to be a reference to the presented topics."
  },
  {
    "objectID": "index.html#bootcamp-modules",
    "href": "index.html#bootcamp-modules",
    "title": "About",
    "section": "Bootcamp modules",
    "text": "Bootcamp modules\nModule 1 Introduction to R scripting and the RStudio interface for beginners\nModule 2 Introduction to using R for simple data analysis\nModule 3 Introduction to reproducible code, R Markdown, and Github.\nThese pages will help orient you if you are a beginner, and we will also cover our design approach for the materials and the tools you will need to get started right away. If you are already an RStudio user and comfortable managing and storing script files and scripts in R, you may want to jump straight to the exercises on each page.\n\nWe make a few suggestions about this material:\n\nFollow along consecutively with the workshop pages\nActually type and run all of the example code yourself\nComplete each page before moving on to the next\nFormally attempt all problems at the end of each page\nJoin us in Slack if you need assistance\n\n\nThat’s it!"
  },
  {
    "objectID": "index.html#harper-adams-data-science",
    "href": "index.html#harper-adams-data-science",
    "title": "About",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module supports students and staff at Harper Adams University and the MSc in Data Science for Global Agriculture, Food, and Environment led by Ed Harris."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "I call this “Robot farmer with calendar”"
  },
  {
    "objectID": "schedule.html#bootcamp-resources",
    "href": "schedule.html#bootcamp-resources",
    "title": "Schedule",
    "section": "Bootcamp resources",
    "text": "Bootcamp resources\nThis material is intended to require a significant time commitment to complete (most students report about 20 hours at minimum). The schedule below is intended to be a concise overview of the different resources for the Bootcamp. Some pages have associated slides and videos linked here.\nFor all of the lab pages it is intended that you type and run 100% of the code in examples and then explicitly answer the exercise questions, which are based on examples for given code on the pages. Thus, to answer the questions it is recommended to adapt code you have already run when completing the pages.\n\n\n\n\nTopics\n\n\n\n\n  Bootcamp overview\n\n\nModule 1: R scripting\n1 R and RStudio Setup\n2 R language\n3 Functions and packages\n4 Data objects\n5 Data frames\n6 Data sub-setting and manipulation\n\n\nModule 2: Statistics review\n7 Question, explore, analyze\n8 Sampling distributions\n9 Correlation\n10 Simple linear regression\n11 T-test\n12 1-way ANOVA\n\n\nModule 3: reproducibility, collaboration\n13 Reproducibility\n14 Automate reports\n15 Git and version control"
  },
  {
    "objectID": "schedule.html#harper-adams-data-science",
    "href": "schedule.html#harper-adams-data-science",
    "title": "Schedule",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module supports students and staff at Harper Adams University and the MSc in Data Science for Global Agriculture, Food, and Environment led by Ed Harris."
  },
  {
    "objectID": "1-setup.html#bootcamp-introduction-first-day-with-r",
    "href": "1-setup.html#bootcamp-introduction-first-day-with-r",
    "title": "1 Setup & intro",
    "section": "1 Bootcamp Introduction (first day with R)",
    "text": "1 Bootcamp Introduction (first day with R)\n\nThis page is intended to guide people during their first installation and use of R and RStudio.\n\n\n1.1 Objectives\nHere is what we will work on:\n\nHow the R Stats Bootcamp works\nR motivation\nInstall R and RStudio or set up RStudio Cloud\nRStudio components and setup\nWorkflow for scripts in R\nPractice exercises"
  },
  {
    "objectID": "1-setup.html#how-the-r-stats-bootcamp-works",
    "href": "1-setup.html#how-the-r-stats-bootcamp-works",
    "title": "1 Setup & intro",
    "section": "2 How the R Stats Bootcamp works",
    "text": "2 How the R Stats Bootcamp works\nThe R Stats Bootcamp aims to provide practical, open instructional materials to support learning the R programming language, to review simple statistics in R, and to introduce reproducibility and collaboration tools. The content is a blend of practical, referenced material with videos and self-assessment.\nWe also a have a friendly community Slack channel - go over and introduce yourself and say hi!"
  },
  {
    "objectID": "1-setup.html#r-motivation",
    "href": "1-setup.html#r-motivation",
    "title": "1 Setup & intro",
    "section": "3 R motivation",
    "text": "3 R motivation\nThe motivation for using R is that it is designed to help people with no programming experience to perform sophisticated statistical analysis with minimum effort. R has grown in popularity recently and is used extensively by universities, companies, and researchers everywhere. Because of this, there is a very large community of users and a demand in business and academia for skills using R.\nR is free and open source. R is easy to learn and works the same for folks with fast and slow computers, no matter what kind of operating system or computer they like to use, and it is easy to use via the web on any device."
  },
  {
    "objectID": "1-setup.html#install-r-and-rstudio-or-set-up-rstudio-cloud",
    "href": "1-setup.html#install-r-and-rstudio-or-set-up-rstudio-cloud",
    "title": "1 Setup & intro",
    "section": "4 Install R and RStudio or set up RStudio Cloud",
    "text": "4 Install R and RStudio or set up RStudio Cloud\nYou have two options for following along with these materials as they are intended.\n\nOption 1 Download and install R from CRAN and then download and install RStudio desktop.\nInstall R first, then RStudio. It is probably a good idea to go ahead and install the latest version of each if you have older versions installed. If you have a PC or laptop you regularly use, this option is probably best and will work for almost all hardware and operating systems.\nHelp for Windows\nHelp for Macs\nHelp for Linux\n\nOption 2 If you can’t install R or do not wish to, or if you prefer to work in “the cloud” from a web browser, you may wish to start a free account at RStudio Cloud and follow along that way."
  },
  {
    "objectID": "1-setup.html#rstudio-components-and-setup",
    "href": "1-setup.html#rstudio-components-and-setup",
    "title": "1 Setup & intro",
    "section": "5 RStudio components and setup",
    "text": "5 RStudio components and setup\nRStudio desktop is an environment to write R code, perform statistical analysis, organize big or small projects with multiple files, and view and organize outputs. There are many features of RStudio, but we are only going to point out a few. One of the most useful features is syntax highlighting, that gives visual cues to help you write computer code.\n\n\n\n\nRStudio layout\n\n\n\nBe aware (beware?) of:\nThe Script window\nThe script window is located in the upper right of the RStudio console by default. You may need to open a script or start a new one: File > New File > R Script (hotkey Ctrl+Shift+N).\nThe script window is where you are likely to spend most of your time building scripts and executing commands you write. You can have many scripts open at the same time (in “tabs”), and you can have different kinds of scripts, e.g., for different parts of a project or even for programming languages.\n\nThe Console window\nThe Console window is in the lower left by default. Notice there are several other tabs visible, but we will only mention the Console for now. The Console is the place where text outputs will be printed (e.g. the results of statistical tests), and also is a place where R will print Warning and Error messages.\n\nThe Global Environment\nThe Global Environment is in the Environment tab in the upper right of RStudio by default. This pane is useful in displaying data objects that you have loaded and available.\n\nThe Plots window\nThe Plots window is a tab in the lower right by default. This is the place where graphics output is displayed and where plots can be named, resized, copied and saved. There are some other important tabs here as well, which you can also explore. When a new plot is produced, the Plots tab will become active."
  },
  {
    "objectID": "1-setup.html#workflow-for-r-scripts",
    "href": "1-setup.html#workflow-for-r-scripts",
    "title": "1 Setup & intro",
    "section": "6 Workflow for R scripts",
    "text": "6 Workflow for R scripts\nScript setup\nAn R script is a plain text file where the file name ends in “dot R” (.R) by default.\nAn R script serves several purposes:\nFirst, it documents your analysis allowing it to be reproduced exactly by yourself (your future self!) or by others like collaborators, friends, colleagues, your professor, your student, etc.\nSecond, it is the interface between your commands and R software.\nA goal is that your scripts should contain only important R commands and information, in an organized and logical way that has meaning for other people, maybe for people you have never spoken to. A typical way to achieve this is to organize every script according to the same plan.\n\n\nYour R script should be a file good enough to show to a person in the future (like a supervisor, or even your future self). Someone who can help you, but also someone who you may not be able to explain the contents to. The script should be documented and complete. Think of this future person as a friend you respect.\n\n\nAlthough there are many ways to achieve this, for the purposes of the Bootcamp we strongly encourage you to organize you scripts like this:\n\nHeader\nContents\nOne separate section for each item of contents\n\n\nHeader\nStart every script with a Header, that contains your name, the date of the most recent edit, and a short description of the PURPOSE of the script.\n# A typical script Header\n\n## HEADER ####\n## Who: <your name>\n## What: My first script\n## Last edited: yyyy-mm-dd (ISO 8601 date format... Google it!)\n####\n\n\n6.1 Contents\nA Contents section should also be present near the top, to provide a road map for the analysis.\n# A typical script Contents section\n\n## CONTENTS ####\n## 00 Setup\n## 01 Graphs\n## 02 Analysis\n## 03 Etc\n\n\n\n6.2 Section for each item of contents\nFinally, code chunk breaks should be used to aid the readability of the script and to provide a section for each item in your table of contents. A code chunk is just a section of code set off from other sections.\nBelow is the beginning of a typical code chunk in an R script.\n\nCode chunks must start with at least one hash sign “#”,\nshould have a title descriptive of code chunk contents,\nand end with (at least) four hash signs “####”\nconsecutively numbered titles makes things very tidy\n\n## 01 This here is the first line of MY CODE CHUNK ####\n\nWe will practice each of these components.\n\n\n\n6.3 Comments\nComments are messages that explain code in your script, and they should be used throughout every script. You can think of comments like the methods section of a scientific paper - there should be enough detail to exactly replicate and understand the script, but it should also be concise.\nComment lines begin with the # character and are not treated as “code” by R.\n\n# Make a vector of numbers <--- a comment\nmy_variable <- c(2,5,3,6,3,4,7)\n\n# Calculate the mean of my_variable <--- another comment\nmean(my_variable)\n\n[1] 4.285714\n\n\n\n\n\n6.4 Submitting commands\nA final thing that must be mentioned here is how to actually submit commands in your R script for R to execute. There are a few ways to do this.\n\nrun the whole line of code your cursor rests on (no selection) Ctrl+Enter (Cmd+Return in Macs)\nrun code you have selected with your cursor Ctrl+Enter (Cmd+Return in Macs).\nUse the “Run” button along the top of the Script window\nRun code from the menu Code > Run Selected Line(s)."
  },
  {
    "objectID": "1-setup.html#practice-exercises",
    "href": "1-setup.html#practice-exercises",
    "title": "1 Setup & intro",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\n\n7.1\nDownload this script and open it with RStudio. Save the script in a specific folder on your computer that you can find again and where you will save other scripts for the Bootcamp.\nRead the script comments and examine the structure of the code chunks. Run the code in the script using one of the methods above, and examine the output in the Console window.\n\n\n\n7.2\nAdd a code chunk title to your CONTENTS section and to your script. Make sure to write brief comments for your code. Add the following code to your chunk run it and examine the output:\nDon’t worry about understanding the code for now. We are just working on interfacing with R and submitting commands.\n# Create a new variable\nmy_variable <- c(6.5, 1.35, 3.5)\n\n# Calculate the mean of my_variable\nmean(my_variable)\n\n# Calculate the standard deviation of my_variable\nsd(my_variable)"
  },
  {
    "objectID": "1-setup.html#harper-adams-data-science",
    "href": "1-setup.html#harper-adams-data-science",
    "title": "1 Setup & intro",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module supports students and staff at Harper Adams University and the MSc in Data Science for Global Agriculture, Food, and Environment led by Ed Harris."
  },
  {
    "objectID": "6-data-manipulation.html#aggregate-function",
    "href": "6-data-manipulation.html#aggregate-function",
    "title": "6 Data subsetting and manipulation",
    "section": "5 aggregate() function",
    "text": "5 aggregate() function\nWe often may wish to summarize parts of a data set according to some index of variable values. A very convenient tool for the is the aggregate() function, which we will practice here.\nhelp(aggregate)\n\n# A few important things to note about how this function works:\n# The \"x\" argument is a data object you input, but should only contain numeric values usually\n# If a data.frame object is input as x, a data.frame object is the output\n# The \"by\" argument must be a list() object and can be one or more indices\n# The FUN argument is the name of the function that will act on the \"x\" argument data\n\n# Let's try a few examples\n\n## 1 calculate the mean of decrease by treatment in OrchardSprays\n\naggregate(x = OrchardSprays$decrease,\n          # NB use of list() and naming it \"treatment\"\n          by = list(treatment = OrchardSprays$treatment), \n          FUN = mean)\n\n# we can \"recycle\" the code above to apply different functions\n# standard deviation with sd()\naggregate(x = OrchardSprays$decrease,\n          # NB use of list() and naming it \"treatment\"\n          by = list(treatment = OrchardSprays$treatment), \n          FUN = sd)\n\n# Range with range()\naggregate(x = OrchardSprays$decrease,\n          # NB use of list() and naming it \"treatment\"\n          by = list(treatment = OrchardSprays$treatment), \n          FUN = range)\n\n# What if we want several summary statistics?\n\naggregate(x = OrchardSprays$decrease,\n          # NB use of list() and naming it \"treatment\"\n          by = list(treatment = OrchardSprays$treatment), \n          # NB use of function() \n          FUN = function(x) c(mean = mean(x), # Add naming\n                              sd = sd(x), \n                              range = range(x)))\n\n## Example of use of aggregate object\n# Say you would like to graph a barplot of the MEAN of decrease by treatment\n# and you would like to show STANDARD DEVIATION error bars\n\n# Make data frame with summary values using aggregate()\nmy_mean <- aggregate(x = OrchardSprays$decrease,\n                        by = list(treatment = OrchardSprays$treatment), \n                        FUN = mean)\n\nmy_sd <- aggregate(x = OrchardSprays$decrease,\n                   by = list(treatment = OrchardSprays$treatment), \n                   FUN = sd)\n\nmy_mean\nmy_sd\n\n# Tidy things up in a new data frame using data.frame()\n# Take care of naming variables for clarity\nhelp(data.frame) # Continue using help() as a good habit\nnew_data <- data.frame(treatment = my_mean$treatment,\n                       mean = my_mean$x,\n                       sd = my_sd$x)\nnew_data # Looks good\n\n# There is a lot going on in the following code\n# The point is to show what is possible\n\n(bar_centers <- barplot(new_data$mean,#   use mean for barheight\n                        ylim = c(0, 115),\n                        ylab = \"Mean solution decrease (+- 1 SD)\",\n                        xlab = \"Treatment\"))\n# NB bar_centers holds the numerical position value of the bars...\n\nhelp(arrows)  # Use to draw error bars\narrows(x0 = bar_centers, \n       x1 = bar_centers,\n       y0 = new_data$mean , # start error bar at top of the bar!\n       y1 = new_data$mean + new_data$sd, # end error bar here!\n       angle = 90,\n       length = 0.1)\n\n# Last step: label the x axis\naxis(side = 1,\n     at = bar_centers,\n     labels = new_data$treatment)\n     \n# Flash challenge: Draw a new barplot by recycling the code above\n# This time, add error bars showing on both the top and the bottom of the mean values"
  },
  {
    "objectID": "6-data-manipulation.html#practice-exercises",
    "href": "6-data-manipulation.html#practice-exercises",
    "title": "6 Data subsetting and manipulation",
    "section": "6 Practice exercises",
    "text": "6 Practice exercises\nFor the following exercises, use the trees dataset built into R, which has Girth, Height and Volume variables for 31 Black Cherry trees.\n# Examine the data\nhelp(trees)\ndata(trees)\nstr(trees)\n\n\n6.1\nShow code to calculate the mean Girth of Black Cherry trees with Height less than 75 ft.\n\n\n\n6.2\nUse help(cut) and then use the cut() function to create a new factor variable based on the Height numeric variable in the trees dataset. Try setting the breaks argument to 2 or 3. Rename the levels of your new factor to something meaningful. Show the code.\n\n\n\n6.3\nUsing the new factor from question 2, use aggregate() to calculate the mean and standard deviation of all three variables in the trees data. Show your code and report the results to 2 decimal points of accuracy.\n\n\n\n6.4\nShow the code using which() and boolean phrases as appropriate to find the rows in the trees dataset where Girth is higher than 11 and Height is lower than 75.\n\n\n\n6.5\nRun the following code:\ndata_1 <- data.frame(volume = c(4,5,6,5,6,7,6,5,6,8,7,3,8,7,NA,10),\n           population = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n           \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\"))\nUse aggregate() to calculate the mean of volume for each population (hint: you may need to use help for the functions involved and pay close attention to your data frame…).\n\n\n\n6.6\nWrite a plausible practice question involving any aspect of using which(), boolean phrases and/or aggregate() involving the in-built R dataset iris."
  },
  {
    "objectID": "7-question-explore.html",
    "href": "7-question-explore.html",
    "title": "7 Question, explore, analyze",
    "section": "",
    "text": "We call sizing up the data “weighing the pig”"
  },
  {
    "objectID": "7-question-explore.html#question-explore-analyze-a-workflow-for-data-science",
    "href": "7-question-explore.html#question-explore-analyze-a-workflow-for-data-science",
    "title": "7 Question, explore, analyze",
    "section": "1 Question, explore, analyze (a workflow for data science)",
    "text": "1 Question, explore, analyze (a workflow for data science)\n\n\nA dataset often comes to the Data Scientist in an imperfect state, possibly incomplete, containing errors, and with minimal description. Likewise, it may contain wonderful knowledge, there to discover. Either way, your first task is to weigh the pig.\n\n\nThe very first task for any data analysis is to gain an understanding of the data itself. This typically involves examining the variables. Are they as we expect? Do we need to adjust the variable types?\nEDA Exploratory Data Analysis\nThis almost always involves graphing the data, and possibly examining numerical summaries and statistical assumptions. Further, it is necessary to look for errors in the data both trivial (e.g. misspelling factor level names like “control” with an extra space “control”), and more serious errors such as numerical typographical errors (e.g. misplacing a decimal point is a classic: height of 5 men in feet: c(5.5, 5.8, 6.1, 5.9, 52.).\nIn total, this part of data analysis is sometimes referred to as Exploratory Data Analysis.\nEDA is part practical and part philosophical in that is requires skill and experience, but is also subjective. Think of it as a step that might take a long while, where the data scientists decides what the analysis is that will be applied to the data, that the analysis is correct and appropriate. Ironically, while EDA is considered very important and can take a large proportion of the total time spent analyzing data, it is usually only reported on very briefly if at all.\nThe order of operation for most analyses should be\n\n1 question\n2 explore\n3 analyse\n\n\n\nYou choose your data analysis prior to collecting the first data point.\n\nFocus on the question and make sure it is clear in formulation, and choose an analysis approach that can resolve the question , given the data… But the data collection should be DESIGNED to fit the question and chosen analysis prior to collection. Explore the data to examine any assumptions required for the analysis, including the use of graphing and any diagnostic or summary statistics. Finally, perform and summarize the analysis. We will practice this workflow for different basic questions, with an emphasis on simple quantitative data.\n\n\n1.1 Objectives\n\nQuestion formulation and hypothesis testing\nSummarize: Weighing the Pig\nVariables and graphing\n“Analysis” versus “EDA”\nStatistical Analysis Plan: the concept\nPractice exercises"
  },
  {
    "objectID": "7-question-explore.html#question-formulation-and-hypothesis-testing",
    "href": "7-question-explore.html#question-formulation-and-hypothesis-testing",
    "title": "7 Question, explore, analyze",
    "section": "2 Question formulation and hypothesis testing",
    "text": "2 Question formulation and hypothesis testing\n\nIt is the primary responsibility of the scientist to agree on the specific details of generating evidence from data to answer questions (i.e., statistical analysis). When these roles are occupied by the same person, this matter should be settled before collecting any data.\n\nThe general topic of formulating statistical questions is vast; many books have been written on the subject. The tradition and practice of statistical analysis has evolved through time. Here we will focus on the traditional starting point for a “first statistics course”, within the context of Null Hypothesis Significance testing (NHST).\n\n2.1 Sampling concept and NHST\nThe gambit of NHST is that there is a population of interest but that the population cannot be directly measured because it is too big or otherwise inconvenient or impossible to measure. Thus, experimental samples are drawn randomly from the population, possibly subjected to experimental conditions, and the magnitude of observed differences or measured associations are summarized by various test statistics and compared to how likely such an observed difference or association would be to observe in the absence of the hypothesized effect.\nThe null hypothesis is the one consistent with no effect or difference. We evaluate whether to reject the null hypothesis using the P-value, the (conditional) probability that the observed effect is unlikely to arise duie to sampling or experimental error.\nTraditionally, the P-value is compared to the alpha value, almost always set to 0.05. This alpha value can be interpreted as the maximum probability that is acceptable of making a mistake and concluding there IS a difference, when in fact a difference does not exist. When the P-value is less than 0.05, we conclude there is a difference, rejecting the null hypothesis and “accepting” the hypothesis we predicted was true, usually referred to as the alternative hypothesis.\n\n\n\n2.2 NHST notes\nBenefits of NHST\n\nFamiliar and acceptable to majority of researchers\nTypically robust to assumptions when applied correctly\nStrong framework for evidence, especially for experiments\nThe basic idea is objective and simple\n\n\nCriticism of HNST\n\nOften conceived, applied and interpreted under error\nValidation of analysis (e.g. assumptions testing) is often neglected\nEducation for applied researchers often deficient\nThough simple, practitioners may be ignorant of subtle concepts\n\n\n\n\n2.3 Further reading\nIf the idea is new to you that NHST in statistics is not perfect and you want to get serious about understanding why, like most subjects, you will need to pursue further sources.\nAnderson, D.R., Burnham, K.P. and Thompson, W.L., 2000. Null hypothesis testing: problems, prevalence, and an alternative. The journal of wildlife management, pp.912-923.\nNickerson, R.S., 2000. Null hypothesis significance testing: a review of an old and continuing controversy. Psychological methods, 5(2), p.241.\nNix, T.W. and Barnette, J.J., 1998. The data analysis dilemma: Ban or abandon. A review of null hypothesis significance testing. Research in the Schools, 5(2), pp.3-14.\nStephens, P.A., Buskirk, S.W., Hayward, G.D. and Martinez Del Rio, C., 2005. Information theory and hypothesis testing: a call for pluralism. Journal of applied ecology, 42(1), pp.4-12."
  },
  {
    "objectID": "7-question-explore.html#summarize-weighing-the-pig",
    "href": "7-question-explore.html#summarize-weighing-the-pig",
    "title": "7 Question, explore, analyze",
    "section": "3 Summarize: Weighing the Pig",
    "text": "3 Summarize: Weighing the Pig\n\n\nThe best way gain skill in handling data is to practice.\n\nWeighing the pig is the term we use to describe creating a summary-at-a-glance of a dataset. Usually this includes graphics and statistical summary, as well a description of how much data we have. A key consideration is, also, the specification of the variables.\nWe will practice data handling with the data file chickwts.xlsx.\nDownload the file, read it into a data object in R called chicks, and convert the feed variable to a factor if necessary.\n# Try this:\n\n# Download the 7-chickwts.xlsx file, read it into a data \n# object in R called \"chicks\", \n# and convert the \"feed\" variable to a factor if necessary.\n\n# Do not neglect looking inside the \"raw\" data file\n# Is it as you expect?  Is the data dictionary present and clear?\n\n# Load necessary libraries\nlibrary(openxlsx)\n\n# Read file\nsetwd(\"D:/Dropbox/git/DSgarage/public/data\") # NB change to YOUR file path...\nchicks <- read.xlsx(\"7-chickwts.xlsx\")\n\n# Convert feed to factor if needed\nclass(chicks$feed) # Character\nchicks$feed <- factor(chicks$feed)\nclass(chicks$feed) # Factor\n\n\n3.1 Chick data\n\nThe hypothesis voices “how you think the world works” or what you predict to be true”\n\nThe hypothesis we believe is true for the chicks dataset might be phrased in different ways.\n\nChick weight differs after 6 weeks according to feed additive type\nMean chick weight varies according to feed additive type\nThe variance between chick weight for different feed additives is bigger than the variance within chick weight as a whole\n\n\n\n\n3.2 Hypothesis\nThe minimum amount of information we are usually interested in when sizing up a dataset is How much data is there?, What is the central tendency (e.g. the mean, variance, etc.)?, and possibly Are there rare values?.\nWe would typically start graphing the data right away. If we have a notion of what our questions or hypotheses are, they should inform the initial peek at the data. For example, in the chickwts data, we know our question will be related not to the overall central tendency of chick weight, but to chick weight for each individual feed type.\nWe do not approach this sizing up of the data in a workhorse fashion, merely to check a tick box. We are looking quickly for details in the data that give us insight into what the data is like. For example, we peek at whether the mean and median are close to each other (indicator our data may be Gaussian), we compare the standard deviation, variance or standard error of a numeric variable relative to different levels of a factor, to see if they are similar.\n# Try this:\n\n# Summarize the whole dataset\n# summary() provides summary statistics for numeric variables and counts\nsummary(chicks)\n\n# we might want to look at summary for different levels of feed\n?summary\nsummary(object = chicks$weight[which(chicks$feed == \"casein\")])\nsummary(object = chicks$weight[which(chicks$feed == \"horsebean\")])\n# etc. - this method is easy but inelegant?\n\n# aggregate()\n?aggregate\n\n# mean\naggregate(x = chicks$weight, by = list(chicks$feed), FUN = mean)\n\n# standard deviation\naggregate(x = chicks$weight, by = list(chicks$feed), FUN = sd)\n\n# You can make your own function for the FUN argument\n# stadard error of mean, SEM = standard deviation / square root of sample size\naggregate(x = chicks$weight, by = list(chicks$feed), \n          FUN = function(x){ sd(x)/sqrt(length(x)) })\n\n# You can apply several functions and name them!\naggregate(x = chicks$weight, by = list(feed = chicks$feed), \n          FUN = function(x){ c(mean = mean(x), \n                               sd = sd(x),  \n                               SEM = sd(x)/sqrt(length(x)))})"
  },
  {
    "objectID": "7-question-explore.html#variables-and-graphing",
    "href": "7-question-explore.html#variables-and-graphing",
    "title": "7 Question, explore, analyze",
    "section": "4 Variables and graphing",
    "text": "4 Variables and graphing\n\nA good graph usually tells the whole story, but a bad graph is worse than no graph at all.\n\n\n\n\n\n\nXKCD Convinced by data\n\n\n\n\nThere are a few topics in graphing data that are important to consider here, but the topic is wide and deep, analytical, creative, and even artistic. We make a distinction between graphs used to explore data during EDA (meant to be “consumed” only by the data scientist who made them and are of no use to document a pattern to others) and graphs intended to constitute evidence.\n\n\n4.1 Scientific graphs\nA few graphing principles:\n\nMust convey the relevant information\nShould be consistent in aesthetics\nMust be self-contained (meaning is contained 100% within the figure and legend)\nShould reflect a hypothesis or statistical concept (if not purely descriptive)\nShould be appropriate to the data\n\n\nYou can think of R graphics as a way to “build up information in layers” onto a graph. There are many aesthetic features of graph that can be controlled, like adding colors, text, lines, legends, etc. The R graphics system is very simple to use, but can also be very powerful (mastering this takes practice). We make a distinction here between R base graphics and packages that can be used to make specialized and varied graphs (like the powerful and popular package {ggplot})\n\n\n4.2 Layering information\nWe can look at graphing the chicks data in a few different ways. We will try a few different graphs in this way, building up features. We might build up features on a graph using arguments in a particular graph function.\n\nLike, adding\n\na main title with the argument main\nthe x axis title with the argument xlab\nadding lines with the functions abline() or lines()\n\n\n\n\n4.3 Types of graphs\nTypically you would choose the type of graph that both fits the type of data you have and that conveys the information you wish to examine or showcase. E.g., for a single numeric variable, you might wish to show:\n\nThe distribution of data with a histogram: hist()\nThe central tendency relative to a factor with a boxplot: boxplot()\n\n\nHistogram of the chicks data\n# The least you can do\nhelp(hist)\nhist(x = chicks$weight)\n\n\nAdd a title with main\n# Argument main\nhist(x = chicks$weight,\n     main = \"Distribution of chick weights (all feeds)\")\n\n\nAdd an x axis title with xlab\n# x axis title\nhist(x = chicks$weight,\n     main = \"Distribution of chick weights (all feeds)\",\n     xlab = \"Chick weight (grams)\")\n\n\nAdd a vertical line for the weight mean with abline()\n# Add vertical line for mean weight\nhist(x = chicks$weight,\n     main = \"Distribution of chick weights (all feeds)\",\n     xlab = \"Chick weight (grams)\")\n\nhelp(abline)\nabline(v = mean(chicks$weight), col = \"red\", lty = 2, lwd = 3)\n\n\n# Try a boxplot\n\nhelp(boxplot)\nboxplot(x = chicks$weight)\n\n# I have seen worse graphs, but I can't remember when.\n# Flash challenge: Improve the graph\n\n\n# weight as a function of feed\nboxplot(formula = weight ~ feed,\n        data = chicks)\n# This is probably a good representation of our hypothesis\n# Flash challenge: Improve the graph..."
  },
  {
    "objectID": "7-question-explore.html#harper-adams-data-science",
    "href": "7-question-explore.html#harper-adams-data-science",
    "title": "7 Question, explore, analyze",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module supports students and staff at Harper Adams University and the MSc in Data Science for Global Agriculture, Food, and Environment led by Ed Harris."
  },
  {
    "objectID": "7-question-explore.html#analysis-versus-eda",
    "href": "7-question-explore.html#analysis-versus-eda",
    "title": "7 Question, explore, analyze",
    "section": "5 “Analysis” versus “EDA”",
    "text": "5 “Analysis” versus “EDA”\nAlthough you could consider Exploratory Data Analysis, EDA, an important part of the complete process of data analysis, we might make a distinction between “Analysis” the part of analysis that generates Evidence, and that of EDA which is used to explore data and test assumptions.\n\n\n5.1 Analysis\nA data analysis is\n\nDesigned to fit a specific question or hypothesis\nPart of a workflow: Informal hypothesis statement (in plain language) > Statistical hypothesis (specifies a or implies a statistical test) > Evidence (the specific results)\nDesigned and usually formatted to present to others, such as in a report or a scientific manuscript\nContains only bare essentials as relates to the initial hypothesis (e.g. a good graph, the summary of a statistical analysis)\nShould strictly be reproducible via a script and archived data\nDone in conjunction with EDA\n\n\n\n\n5.2 EDA\nExploratory data analysis is\n\nInformal and may be haphazard\nDesigned to explore or gain understanding of data\nAssumptions testing\nUsually not designed to document or show to others\nOccurs primarily before (every) analysis\nMay or may not be documented to be reproducible\nDone before the final, evidence-generating Analysis\n\n\nWe can keep this concept of EDA versus Analysis in our mind while we discuss the Statistical Analysis Plan."
  },
  {
    "objectID": "7-question-explore.html#statistical-analysis-plan-the-concept",
    "href": "7-question-explore.html#statistical-analysis-plan-the-concept",
    "title": "7 Question, explore, analyze",
    "section": "6 Statistical Analysis Plan: the concept",
    "text": "6 Statistical Analysis Plan: the concept\n\nI have a cunning (statistical analysis) plan -Baldrick\n\nA Statistical Analysis Plan (SAP) is a formal document that should be used to design data analysis. One of the most important functions of the SAP is to make a formal connection between the hypothesis, the data collected and and the method of analysis that will be used to generate evidence to support or refute the hypothesis. This is conducted before any data are collected.\nThe components of a basic SAP are:\n\nThe hypotheses stated in plain language\nEach hypothesis translated into a specific statistical model\nSpecification of data and and data collection methods\n\n-) Specification of effect size\n\nJustification of sample size through power analysis or other means\n\n\nDefinition of all of these components is beyond the boundaries of this Bootcamp, however the explicit connection of hypotheses with a statistical model is one of the very basic elements of best practice in science.\n\n\n6.1 The scientific method, Classic version\nWe usually learn the scientific method as a cycle where we conceive a problem, form a hypothesis, conduct an experiment, evaluate the result and so on. We learn and teach this as a literal cycle.\n\n\n\n\n\nThe classic view of the scientific process\n\n\n\n\nThis classic view of the scientific process implies that we plan the analysis only after we conduct the experiment and collect data. While many data scientists or statisticians would agree that this model is widely used in science, it is considered very poor practice for several reasons.\n\nThe expected difference or relationship (i.e., the effect size) should explicitly be part of the hypothesis and quantified BEFORE collecting data\nThe statistical test must be chosen prior to collect the data to insure the evidence matches the expectation\nThe sample size should be justified, using power analysis or a less formal means. Collecting too little data will likely result in failing to detect a difference (even if your hypothesis is correct!); Collecting too much data is simply a waste of resources.\n\n\n\n\n\n\nScientific Process - what we teach children in school is not quite right\n\n\n\n\n\n\n6.2 Best practice scientific method\nThe traditional view of the scientific method should probably be adjusted to explicitly accommodate planning the analysis at the same time as the hypothesis formulation stage. Likewise, the analysis plan should specifically influence the design of the data collection for the experiment.\n\n\n\n\n\nModern scientific process\n\n\n\n\nA modern view of best practice of scientific endeavor includes an experimental design phase, with consideration of effect size and power analysis, and the production of a statistical analysis plan that contains a formal statistical hypothesis. All off this happens prior to any data collection."
  },
  {
    "objectID": "7-question-explore.html#practice-exercises",
    "href": "7-question-explore.html#practice-exercises",
    "title": "7 Question, explore, analyze",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\nFor the following questions, use the field-trial.xlsx dataset.\nThis is real data in Tidy Data format, but our information for these exercises is limited precisely to the contents of the file, including the data dictionary. In this experiment, seeds were raised under field trial conditions for two weeks to look at the effect of different treatment conditions on mass of gain during germination. There are several measured variables, with the calculated pct variable probably intended to be the dependent variable, with the factor treatment being the main explanatory variable for variation in pct.\n\n\n7.1\nShow code to set up an R analysis file with a header, table of contents, and a setup section that sets your working directory, loads any required libraries and reads in the data. Call the data.frame object you create seed.\n\n\n\n7.2\n\npct, wet and dry should be numeric\nblock and trial should be factors\ntreatment should be a factor with the level “Control” set as the reference.\n\nShow the code to do this.\n\n\n\n7.3\nUse aggregate() to calculate the mean, standard deviation, standard error, and the count (e.g. length()) of pct for each level of treatment. Show the code.\n\n\n\n7.4\nMake a fully labelled boxplot of the pct variable as a function of treatment. Add a horizontal line (red and dashed) for the overall mean of pct, and two horizontal lines (gray, dotted) for the overall mean of pct +/- 1 standard deviation.\n\n\n\n7.5 (hard: may require tinkering and problem solving)\nExperiment making a boxplot showing pct ~ treatment separated for each trial\n\n\n\n7.6\nWrite a plausible practice question involving aggregate() and boxplot() in-built R dataset iris."
  }
]