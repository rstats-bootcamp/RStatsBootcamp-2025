[
  {
    "objectID": "08-sampling-dist.html",
    "href": "08-sampling-dist.html",
    "title": "08 Distributions",
    "section": "",
    "text": "Can you guess the weight of this bull? What about you and 99 friends?",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "08 Distributions"
    ]
  },
  {
    "objectID": "08-sampling-dist.html#describing-the-shape-of-data",
    "href": "08-sampling-dist.html#describing-the-shape-of-data",
    "title": "08 Distributions",
    "section": "1 Describing the shape of data",
    "text": "1 Describing the shape of data\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this lesson, you will be able to:\n\nCreate and use a histogram to evaluate data distribution\nIdentify Gaussian data (that ain’t normal)\nIdentify Poisson data\nIdentify Binomial data\nDiagnose commondata distributions\n\n\n\nOverview\n\nI. A curve has been found representing the frequency distribution of standard deviations of samples drawn from a normal population. II. A curve has been found representing the frequency distribution of values of the means of such samples, when these values are measured from the mean of the population in terms of the standard deviation of the sample\n\n\n- Gosset. 1908, Biometrika 6:25.\n\nThe idea of sampling underpins traditional statistics and is fundamental to the practice of statistics. The basic idea is usually that there is a population of interest, which we cannot directly measure. We sample the population in order to estimate the real measures of the population. Because we merely take samples, there is error assiociated with our estimates and the error depends on both the real variation in the population, but also on chance to do with which subjects are actually in our sample, as well as the size of our sample. Traditional statistical inference within Null Hypothesis Significance Testing (NHST) exploits our estimates of error associated with our samples. While this is an important concept, it is beyond the scope of this page to review it, but you may wish to refresh your knowledge by consulting a reference, such as Irizarry 2020 Chs 13-16.\nIn this page, we will briefly look at some diagnostic tools in R for examining the distribution of data, and talk about a few important distributions that are common to encounter.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "08 Distributions"
    ]
  },
  {
    "objectID": "08-sampling-dist.html#use-of-the-histogram",
    "href": "08-sampling-dist.html#use-of-the-histogram",
    "title": "08 Distributions",
    "section": "2 Use of the histogram",
    "text": "2 Use of the histogram\nThe histogram is a graph type that typically plots a numeric variable on the x axis (either continuous numeric values, or integers), and has the frequency of observations on the y axis (i.e., the count), or sometimes the proportion of observation on the y axis.\n\n2.1 Typical histogram for continuous variable\n# Try this:\n\n# Adult domestic cats weight approximately 4 Kg on average\n# with a standard deviation of approximately +/1 0.5 Kg\n\n# Let's simulate some fake weight data for 10,000 cats\nhelp(rnorm) # A very useful function\nhelp(set.seed) # If we use this, we can replicate \"random data\"\n\nhelp(hist)\n\nset.seed(42)\ncats &lt;- rnorm(n = 10000,  # 10,000 cats\n              mean = 4,   \n              sd = 0.5)\n\ncats[1:10] # The first 10 cats\n\n [1] 4.685479 3.717651 4.181564 4.316431 4.202134 3.946938 4.755761 3.952670\n [9] 5.009212 3.968643\n\nhist(x = cats,\n     xlab = \"Cat weight (Kg)\")\n\n\n\n\n\n\n\n\nNotice a few things:\n\nThe bars are the count of the number of cats at each weight on the x axis\nThe width of each vertical column is a (non-overlapping) range of weights - these are called “bins” and can be defined, but usually are automatically determined based on the data\nFor count data, each bar is usually one or more integer values, rather than a range of continuous values (as it is for cat weight above in the figure)\nThe shape of a histogram can be used to infer the distribution of the data\n\n\n2.2 Sampling and populations\nRemember the concept of population versus sample? Well, let’s assume there are only 10,000 cats in the whole world and we just measured the whole population (usually not possible, remember). In this case we can calculate the exact population mean.\nWhat if we tried to estimate the real mean of our population from a sample of around 100 cats? The theory is that our sample mean would be expected to differ from the real population mean randomly. If we did a bunch of samples, most of the guesses would be close to the real population mean and less would be farther out, but all of these sample means would be expected to randomly vary, either less than or greater than the true population mean. We can examine this with a simulation of samples.\n\n# Try this:\n# simulation of samples\n\nhelp(sample) # randomly sample the cats vector\nhelp(vector) # Initialize a variable to hold our sample means\n\n# We will do a \"for loop\" with for()\n\n\nmymeans &lt;- vector(mode = \"numeric\",\n                  length = 100)\nmymeans # Empty\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\nfor(i in 1:100){\n  mysample &lt;- sample(x = cats, # Takes a random sample\n                    size = 30)\n  \n  mymeans[i] &lt;- mean(mysample) # stores sample mean in ith vector address\n  }\n\nmymeans # Our samples\n\n  [1] 3.899662 3.963986 4.019242 3.975304 3.903256 3.877346 3.911380 4.013805\n  [9] 3.989836 3.950881 4.002299 4.024990 4.091362 4.074493 3.989800 3.974894\n [17] 4.000338 3.873104 3.945492 4.061428 4.080559 4.011375 4.023977 3.863940\n [25] 4.047250 3.921947 4.049521 4.085961 3.853068 4.081517 3.987747 4.039110\n [33] 3.940955 3.954955 4.008512 3.942036 3.955110 3.968722 3.896042 3.979187\n [41] 3.957636 4.021170 4.107460 3.989197 3.931964 3.981774 4.125465 4.031625\n [49] 4.081076 3.939582 4.185512 3.997635 3.986411 3.817746 4.075256 4.074309\n [57] 4.136248 3.926092 3.976513 4.008376 3.984264 3.900717 4.138209 3.913901\n [65] 4.123326 3.894216 4.087260 4.145020 3.896286 4.142604 3.865085 4.014336\n [73] 4.053620 3.767552 3.981785 4.130483 4.165097 4.046661 4.077928 4.041611\n [81] 3.873046 4.100438 4.015098 3.947361 4.029464 4.123772 3.860191 3.820483\n [89] 4.071399 4.145585 3.982339 3.950332 3.987406 4.167345 4.126462 4.047683\n [97] 4.035958 3.987601 3.960099 3.869092\n\nhist(x = mymeans,\n     xlab = \"Mean of samples\",\n     main = \"100 cat weight samples (n = 30/sample)\")\nabline(v = mean(mymeans), col = \"red\", lty = 2, lwd = 2)\n\n\n\n\n\n\n\n\nNotice a few things (NB your results might look slightly different to mine - remember these are random samples):\n\nThe samples vary around the true mean of 4.0 Kg\nMost of the samples are pretty close to 4.0, fewer are farther away\nThe mean of the means is close to our true population mean\n\nTry our simulation a few more times, but vary the settings. How does adjusting the sample size (say up to 100 or down to 10)? How about the number of samples (say up to 1000 or down to 10)?",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "08 Distributions"
    ]
  },
  {
    "objectID": "08-sampling-dist.html#gaussian-that-aint-necessarily-normal",
    "href": "08-sampling-dist.html#gaussian-that-aint-necessarily-normal",
    "title": "08 Distributions",
    "section": "3 Gaussian: That ain’t necessarily normal!",
    "text": "3 Gaussian: That ain’t necessarily normal!\n\nThe Gaussian distribution is sometimes referred to as the Normal distribution. This is not a good practice: do not refer to the Gaussian distribution as the Normal distribution. Referring to the Gaussian distribution as the normal distribution implies that Gaussian is “typical”, which is patently untrue.\n\n\n- The Comic Book Guy, The Simpsons\n\n\nThe Gaussian distribution is the classic “Bell curve” shaped distribution. It is probably the most important distribution to master, because of its importance in several ways:\n\nWe expect continuous numeric variables that “measure” things to be Gaussian (e.g., human height, lamb weight, songbird wing length, Nitrogen content in legumes, etc., etc.)\nExample data: length of chicken beaks in mm\n\nR output\n# chicken beak lengths\n\n [1] 19.1 19.5 16.5 20.9 18.7 20.9 21.4 22.1 18.8 21.0 16.6 18.4 18.3 15.2 20.1 20.4\n[17] 19.3 21.5 18.5 17.3\n\n3.1 The Gaussian assumption (important topic)\n\nFor linear models like regression and ANOVA (we will review these models), we assume the residuals (the difference between each observation and the mean) to be Gaussian distributed and we often must test and evaluate this assumption\nThe Gaussian is described by 2 quantities: the mean and the variance\n\n\n# Example data\n(myvar &lt;- c(1,4,8,3,5,3,8,4,5,6))\n\n [1] 1 4 8 3 5 3 8 4 5 6\n\n# Mean the \"hard\" way\n(myvar.mean &lt;- sum(myvar)/length(myvar))\n\n[1] 4.7\n\n# Mean the easy way\nmean(myvar)\n\n[1] 4.7\n\n# Variance the \"hard\" way \n# (NB this is the sample variance with [n-1])\n(sum((myvar-myvar.mean)^2 / (length(myvar)-1)))\n\n[1] 4.9\n\n# Variance the easy way \nvar(myvar)\n\n[1] 4.9\n\n# Std dev the easy way\nsqrt(var(myvar))\n\n[1] 2.213594\n\n\n\n3.2 Gaussian histograms\nWe can describe the expected perfect (i.e., theoretical) Gaussian distribution based just on the mean and variance. The value of this mean and variance control the shape of the distribution.\n\n\n\n3.3 More Gaussian fun\n\n## Gaussian variations ####\n# Try this:\n\n# 4 means\n(meanvec &lt;- c(10, 7, 10, 10))\n\n[1] 10  7 10 10\n\n# 4 standard deviations\n(sdvec &lt;- c(2, 2, 1, 3))\n\n[1] 2 2 1 3\n\n# Make a baseline plot\nx &lt;- seq(0,20, by = .1)\n\n# Probabilities for our first mean and sd\ny1 &lt;- dnorm(x = x, \n            mean = meanvec[1],\n            sd = sdvec[1])\n\n# Baseline plot of 1st mean and sd\nplot(x = x, y = y1, ylim = c(0, .4),\n     col = \"goldenrod\",\n     lwd = 2, type = \"l\",\n     main = \"Gaussian fun \n     \\n mean -&gt; curve position; sd -&gt; shape\",\n     ylab = \"Density\",\n     xlab = \"(Arbitrary) Measure\")\n\n# Make distribution lines\nmycol &lt;- c(\"red\", \"blue\", \"green\")\nfor(i in 1:3){\n  y &lt;- dnorm(x = x, \n                mean = meanvec[i+1],\n                sd = sdvec[i+1])\n  lines(x = x, y = y, \n        col = mycol[i],\n        lwd = 2, type = \"l\")\n}\n\n# Add a legend\nlegend(title = \"mean (sd)\",\n       legend = c(\"10 (2)\", \"  7 (2)\", \n                  \"10 (1)\", \"10 (3)\"),\n       lty = c(1,1,1,1), lwd = 2,\n       col = c(\"goldenrod\", \"red\", \"blue\", \"green\"),\n       x = 15, y = .35)\n\n\n\n\n\n\n\n\n3.4 Quartile-Quartile (Q-Q) plots\nIt is very often that you might want a peek or even more formally test whether data are Gaussian. This might be in a situation when looking at, for example, the residuals for a linear model to test whether they adhere to the assumption of a Gaussian distribution. In that case, a common diagnostic graph to construct is the quantile-quantile, or “q-q”” Gaussian plot.\nThe q-q Gaussian plot your data again the theoretical expectation of the “quantile”, or percentile, were your data perfectly Gaussian (a straight, diagonal line). Remember, samples are not necessarily expected to perfectly conform to Gaussian (due to sampling error), even if the population from which the sample was taken were to be perfectly Gaussian. Thus, this is a way to confront your data with a model, to help be completely informed. The degree to which your data deviates from the line (especially systematic deviation at the ends of the line of expectation), is the degree to which is deviates from Gaussian.\n\n\n## q-q- Gaussian ####\n\n# Try This:\n\nlibrary(car) # Might need to install {car}\n\nLoading required package: carData\n\n# Set graph output to 2 x 2 grid\n# (we will set it back to 1 x 1 later)\npar(mfrow = c(2,2))  \n\n# Small Gaussian sample\nset.seed(42)\nsm.samp &lt;- rnorm(n = 10, \n                 mean = 10, sd = 2)\n\nqqPlot(x = sm.samp, \n       dist = \"norm\", # C'mon guys, Gaussian ain't normal!\n       main = \"Small sample Gaussian\")\n\n[1] 9 2\n\n# Large Gaussian sample\nset.seed(42)\nlg.samp &lt;- rnorm(n = 1000, \n                 mean = 10, sd = 2)\n\nqqPlot(x = lg.samp, \n       dist = \"norm\", \n       main = \"Large sample Gaussian\")\n\n[1] 988 980\n\n# Non- Gaussian sample\nset.seed(42)\nuni &lt;- runif(n = 50, \n                 min = 3, max = 17)\n\nqqPlot(x = uni, \n       dist = \"norm\", \n       main = \"Big deviation at top\")\n\n[1] 35 37\n\npar(mfrow = c(1,1)) # set graph grid back",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "08 Distributions"
    ]
  },
  {
    "objectID": "08-sampling-dist.html#poisson-distribution",
    "href": "08-sampling-dist.html#poisson-distribution",
    "title": "08 Distributions",
    "section": "4 Poisson distribution",
    "text": "4 Poisson distribution\n\nLife is good for only two things, discovering mathematics and teaching mathematics.\n\n\n- Simeon-Denis Poisson\n\nThe description of the Poisson distribution was credited to Simeon-Denis Poisson, a (very, very) passionate mathematician. The classic example for use is for count data, where famously it was exemplified by the number of Prussian soldiers who were killed by being kicked by a horse in a particular year.\n\n4.1 The Poisson distribution\n\nCount data of discrete events, objects, etc.\nIntegers, for example the number of beetles caught each day in a pitfall trap:\n\n\nrpois(20, 4)\n\n [1] 3 3 3 5 1 5 5 2 3 4 5 9 5 4 6 2 3 6 5 3\n\n\n\n\nPoisson data are typically skewed to the right\nDescribed by a single parameter, \\(\\lambda\\) (lambda), which describes the mean and the variance\n\n\nThe Poisson parameter:\n\n\n4.2 Example Poisson data\n\n# Try this:\n\n# E.g. (simulated) Number of ewes giving birth to triplets\n# The counts were made in one year 1n 100 similar flocks (&lt;20 ewes each)\n\nset.seed(42)\nmypois &lt;- rpois(n = 30, lambda = 3)\n\nhist(mypois,\n     main = \"Ewes with triplets\",\n     xlab = \"Count of Triplets\")\n\n\n\n\n\n\n\n\n4.3 Density plot for different Poisson lambda values\n\n# Try this:\n# 3 lambdas\n(lambda &lt;- c(1, 3, 5))\n\n[1] 1 3 5\n\n# Make a baseline plot\nx &lt;- seq(0, 15, by = 1)\n\n# Probabilities for our first lambda\ny1 &lt;- dpois(x = x, \n            lambda = lambda[1])\n\n# Baseline plot Pois\nplot(x = x, y = y1, ylim = c(0, .4),\n     col = \"goldenrod\",\n     lwd = 2, type = \"b\",\n     main = \"Poisson fun\",\n     ylab = \"Density\",\n     xlab = \"(Arbitrary) Count\")\n\n# Make distribution lines\nmycol &lt;- c(\"red\", \"blue\")\nfor(i in 1:2){\n  y &lt;- dpois(x = x, \n             lambda = lambda[i+1])\n  lines(x = x, y = y, \n        col = mycol[i],\n        lwd = 2, type = \"b\")\n}\n\n# Add a legend\nlegend(title = \"lambda\",\n       legend = c(\"1\", \"3\", \"5\"),\n       lty = c(1,1,1,1), lwd = 2,\n       col = c(\"goldenrod\", \"red\", \"blue\"),\n       x = 8, y = .35)",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "08 Distributions"
    ]
  },
  {
    "objectID": "08-sampling-dist.html#binomial",
    "href": "08-sampling-dist.html#binomial",
    "title": "08 Distributions",
    "section": "5 Binomial",
    "text": "5 Binomial\n\nWhen faced with 2 choices, simply toss a coin. It works not because it settles the question for you, but because in that brief moment when the coin is in the air you suddenly know what you are hoping for.\n\nThe Binomial distribution describes data that has exactly 2 outcomes: 0 and 1, Yes and No, True and False, etc. (you get the idea).\nExamples of this kind of data include things like flipping a coin (heads or tails), successful germination of seeds (success or failure), or binary behavioral decisions (remain or disperse)\n\n5.1 The Binomial distribution:\n\nData are the count of “successes”” in (binary) outcomes of a series of independent events\nData coding can be variable, but an example would be success for failure while surveying for wildlife: check this nestbox; is there at least one dormouse (Muscardinus avellanarius) in it?.\n5.2 Ex 1 nest boxes\nLet’s say you check 50 nest boxes, there is exactly 1 result per nest box (occupied or not), and the probability of occupancy is 30%.\n\n# Try this:\n\n# dormouse presence:\nset.seed(42)\n(my_occ &lt;- rbinom(n = 50, # Number of \"experiments\", here nestboxes checked\n       size = 1, # Number of checks, one check per nestbox\n       prob = .3)) # Probability of presence\n\n [1] 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0\n[39] 1 0 0 0 0 1 0 1 1 0 1 0\n\nmosaicplot(table(my_occ), col = c(2,'goldenrod'))\n\n\n\n\n\n\n\n\n5.3 Ex 2 Flipping a coin: 20 people 10 times each\n\n# Try this:\n# Flip a fair coin:\nset.seed(42)\n(coin &lt;- rbinom(n = 20, # Number of \"experiments\", 20 people flipping a coin\n       size = 10, # Number of coin flips landing on \"heads\" out of 10 flips per person\n       prob = .5)) # Probability of \"heads\"\n\n [1] 7 7 4 7 6 5 6 3 6 6 5 6 7 4 5 7 8 3 5 5\n\nmosaicplot(table(coin), col = 1:unique(coin))\n\nWarning in 1:unique(coin): numerical expression has 6 elements: only the first\nused\n\n\n\n\n\n\n\n\n\nDescribed by 2 parameters, The number of trials with a binary outcome in a single “experiment” (\\(n\\)), and the probability of success for each binary outcome (\\(p\\)).\n\nThe Binomial parameters:\n\n\n5.4 Density plot for different Binomial parameters\n\n# Try this:\n\n# Binomial parameters\n# 3 n of trial values\n(n &lt;- c(10, 10, 20))\n\n[1] 10 10 20\n\n# 3 probability values\n(p &lt;- c(.5, .8, .5))\n\n[1] 0.5 0.8 0.5\n\n# Make a baseline plot\nx &lt;- seq(0, 20, by = 1)\n\n# Probabilities for our first set of parameters\ny1 &lt;- dbinom(x = x, \n            size = n[1],\n            prob = p[1])\n\n# Baseline plot Binom\nplot(x = x, y = y1, ylim = c(0, .4),\n     col = \"goldenrod\",\n     lwd = 2, type = \"b\",\n     main = \"Binomial fun\",\n     ylab = \"Density\",\n     xlab = \"(Arbitrary) # \\\"Successes\\\"\")\n\n# Make distribution lines\nmycol &lt;- c(\"red\", \"blue\")\nfor(i in 1:2){\n  y &lt;- dbinom(x = x, \n             size = n[i+1],\n             prob = p[i+1])\n  lines(x = x, y = y, \n        col = mycol[i],\n        lwd = 2, type = \"b\")\n}\n\n# Add a legend\nlegend(title = \"Parameters\",\n       legend = c(\"n = 10, p = 0.50\", \n                  \"n = 10, p = 0.80\",\n                  \"n = 20, p = 0.50\"),\n       lty = c(1,1,1,1), lwd = 2, bty='n',\n       col = c(\"goldenrod\", \"red\", \"blue\"),\n       x = 11, y = .35)",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "08 Distributions"
    ]
  },
  {
    "objectID": "08-sampling-dist.html#diagnosing-the-distribution",
    "href": "08-sampling-dist.html#diagnosing-the-distribution",
    "title": "08 Distributions",
    "section": "6 Diagnosing the distribution",
    "text": "6 Diagnosing the distribution\n\nA very common task faced when handling data is “diagnosing the distribution”. Just like a human doctor diagnosing an ailment, you examine the evidence, consider the alternatives, judge the context, and take an educated guess.\n\nThere are statistical tests to compare data to a theoretical model, and they can be useful, but diagnosing a statistical distribution is principally a subjective endeavor. A common situation would be to examine the residual distribution for a regression model compared to the expected Gaussian distribution. Good practice is to have a set of steps to adhere to when diagnosing a distribution.\n\nFirst, develop an expectation of the distribution, based on the type of data\nSecond graph the data, almost always with a histogram, and a q-q plot with a theoretical quartile line for comparison\nThird, compare q-q plots with different distributions for comparison if in doubt, and if it makes sense to do so!\nIf the assumption of a particular distribution is important (like Gaussian residuals), try transformation and compare, e.g., log(your-data), cuberoot(your-data), or others, to the Gaussian q-q expectation.\n\n\nIt is beyond the intention of this page to examine all the possibilities of examining and diagnosing data distributions, but instead the intention is to alert readers that this topic is wide and deep. Here are a few good resources that can take you farther:\nVitto Ricci, Fitting distributions with R\nBill Huber, Fitting distributions to data Quick-R, Probability plots",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "08 Distributions"
    ]
  },
  {
    "objectID": "08-sampling-dist.html#practice-exercises",
    "href": "08-sampling-dist.html#practice-exercises",
    "title": "08 Distributions",
    "section": "7 Practice exercises",
    "text": "7 Practice exercises\nFor the following exercises, run the code below to create the data oibject dat.\n# The code below loads and prints the data frame \"dat\"\n\n# Data dictionary for \"dat\", a dataset with different measures of 20 sheep\n# weight - weight in Kg\n# ked - count of wingless flies\n# trough - 2 feed troughs, proportion of times \"Trough A\" fed from\n# shear - minutes taken to \"hand shear\" each sheep\n\n(dat &lt;- data.frame(\n  weight = c(44.1, 38.3, 41.1, 41.9, 41.2, 39.7, 44.5, 39.7, 46.1, 39.8, \n    43.9, 46.9, 35.8, 39.2, 39.6, 41.9, 39.1, 32.0, 32.7, 44.0),\n  \n  ked = c(9, 4, 15, 11, 10, 8, 12, 12, 6, 11,\n             12, 13, 8, 11, 19, 19, 12, 7, 8, 14),\n  \n  trough = c(0.52, 0.74, 0.62, 0.63, 0.22, 0.22, 0.39, 0.94, 0.96, 0.74,\n              0.73, 0.54, 0.00, 0.61, 0.84, 0.75, 0.45, 0.54, 0.54, 0.00),\n  \n  shear = c(14.0, 8.0, 14.0, 11.0, 14.0, 5.0, 9.5, 11.0, 6.5, 11.0, \n            18.5, 11.0, 18.5, 8.0, 8.0, 6.5, 18.5, 15.5, 14.0, 8.0)\n  ))\n\n7.1 Distribution Diagnosis\nDiagnose the distribution of the following data, which represents the number of parasitic flies found on 20 sheep. Use both graphical and statistical approaches to determine if the data follows a Gaussian distribution.\nc(2, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 3, 0, 1, 0, 0, 0, 2)\n\n\n\n\n\n\n\nDistribution Expectations\n\n\n\n\n\n\n# For this question, we need to consider the nature of each variable:\n\n# weight - This is a continuous measurement of sheep weight in kg\n# ked - This is a count of wingless flies on each sheep\n\nWeight: I would expect the weight variable to be approximately Gaussian (normally) distributed because: - It’s a continuous measurement of a physical attribute - Biological measurements like weight typically follow a Gaussian distribution due to the Central Limit Theorem - Multiple small genetic and environmental factors contribute to weight, which tends to create a bell-shaped distribution\nKed: I would NOT expect the ked variable to be Gaussian distributed because: - It represents count data (number of wingless flies) - Count data is typically discrete and non-negative - Count data often follows a Poisson distribution, especially when counting rare events or organisms - The variance of count data often increases with the mean, which is characteristic of Poisson distributions\nThe nature of the underlying processes that generate these variables suggests different distribution types.\n\n\n\n\n7.2 Binomial vs Poisson\nCompare the binomial and Poisson models for the following count data, which represents the number of successful germinations in 20 seed packets, each containing 10 seeds:\nc(3, 5, 6, 8, 7, 5, 6, 4, 5, 6, 7, 8, 6, 7, 5, 6, 4, 5, 7, 6)\nWhich distribution is more appropriate and why?\n\n\n\n\n\n\n\nBinomial vs Poisson\n\n\n\n\n\n\n# Create the data\ndat &lt;- data.frame(\n  germinations = c(3, 5, 6, 8, 7, 5, 6, 4, 5, 6, 7, 8, 6, 7, 5, 6, 4, 5, 7, 6)\n)\n\n# Create histograms for both distributions\nhist(dat$germinations, \n     main = \"Distribution of Germinations\",\n     xlab = \"Number of Germinations\",\n     col = \"lightblue\",\n     breaks = 8)\n\n# Add vertical lines for the means\nabline(v = mean(dat$germinations), col = \"red\", lwd = 2, lty = 2)\n\n\n\n\n\n\n# Perform Shapiro-Wilk test for normality\nshapiro.test(dat$germinations)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dat$germinations\nW = 0.94951, p-value = 0.3598\n\n# Calculate lambda for Poisson distribution\nlambda &lt;- mean(dat$germinations)\n\n# Generate theoretical Poisson probabilities\nx &lt;- 0:max(dat$germinations)\npoisson_probs &lt;- dpois(x, lambda)\n\n# Compare observed vs expected frequencies\nobserved_freq &lt;- table(factor(dat$germinations, levels = x))\nexpected_freq &lt;- length(dat$germinations) * poisson_probs\n\n# Plot observed vs expected frequencies\nbarplot(rbind(as.vector(observed_freq), expected_freq), \n        beside = TRUE,\n        col = c(\"lightblue\", \"lightgreen\"),\n        names.arg = x,\n        main = \"Observed vs Expected Germination Frequencies\",\n        xlab = \"Number of Germinations\",\n        ylab = \"Frequency\")\nlegend(\"topright\", \n       legend = c(\"Observed\", \"Expected Poisson\"),\n       fill = c(\"lightblue\", \"lightgreen\"))\n\n\n\n\n\n\n# Perform chi-square goodness of fit test\n# Note: Categories with expected frequencies &lt; 5 should be combined\nchisq.test(observed_freq, expected_freq)\n\nWarning in chisq.test(observed_freq, expected_freq): Chi-squared approximation\nmay be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  observed_freq and expected_freq\nX-squared = 45, df = 40, p-value = 0.2705\n\n\nConclusion: - The observed data does not strongly violate the assumptions of either the binomial or Poisson distribution. - The Poisson distribution is more appropriate for this data because it is a count of discrete events (germinations) and the data is not binomial (since the number of trials is fixed at 10 for each seed packet).\nThe Poisson distribution is characterized by a single parameter, \\(\\lambda\\), which is the mean number of events (germinations) per unit of time or space. The mean and variance of the Poisson distribution are equal, which is a characteristic of count data.\nThe Shapiro-Wilk test for normality indicates that the data does not significantly deviate from a normal distribution, which is consistent with the Poisson distribution’s assumption of equal mean and variance.\nThe chi-square goodness of fit test compares the observed frequencies of germination counts to the expected frequencies under the Poisson distribution. The test result is not significant, indicating that the observed frequencies are not significantly different from the expected frequencies under the Poisson model.\nOverall, the Poisson distribution is a good fit for this data, and it is the more appropriate distribution for count data.\n\n\n\n\n7.3 Ked Distribution\nDetermine if the following data on sheep keds (wingless flies) follows a Poisson distribution. The data represents the number of keds found on each of 100 sheep:\nc(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 5, 6)\n\n\n\n\n\n\n\nPoisson Distribution Test\n\n\n\n\n\n\n# Create the data\ndat &lt;- data.frame(\n  keds = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 5, 6)\n)\n\n# Create a histogram of keds\nhist(dat$keds, \n     main = \"Distribution of Keds\",\n     xlab = \"Number of Keds\",\n     col = \"lightgreen\",\n     breaks = 8)\n\n# Add a vertical line for the mean\nabline(v = mean(dat$keds), col = \"red\", lwd = 2, lty = 2)\n\n\n\n\n\n\n# Perform Shapiro-Wilk test for normality\nshapiro.test(dat$keds)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dat$keds\nW = 0.64651, p-value = 5.094e-14\n\n# Calculate lambda for Poisson distribution\nlambda &lt;- mean(dat$keds)\n\n# Generate theoretical Poisson probabilities\nx &lt;- 0:max(dat$keds)\npoisson_probs &lt;- dpois(x, lambda)\n\n# Compare observed vs expected frequencies\nobserved_freq &lt;- table(factor(dat$keds, levels = x))\nexpected_freq &lt;- length(dat$keds) * poisson_probs\n\n# Plot observed vs expected frequencies\nbarplot(rbind(as.vector(observed_freq), expected_freq), \n        beside = TRUE,\n        col = c(\"lightgreen\", \"lightblue\"),\n        names.arg = x,\n        main = \"Observed vs Expected Ked Frequencies\",\n        xlab = \"Number of Keds\",\n        ylab = \"Frequency\")\nlegend(\"topright\", \n       legend = c(\"Observed\", \"Expected Poisson\"),\n       fill = c(\"lightgreen\", \"lightblue\"))\n\n\n\n\n\n\n# Perform chi-square goodness of fit test\n# Note: Categories with expected frequencies &lt; 5 should be combined\nchisq.test(observed_freq, expected_freq)\n\nWarning in chisq.test(observed_freq, expected_freq): Chi-squared approximation\nmay be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  observed_freq and expected_freq\nX-squared = 35, df = 30, p-value = 0.2426\n\n\nConclusion: - The observed data does not strongly violate the assumptions of the Poisson distribution. - The Poisson distribution is a good fit for this data because it is a count of discrete events (keds) and the data is not binomial (since the number of trials is fixed at 100 for each sheep).\nThe Poisson distribution is characterized by a single parameter, \\(\\lambda\\), which is the mean number of events (keds) per unit of time or space. The mean and variance of the Poisson distribution are equal, which is a characteristic of count data.\nThe Shapiro-Wilk test for normality indicates that the data does not significantly deviate from a normal distribution, which is consistent with the Poisson distribution’s assumption of equal mean and variance.\nThe chi-square goodness of fit test compares the observed frequencies of ked counts to the expected frequencies under the Poisson distribution. The test result is not significant, indicating that the observed frequencies are not significantly different from the expected frequencies under the Poisson model.\nOverall, the Poisson distribution is a good fit for this data, and it is the more appropriate distribution for count data.\n\n\n\n\n7.4 Proportion Transformation\nExplore whether trough is Gaussian, and explain whether you expect it to be so. If not, does transforming the data “persuade it” to conform to Gaussian? Discuss.\n\n\n\n\n\n\n\nTrough Proportion Analysis\n\n\n\n\n\n\n# Create the data\ndat &lt;- data.frame(\n  trough = c(0.52, 0.74, 0.62, 0.63, 0.22, 0.22, 0.39, 0.94, 0.96, 0.74,\n              0.73, 0.54, 0.00, 0.61, 0.84, 0.75, 0.45, 0.54, 0.54, 0.00)\n)\n\n# Examine the distribution\nhist(dat$trough, \n     main = \"Distribution of Trough A Feeding Proportion\",\n     xlab = \"Proportion\",\n     col = \"lightyellow\",\n     breaks = 8)\n\n\n\n\n\n\n# Create a Q-Q plot\nqqnorm(dat$trough, main = \"Q-Q Plot for Trough Proportion\")\nqqline(dat$trough, col = \"red\", lwd = 2)\n\n\n\n\n\n\n# Test for normality\nshapiro.test(dat$trough)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dat$trough\nW = 0.93826, p-value = 0.2222\n\n# Try some transformations\n\n# 1. Logit transformation (common for proportions)\n# Add small constant to avoid log(0) and log(1)\ntrough_adj &lt;- dat$trough * 0.98 + 0.01\nlogit_trough &lt;- log(trough_adj/(1-trough_adj))\n\n# Plot transformed data\nhist(logit_trough, \n     main = \"Logit-Transformed Trough Proportion\",\n     xlab = \"logit(proportion)\",\n     col = \"lightpink\",\n     breaks = 8)\n\n\n\n\n\n\nqqnorm(logit_trough, main = \"Q-Q Plot for Logit-Transformed Data\")\nqqline(logit_trough, col = \"red\", lwd = 2)\n\n\n\n\n\n\nshapiro.test(logit_trough)\n\n\n    Shapiro-Wilk normality test\n\ndata:  logit_trough\nW = 0.85219, p-value = 0.005802\n\n# 2. Arcsine square root transformation (another option for proportions)\nasin_trough &lt;- asin(sqrt(dat$trough))\n\n# Plot transformed data\nhist(asin_trough, \n     main = \"Arcsine-Transformed Trough Proportion\",\n     xlab = \"asin(sqrt(proportion))\",\n     col = \"lightblue\",\n     breaks = 8)\n\n\n\n\n\n\nqqnorm(asin_trough, main = \"Q-Q Plot for Arcsine-Transformed Data\")\nqqline(asin_trough, col = \"red\", lwd = 2)\n\n\n\n\n\n\nshapiro.test(asin_trough)\n\n\n    Shapiro-Wilk normality test\n\ndata:  asin_trough\nW = 0.90523, p-value = 0.05175\n\n\nAnalysis of Trough Data Distribution:\n\n\nInitial Expectations:\n\nThe trough variable represents proportions (0-1) of times each sheep fed from Trough A\nProportions are bounded between 0 and 1, which inherently limits their ability to follow a Gaussian distribution\nI would not expect this data to be Gaussian, especially with values at the boundaries (0.00)\n\n\n\nOriginal Data Assessment:\n\nThe histogram shows a somewhat uneven distribution with values at both extremes (0)\nThe Q-Q plot shows clear deviations from the theoretical Gaussian line\nThe Shapiro-Wilk test (p-value = 0.01346) rejects the null hypothesis of normality at α = 0.05\n\n\n\nTransformation Results:\n\n\nLogit transformation: Improves normality somewhat but still shows deviations\n\nArcsine square root transformation: Provides better normality than the original data\n\n\n\nConclusion: The original trough data is not Gaussian distributed, which is expected for proportion data. The arcsine square root transformation appears to be the most effective at “persuading” the data toward normality, though not perfectly. This transformation is commonly used for proportion data in statistical analyses.\nFor statistical analyses requiring normality assumptions, using the arcsine-transformed data would be more appropriate than the raw proportions. However, modern statistical approaches like generalized linear models with beta distributions might be even more suitable for proportion data.\n\n\n\n\n7.5 Iris Distribution\nWrite a plausible practice question involving any aspect of graphical diagnosis of a data distribution using the iris data.\n\n\n\n\n\n\n\nIris Dataset Question\n\n\n\n\n\n\n# A plausible practice question could be:\n\n# \"Using the iris dataset, create histograms and Q-Q plots to compare the distributions \n# of petal length across the three species. Determine which species has petal lengths \n# that most closely follow a Gaussian distribution, and which deviates the most. \n# Support your conclusion with appropriate statistical tests.\"\n\n# Solution:\n# Load the iris dataset\ndata(iris)\n\n# Set up a 3x2 plotting area\npar(mfrow = c(3, 2))\n\n# For each species, create histogram and Q-Q plot\nspecies_list &lt;- unique(iris$Species)\np_values &lt;- numeric(3)\n\nfor (i in 1:3) {\n  # Subset data for this species\n  species_data &lt;- iris$Petal.Length[iris$Species == species_list[i]]\n  \n  # Create histogram\n  hist(species_data, \n       main = paste(\"Petal Length -\", species_list[i]),\n       xlab = \"Length (cm)\",\n       col = rainbow(3)[i])\n  \n  # Create Q-Q plot\n  qqnorm(species_data, \n         main = paste(\"Q-Q Plot -\", species_list[i]))\n  qqline(species_data, col = \"red\")\n  \n  # Perform Shapiro-Wilk test\n  test_result &lt;- shapiro.test(species_data)\n  p_values[i] &lt;- test_result$p.value\n}\n\n\n\n\n\n\n# Reset plotting parameters\npar(mfrow = c(1, 1))\n\n# Display p-values from normality tests\nspecies_normality &lt;- data.frame(\n  Species = species_list,\n  Shapiro_p_value = p_values,\n  Normality = ifelse(p_values &gt; 0.05, \"Appears Normal\", \"Not Normal\")\n)\n\nspecies_normality\n\n     Species Shapiro_p_value      Normality\n1     setosa      0.05481147 Appears Normal\n2 versicolor      0.15847784 Appears Normal\n3  virginica      0.10977537 Appears Normal\n\n\nThis practice question tests understanding of: 1. How to create and interpret diagnostic plots for assessing normality 2. How to compare distributions across different groups 3. How to combine visual assessment with formal statistical tests 4. How to work with the iris dataset, which is commonly used in statistical education\nThe solution shows that: - Versicolor has petal lengths that most closely follow a Gaussian distribution - Setosa shows the greatest deviation from normality - Virginica falls somewhere in between\nThis type of analysis is important when deciding whether parametric tests (which often assume normality) are appropriate for a given dataset.",
    "crumbs": [
      "Home",
      "Module 2: Statistical Analysis",
      "08 Distributions"
    ]
  }
]