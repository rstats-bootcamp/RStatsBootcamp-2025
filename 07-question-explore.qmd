---
title: "07 Explore data"
---

<center>![We call sizing up the data "weighing the pig"](img/unsplash-pig.png)</center>

## 1 Question, explore, analyze (a workflow for data science)

::: callout-note
## Learning Objectives

By the end of this lesson, you will be able to:

-   Formulate statistical questions
-   Perform hypothesis testing
-   Summarize what "Weighing the Pig" means
-   Discuss the use of variables in graphing
-   Distinguish between "Analysis" versus "EDA"
-   Explain the concept of a Statistical Analysis Plan

:::

<br>

> A dataset often comes to the Data Scientist in an imperfect state, possibly incomplete, containing errors, and with minimal description. Likewise, it may contain wonderful knowledge, there to discover. Either way, your first task is to **weigh the pig**.

<br>

The very first task for any data analysis is to gain an understanding of the data itself. This typically involves examining the variables. Are they as we expect? Do we need to adjust the variable types?

<br>

**EDA Exploratory Data Analysis**

This almost always involves graphing the data, and possibly examining numerical summaries and statistical assumptions. Further, it is necessary to look for errors in the data both trivial (e.g. misspelling factor level names like "control" with an extra space "control"), and more serious errors such as numerical typographical errors (e.g. misplacing a decimal point is a classic: height of 5 men in feet: `c(5.5, 5.8, 6.1, 5.9, 52.)`.

In total, this part of data analysis is sometimes referred to as **Exploratory Data Analysis**.

EDA is part practical and part philosophical in that is requires skill and experience, but is also subjective. Think of it as a step that might take a long while, where the data scientists decides what the analysis is that will be applied to the data, that the analysis is correct and appropriate. Ironically, while EDA is considered very important and can take a large proportion of the total time spent analyzing data, it is usually only reported on very briefly if at all.

The order of operation for most analyses should be

-   **1 question**

-   **2 explore**

-   **3 analyse**

<br>

> You choose your data analysis prior to collecting the first data point.

Focus on the question and make sure it is clear in formulation, and choose an analysis approach that can resolve the question , given the data... But the data collection should be DESIGNED to fit the question and chosen analysis prior to collection. Explore the data to examine any assumptions required for the analysis, including the use of graphing and any diagnostic or summary statistics. Finally, perform and summarize the analysis. We will practice this workflow for different basic questions, with an emphasis on simple quantitative data.

<br>

## 2 Question formulation and hypothesis testing

> It is the primary responsibility of the scientist to agree on the specific details of generating evidence from data to answer questions (i.e., statistical analysis). When these roles are occupied by the same person, this matter should be settled before collecting any data.

The general topic of formulating statistical questions is vast; many books have been written on the subject. The tradition and practice of statistical analysis has evolved through time. Here we will focus on the traditional starting point for a "first statistics course", within the context of **Null Hypothesis Significance testing (NHST)**.

<br>

### 2.1 Sampling concept and NHST

The gambit of NHST is that there is a **population of interest** but that the population cannot be directly measured because it is too big or otherwise inconvenient or impossible to measure. Thus, **experimental samples** are drawn randomly from the population, possibly subjected to **experimental conditions**, and the magnitude of observed differences or measured associations are summarized by various **test statistics** and compared to how likely such an observed difference or association would be to observe in the absence of the hypothesized effect.

The **null hypothesis** is the one consistent with no effect or difference. We evaluate whether to reject the null hypothesis using the **P-value**, the (conditional) probability that the observed effect is unlikely to arise duie to sampling or experimental error.

Traditionally, the P-value is compared to the **alpha value**, almost always set to `0.05`. This alpha value can be interpreted as the maximum probability that is acceptable of making a mistake and concluding there IS a difference, when in fact a difference does not exist. When the P-value is less than 0.05, we conclude there is a difference, rejecting the null hypothesis and "accepting" the hypothesis we predicted was true, usually referred to as the **alternative hypothesis**.

<br>

### 2.2 NHST notes

**Benefits of NHST**

-   Familiar and acceptable to majority of researchers

-   Typically robust to assumptions when applied correctly

-   Strong framework for evidence, especially for experiments

-   The basic idea is objective and simple

<br>

**Criticism of HNST**

-   Often conceived, applied and interpreted under error

-   Validation of analysis (e.g. assumptions testing) is often neglected

-   Education for applied researchers often deficient

-   Though simple, practitioners may be ignorant of subtle concepts

<br>

### 2.3 Further reading

If the idea is new to you that NHST in statistics is not perfect and you want to get serious about understanding why, like most subjects, you will need to pursue further sources.

[Anderson, D.R., Burnham, K.P. and Thompson, W.L., 2000. Null hypothesis testing: problems, prevalence, and an alternative. The journal of wildlife management, pp.912-923.](https://www.jstor.org/stable/3803199?seq=1)

[Nickerson, R.S., 2000. Null hypothesis significance testing: a review of an old and continuing controversy. Psychological methods, 5(2), p.241.](https://psycnet.apa.org/journals/met/5/2/241.html?uid=2000-07827-007)

[Nix, T.W. and Barnette, J.J., 1998. The data analysis dilemma: Ban or abandon. A review of null hypothesis significance testing. Research in the Schools, 5(2), pp.3-14.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.453.9776&rep=rep1&type=pdf)

[Stephens, P.A., Buskirk, S.W., Hayward, G.D. and Martinez Del Rio, C., 2005. Information theory and hypothesis testing: a call for pluralism. Journal of applied ecology, 42(1), pp.4-12.](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/j.1365-2664.2005.01002.x)

<br>

## 3 Summarize: Weighing the Pig

<br>

> The best way gain skill in handling data is to practice.

**Weighing the pig** is the term we use to describe creating a summary-at-a-glance of a dataset. Usually this includes graphics and statistical summary, as well a description of **how much data** we have. A key consideration is, also, the specification of the variables.

We will practice data handling with the data file [chickwts.xlsx](data/7-chickwts.xlsx).

Download the file, read it into a data object in R called `chicks`, and convert the `feed` variable to a factor if necessary.

``` r
# Try this:

# Download the 7-chickwts.xlsx file, read it into a data 
# object in R called "chicks", 
# and convert the "feed" variable to a factor if necessary.

# Do not neglect looking inside the "raw" data file
# Is it as you expect?  Is the data dictionary present and clear?

# Load necessary libraries
library(openxlsx)

# Read file
setwd("D:/Dropbox/git/DSgarage/public/data") # NB change to YOUR file path...
chicks <- read.xlsx("7-chickwts.xlsx")

# Convert feed to factor if needed
class(chicks$feed) # Character
chicks$feed <- factor(chicks$feed)
class(chicks$feed) # Factor
```

<br>

### 3.1 Chick data

> The hypothesis voices "how you think the world works" or what you predict to be true"

The hypothesis we believe is true for the chicks dataset might be phrased in different ways.

-   Chick weight differs after 6 weeks according to feed additive type

-   Mean chick weight varies according to feed additive type

-   The variance between chick weight for different feed additives is bigger than the variance within chick weight as a whole

<br>

### 3.2 Hypothesis

The minimum amount of information we are usually interested in when sizing up a dataset is **How much data is there?**, **What is the central tendency (e.g. the mean, variance, etc.)?**, and possibly **Are there rare values?**.

We would typically **start graphing the data** right away. If we have a notion of what our questions or hypotheses are, they should inform the initial peek at the data. For example, in the `chickwts` data, we know our question will be related not to the overall central tendency of chick weight, but to chick weight for each individual feed type.

We do not approach this sizing up of the data in a workhorse fashion, merely to check a tick box. We are looking quickly for details in the data that give us insight into what the data is like. For example, we peek at whether the mean and median are close to each other (indicator our data may be [Gaussian](https://en.wikipedia.org/wiki/Normal_distribution)), we compare the [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation), variance or [standard error](https://en.wikipedia.org/wiki/Standard_error) of a numeric variable relative to different levels of a factor, to see if they are similar.

``` r
# Try this:

# Summarize the whole dataset
# summary() provides summary statistics for numeric variables and counts
summary(chicks)

# we might want to look at summary for different levels of feed
?summary
summary(object = chicks$weight[which(chicks$feed == "casein")])
summary(object = chicks$weight[which(chicks$feed == "horsebean")])
# etc. - this method is easy but inelegant?

# aggregate()
?aggregate

# mean
aggregate(x = chicks$weight, by = list(chicks$feed), FUN = mean)

# standard deviation
aggregate(x = chicks$weight, by = list(chicks$feed), FUN = sd)

# You can make your own function for the FUN argument
# stadard error of mean, SEM = standard deviation / square root of sample size
aggregate(x = chicks$weight, by = list(chicks$feed), 
          FUN = function(x){ sd(x)/sqrt(length(x)) })

# You can apply several functions and name them!
aggregate(x = chicks$weight, by = list(feed = chicks$feed), 
          FUN = function(x){ c(mean = mean(x), 
                               sd = sd(x),  
                               SEM = sd(x)/sqrt(length(x)))})
```

<br>

## 4 Variables and graphing

> A good graph usually tells the whole story, but a bad graph is worse than no graph at all.

<br>

<center>![XKCD Convinced by data](https://imgs.xkcd.com/comics/convincing.png)</center>

<br>

There are a few topics in graphing data that are important to consider here, but the topic is wide and deep, analytical, creative, and even artistic. We make a distinction between **graphs used to explore data during EDA** (meant to be "consumed" only by the data scientist who made them and are of no use to document a pattern to others) and **graphs intended to constitute evidence**.

<br>

### 4.1 Scientific graphs

A few graphing principles:

-   Must convey the relevant information

-   Should be consistent in aesthetics

-   Must be self-contained (meaning is contained 100% within the figure and legend)

-   Should reflect a hypothesis or statistical concept (if not purely descriptive)

-   Should be appropriate to the data

<br>

You can think of `R` graphics as a way to "build up information in layers" onto a graph. There are many aesthetic features of graph that can be controlled, like adding colors, text, lines, legends, etc. The `R` graphics system is very simple to use, but can also be very powerful (mastering this takes practice). We make a distinction here between `R` **base graphics** and packages that can be used to make specialized and varied graphs (like the powerful and popular package {`ggplot`})

<br>

### 4.2 Layering information

We can look at graphing the `chicks` data in a few different ways. We will try a few different graphs in this way, building up features. We might build up features on a graph using **arguments** in a particular graph function.

<br>

Like, adding

-   a main title with the argument `main`

-   the x axis title with the argument `xlab`

-   adding lines with the functions `abline()` or `lines()`

<br>

### 4.3 Types of graphs

Typically you would choose the type of graph that both fits the type of data you have and that conveys the information you wish to examine or showcase. E.g., for a single numeric variable, you might wish to show:

-   The distribution of data with a **histogram**: `hist()`

-   The central tendency relative to a factor with a **boxplot**: `boxplot()`

<br>

**Histogram of the `chicks` data**

``` r
# The least you can do
help(hist)
hist(x = chicks$weight)
```

![](img/2.1-hist1.png)

<br>

**Add a title with `main`**

``` r
# Argument main
hist(x = chicks$weight,
     main = "Distribution of chick weights (all feeds)")
```

![](img/2.1-hist2.png)

<br>

**Add an x axis title with `xlab`**

``` r
# x axis title
hist(x = chicks$weight,
     main = "Distribution of chick weights (all feeds)",
     xlab = "Chick weight (grams)")
```

![](img/2.1-hist3.png)

<br>

**Add a vertical line for the weight mean with `abline()`**

``` r
# Add vertical line for mean weight
hist(x = chicks$weight,
     main = "Distribution of chick weights (all feeds)",
     xlab = "Chick weight (grams)")

help(abline)
abline(v = mean(chicks$weight), col = "red", lty = 2, lwd = 3)
```

![](img/2.1-hist4.png)

<br>

``` r
# Try a boxplot

help(boxplot)
boxplot(x = chicks$weight)

# I have seen worse graphs, but I can't remember when.
# Flash challenge: Improve the graph
```

![](img/2.1-box1.png)

<br>

``` r
# weight as a function of feed
boxplot(formula = weight ~ feed,
        data = chicks)
# This is probably a good representation of our hypothesis
# Flash challenge: Improve the graph...
```

![](img/2.1-box2.png)

<br>

## 5 "Analysis" versus "EDA"

Although you could consider Exploratory Data Analysis, EDA, an important part of the complete process of data analysis, we might make a distinction between "Analysis" the part of analysis that generates **Evidence**, and that of EDA which is used to explore data and test assumptions.

<br>

### 5.1 Analysis

A data analysis is

-   Designed to fit a specific question or hypothesis

-   Part of a workflow: Informal hypothesis statement (in plain language) \> Statistical hypothesis (specifies a or implies a statistical test) \> Evidence (the specific results)

-   Designed and usually formatted to present to others, such as in a report or a scientific manuscript

-   Contains only bare essentials as relates to the initial hypothesis (e.g. a good graph, the summary of a statistical analysis)

-   Should strictly be reproducible via a script and archived data

-   Done in conjunction with EDA

<br>

### 5.2 EDA

Exploratory data analysis is

-   Informal and may be haphazard

-   Designed to explore or gain understanding of data

-   Assumptions testing

-   Usually not designed to document or show to others

-   Occurs primarily before (every) analysis

-   May or may not be documented to be reproducible

-   Done before the final, evidence-generating Analysis

<br>

We can keep this concept of EDA versus Analysis in our mind while we discuss the Statistical Analysis Plan.

<br>

## 6 Statistical Analysis Plan: the concept

> I have a cunning (statistical analysis) plan -Baldrick

A **Statistical Analysis Plan** (SAP) is a formal document that should be used to design data analysis. One of the most important functions of the SAP is to make a formal connection between the hypothesis, the data collected and and the method of analysis that will be used to generate evidence to support or refute the hypothesis. This is conducted before any data are collected.

The components of a basic SAP are:

-   The **hypotheses stated in plain language**

-   Each hypothesis translated into a **specific statistical model**

-   Specification of **data and and data collection methods**

-) Specification of **effect size**

-   **Justification of sample size** through power analysis or other means

<br>

Definition of all of these components is beyond the boundaries of this Bootcamp, however the explicit connection of hypotheses with a statistical model is one of the very basic elements of best practice in science.

<br>

### 6.1 The scientific method, Classic version

We usually learn the scientific method as a cycle where we conceive a problem, form a hypothesis, conduct an experiment, evaluate the result and so on. We learn and teach this as a literal cycle.

<br>

<center>![The classic view of the scientific process](img/2.1-sci-proc1.png)</center>

<br>

This classic view of the scientific process implies that we plan the analysis only after we conduct the experiment and collect data. While many data scientists or statisticians would agree that this model is widely used in science, it is considered very poor practice for several reasons.

-   The expected difference or relationship (i.e., the [**effect size**](https://en.wikipedia.org/wiki/Effect_size)) should explicitly be part of the hypothesis and quantified BEFORE collecting data

-   The statistical test must be chosen prior to collect the data to insure the evidence matches the expectation

-   The [**sample size should be justified**](https://en.wikipedia.org/wiki/Sample_size_determination), using power analysis or a less formal means. Collecting too little data will likely result in failing to detect a difference (even if your hypothesis is correct!); Collecting too much data is simply a waste of resources.

<br>

<center>![Scientific Process - what we teach children in school is not quite right](img/2.1-sci-proc2.png)</center>

<br>

### 6.2 Best practice scientific method

The traditional view of the scientific method should probably be adjusted to explicitly accommodate planning the analysis at the same time as the hypothesis formulation stage. Likewise, the analysis plan should specifically influence the design of the data collection for the experiment.

<br>

<center>![Modern scientific process](img/2.1-sci-proc3.png)</center>

<br>

A modern view of best practice of scientific endeavor includes an experimental design phase, with consideration of **effect size** and **power analysis**, and the production of a **statistical analysis plan** that contains a formal statistical hypothesis. All off this happens prior to any data collection.

<br>

## 7 Practice exercises

For the following questions, use the [field-trial.xlsx dataset](data/field-trial.xlsx).

This is real data in Tidy Data format, but our information for these exercises is limited precisely to the contents of the file, including the data dictionary. In this experiment, seeds were raised under field trial conditions for two weeks to look at the effect of different treatment conditions on mass of gain during germination. There are several measured variables, with the calculated `pct` variable probably intended to be the dependent variable, with the factor `treatment` being the main explanatory variable for variation in `pct`.

<br>

### 7.1 Sampling Distribution

Create a simulation of a sampling distribution. Use the `rnorm()` function to create a "population" of 10,000 values with a mean of 100 and a standard deviation of 15. Then take 100 samples of size 30 from this population and calculate the mean of each sample. Plot a histogram of the sample means and calculate the mean and standard deviation of the sample means. How does the standard deviation of the sample means compare to the standard error of the mean (standard deviation of the population divided by the square root of the sample size)?

<br>

::: {.callout-tip collapse="true"}
## R Analysis File Setup
```{r}
#| echo: true
#| eval: false

## HEADER ####
## Who: Your Name
## What: Field trial data analysis
## Last edited: 2023-05-15
####

## CONTENTS ####
## 00 Setup
## 01 Data Exploration
## 02 Statistical Analysis
## 03 Visualization
####

## 00 Setup ####
# Set working directory (adjust path to your actual directory)
# setwd("path/to/your/directory")

# Load required libraries
library(openxlsx)

# Read in the field trial data
seed <- read.xlsx("data/field-trial.xlsx")

# Check the structure of the data
str(seed)

# View the first few rows
head(seed)
```

This code sets up a properly structured R analysis file with:
- A header containing author information and purpose
- A table of contents outlining the major sections
- A setup section that:
  - Sets the working directory
  - Loads the necessary library for reading Excel files
  - Reads in the data and assigns it to an object called "seed"
  - Includes basic commands to examine the data structure

This structure follows best practices for reproducible research by clearly documenting each step of the analysis process.
:::

<br>

### 7.2 Seed Data Analysis

Using the field-trial.xlsx data file, perform an appropriate statistical analysis to determine if there are differences in germination rates between treatments. Include appropriate data visualization and report your results in the technical style.

<br>

::: {.callout-tip collapse="true"}
## Data Type Conversion
```{r}
#| echo: true
#| eval: false

# Load required libraries and read data
library(openxlsx)
seed <- read.xlsx("data/field-trial.xlsx")

# Check current data types
str(seed)

# Convert pct, wet, and dry to numeric
seed$pct <- as.numeric(seed$pct)
seed$wet <- as.numeric(seed$wet)
seed$dry <- as.numeric(seed$dry)

# Convert block and trial to factors
seed$block <- factor(seed$block)
seed$trial <- factor(seed$trial)

# Convert treatment to factor with "Control" as reference level
seed$treatment <- factor(seed$treatment, levels = c("Control", "Treatment1", "Treatment2", "Treatment3"))

# Verify the conversions
str(seed)
```

This code ensures that all variables have the appropriate data types:
- Numeric variables (`pct`, `wet`, `dry`) are converted using `as.numeric()`
- Categorical variables (`block`, `trial`) are converted to factors
- The `treatment` variable is converted to a factor with "Control" explicitly set as the first level, making it the reference level for statistical analyses

Setting "Control" as the reference level is particularly important for statistical tests that compare treatments against a control condition, as it affects how contrasts are calculated in models.
:::

<br>

### 7.3 Comprehensive Boxplot

Create a boxplot of the seed germination data by treatment, adding horizontal reference lines for the overall mean and ±1 standard deviation. Include a legend explaining these reference lines.

<br>

::: {.callout-tip collapse="true"}
## Aggregating Data by Treatment
```{r}
#| echo: true
#| eval: false

# Load required libraries and read data
library(openxlsx)
seed <- read.xlsx("data/field-trial.xlsx")

# Convert treatment to factor and pct to numeric
seed$treatment <- factor(seed$treatment)
seed$pct <- as.numeric(seed$pct)

# Calculate mean, SD, SE, and count of pct for each treatment level
treatment_stats <- aggregate(pct ~ treatment, data = seed, 
                            FUN = function(x) {
                              c(mean = mean(x),
                                sd = sd(x),
                                se = sd(x)/sqrt(length(x)),
                                count = length(x))
                            })

# Display the results
treatment_stats
```

This code calculates four key statistics for the `pct` variable grouped by treatment:
- Mean: The average percentage value for each treatment
- Standard Deviation (SD): Measures the amount of variation in the data
- Standard Error (SE): Estimates the standard deviation of the sampling distribution of the mean
- Count: The number of observations in each treatment group

The `aggregate()` function is used with a custom function that calculates all these statistics at once. The formula notation `pct ~ treatment` specifies that we want to aggregate the `pct` variable by the `treatment` factor.

This summary provides a comprehensive overview of how the treatments affect the percentage values, including both the central tendency (mean) and the variability (SD, SE).
:::

<br>

### 7.4 Multi-Panel Boxplot

(hard: may require tinkering and problem solving)

Experiment making a boxplot showing `pct ~ treatment` separated for each trial

<br>

::: {.callout-tip collapse="true"}
## Multi-Panel Boxplot by Trial
```{r}
#| echo: true
#| eval: false

# Load required libraries and read data
library(openxlsx)
seed <- read.xlsx("data/field-trial.xlsx")

# Convert variables to appropriate types
seed$treatment <- factor(seed$treatment)
seed$trial <- factor(seed$trial)
seed$pct <- as.numeric(seed$pct)

# Method 1: Using par() to create multiple panels
# First, determine how many trials we have
trials <- unique(seed$trial)
n_trials <- length(trials)

# Set up the plotting area with one row and multiple columns
par(mfrow = c(1, n_trials))

# Create a boxplot for each trial
for (t in trials) {
  # Subset data for this trial
  trial_data <- seed[seed$trial == t, ]
  
  # Create boxplot
  boxplot(pct ~ treatment, data = trial_data,
          main = paste("Trial", t),
          xlab = "Treatment",
          ylab = "Growth Percentage (%)",
          col = "lightblue")
}

# Reset the plotting parameters
par(mfrow = c(1, 1))

# Method 2: Using a single plot with interaction
# Create a new factor that combines treatment and trial
seed$treat_trial <- interaction(seed$treatment, seed$trial)

# Create the boxplot with all combinations
boxplot(pct ~ treat_trial, data = seed,
        main = "Growth Percentage by Treatment and Trial",
        xlab = "",
        ylab = "Growth Percentage (%)",
        col = rainbow(n_trials)[as.numeric(seed$trial)],
        las = 2)  # Rotate x-axis labels

# Add a more informative x-axis
axis(1, at = 1:length(levels(seed$treat_trial)), 
     labels = levels(seed$treat_trial), 
     las = 2, cex.axis = 0.7)

# Add a legend
legend("topright", 
       legend = paste("Trial", trials),
       fill = rainbow(n_trials),
       title = "Trial")
```

This solution presents two different approaches to creating boxplots separated by trial:

1. **Multiple Panel Approach**: Creates a separate boxplot for each trial using `par(mfrow)` to arrange them side by side. This makes it easy to compare treatments within each trial.

2. **Single Plot Approach**: Creates one boxplot with all treatment-trial combinations using the `interaction()` function to create a new factor that represents all possible combinations. This approach uses color coding to distinguish between trials.

Both methods allow you to see how treatment effects might vary across different trials, which is important for assessing the consistency and reproducibility of the results.

The choice between these approaches depends on the number of trials and treatments, and what aspects of the data you want to emphasize in your visualization.
:::

<br>

### 7.5 Iris Aggregation

Write a plausible practice question involving `aggregate()` and `boxplot()` in-built R dataset `iris`.

<br>

::: {.callout-tip collapse="true"}
## Iris Dataset Question
```{r}
#| echo: true
#| eval: true

# A plausible practice question could be:

# "Using the iris dataset, create a boxplot showing petal length by species. 
# Then use aggregate() to calculate the mean, median, and standard deviation 
# of petal length for each species. Add horizontal lines to your boxplot 
# representing the mean petal length for each species using different colors. 
# Include a legend identifying each species mean."

# Solution:
# Load the iris dataset
data(iris)

# Calculate statistics by species
petal_stats <- aggregate(Petal.Length ~ Species, data = iris, 
                        FUN = function(x) {
                          c(mean = mean(x),
                            median = median(x),
                            sd = sd(x))
                        })

# Display the statistics
petal_stats

# Create a boxplot
boxplot(Petal.Length ~ Species, data = iris,
        main = "Petal Length by Species",
        xlab = "Species",
        ylab = "Petal Length (cm)",
        col = c("lightpink", "lightblue", "lightgreen"))

# Define colors for mean lines
species_colors <- c("red", "blue", "darkgreen")

# Add horizontal lines for means of each species
for (i in 1:3) {
  species_data <- iris[iris$Species == levels(iris$Species)[i], ]
  species_mean <- mean(species_data$Petal.Length)
  segments(i-0.4, species_mean, i+0.4, species_mean, 
           col = species_colors[i], lwd = 2)
}

# Add legend
legend("topleft", 
       legend = paste(levels(iris$Species), "mean"),
       col = species_colors,
       lwd = 2,
       title = "Species Means")
```

This practice question tests understanding of:
1. Using `aggregate()` to calculate multiple statistics grouped by a factor
2. Creating informative boxplots with appropriate labels
3. Adding custom elements to enhance a plot's information content
4. Working with the iris dataset, which is commonly used for teaching data visualization

The solution demonstrates how to create a comprehensive visualization that combines boxplots (showing distribution) with horizontal lines (showing means), providing a rich view of how petal length varies across iris species.
:::

<br>


